{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Kamae","text":"<p>Kamae is a Python package comprising a set of reusable components for preprocessing inputs offline (Spark) and online (TensorFlow).</p> <p>Build all your big-data preprocessing pipelines in Spark, and get your Keras preprocessing model for free!</p>"},{"location":"#usage","title":"Usage","text":"<p>The library is designed with three main usage patterns in mind:</p> <ol> <li>Import and use Keras preprocessing layers directly. </li> </ol> <p>This is the recommended usage pattern for complex use-cases. For example when your data is not tabular, or when you need to apply preprocessing steps that are not supported by the provided Spark Pipeline interface. The library provides a set of Keras subclassed layers that can be imported and used directly in a Keras model.  You can chain these layers together to create complex preprocessing steps, and then use the resulting model as the input to a trainable model.</p> <ol> <li>Use the provided Spark Pipeline interface to build Keras preprocessing models.</li> </ol> <p>This is the recommended usage pattern for big data use-cases, (classification, regression, ranking) where your data is tabular,  and you want to apply standard preprocessing steps such as normalization, one-hot encoding, etc. The library provides Spark transformers, estimators and pipelining so that a user can chain together preprocessing steps in Spark, fit the pipeline on a Spark DataFrame, and then export the result as a Keras model. Unit tests ensure parity between the Spark and Keras implementations of the preprocessing layers.</p> <ol> <li>Use the provided Sklearn Pipeline interface to build Keras preprocessing models.</li> </ol> <p>Note: This is provided as an example of how Kamae could be extended to support other pipeline SDKs but it is NOT actively supported. It is far behind the Spark interface in terms of transformer coverage &amp; enhancements we have made such as type &amp; shape parity. Contributions are welcome, but please use at your own risk.</p> <p>Works in the same way as the Spark pipeline interface, just using Scikit-learn transformers, estimators and pipelines. This is the recommended usage pattern for small data use-cases, (classification, regression, ranking) where your data is tabular, and you want to apply standard preprocessing steps such as normalization, one-hot encoding, etc.</p> <p>Keras Tuner support is also provided for the Spark &amp; Scikit-learn Pipeline interface, whereby a model builder function is returned so that the hyperparameters of the preprocessing steps can be tuned using the Keras Tuner API.</p> <p>Once you have created a Kamae preprocessing model, you can use it as the input to a trainable model. See these docs for more information.</p> <p>For advice on achieving type parity between the Spark and Keras implementations of the preprocessing layers, see these docs.</p> <p>For information on achieving shape parity between the Spark and Keras implementations of the preprocessing layers, see these docs.</p>"},{"location":"#pipeline-examples","title":"Pipeline Examples","text":"<p>See the examples directory for various examples of how to use the Spark Pipeline interface. Similarly, see the examples directory for various examples of how to use the Scikit-learn Pipeline interface. Follow the development instructions below to run the examples locally.</p>"},{"location":"#supported-preprocessing-layers","title":"Supported Preprocessing Layers","text":"Transformation Description Keras Layer Spark Transformer Scikit-learn Transformer AbsoluteValue Applies the <code>abs(x)</code> transform. Link Link Not yet implemented ArrayConcatenate Assembles multiple features into a single array. Link Link Link ArrayCrop Crops or pads a feature array to a consistent size. Link Link Not yet implemented ArraySplit Splits a feature array into multiple features. Link Link Link ArraySubtractMinimum Subtracts the minimum element in an array from therest to compute a timestamp difference. Ignores padded values. Link Link Not yet implemented BearingAngle Compute the bearing angle (https://en.wikipedia.org/wiki/Bearing_(navigation)) between two pairs of lat/long. Link Link Not yet implemented Bin Bins a numerical column into string categorical bins. Users can specify the bin values, labels and a default label. Link Link Not yet implemented BloomEncode Hash encodes a string feature multiple times to create an array of indices. Useful for compressing input dimensions for embeddings. Paper: https://arxiv.org/pdf/1706.03993.pdf Link Link Not yet implemented Bucketize Buckets a numerical column into integer bins. Link Link Not yet implemented ConditionalStandardScale Normalises by the mean and standard deviation, with ability to: apply a mask on another column, not scale the zeros, and apply a non standard scaling function. Link Link Not yet implemented CosineSimilarity Computes the cosine similarity between two array features. Link Link Not yet implemented CurrentDate Returns the current date for use in other transformers. Link Link Not yet implemented CurrentDateTime Returns the current date time in the format yyyy-MM-dd HHss.SSS for use in other transformers. Link Link Not yet implemented CurrentUnixTimestamp Returns the current unix timestamp in either seconds or milliseconds for use in other transformers. Link Link Not yet implemented DateAdd Adds a static or dynamic number of days to a date feature. NOTE: Destroys any time component of the datetime if present. Link Link Not yet implemented DateDiff Computes the number of days between two date features. Link Link Not yet implemented DateParse Parses a string date of format YYYY-MM-DD to extract a given date part. E.g. day of year. Link Link Not yet implemented DateTimeToUnixTimestamp Converts a UTC datetime string to unix timestamp. Link Link Not yet implemented Divide Divides a single feature by a constant or divides multiple features against each other. Link Link Not yet implemented Exp Applies the exp(x) operation to the feature. Link Link Not yet implemented Exponent Applies the x^exponent to a single feature or x^y for multiple features. Link Link Not yet implemented HashIndex Transforms strings to indices via a hash table of predeterminded size. Link Link Not yet implemented HaversineDistance Computes the haversine distance between latitude and longitude pairs. Link Link Not yet implemented Identity Applies the identity operation, leaving the input the same. Link Link Link IfStatement Computes a simple if statement on a set of columns/tensors and/or constants. Link Link Not yet implemented Impute Performs imputation of either mean or median value of the data over a specified mask. Link Link Not yet implemented LambdaFunction Transforms an input (or multiple inputs) to an output (or multiple outputs) with a user provided tensorflow function. Link Link Not yet implemented ListMax Computes the listwise max of a feature, optionally calculated only on the top items based on another given feature. Link Link Not yet implemented ListMean Computes the listwise mean of a feature, optionally calculated only on the top items based on another given feature. Link Link Not yet implemented ListMedian Computes the listwise median of a feature, optionally calculated only on the top items based on another given feature. Link Link Not yet implemented ListMin Computes the listwise min of a feature, optionally calculated only on the top items based on another given feature. Link Link Not yet implemented ListRank Computes the listwise rank (ordering) of a feature. Link Link Not yet implemented ListStdDev Computes the listwise standard deviation of a feature, optionally calculated only on the top items based on another given feature. Link Link Not yet implemented Log Applies the natural logarithm <code>log(alpha + x)</code> transform  . Link Link Link LogicalAnd Performs an and(x, y) operation on multiple boolean features. Link Link Not yet implemented LogicalNot Performs a not(x) operation on a single boolean feature. Link Link Not yet implemented LogicalOr Performs an or(x, y) operation on multiple boolean features. Link Link Not yet implemented Max Computes the maximum of a feature with a constant or multiple other features. Link Link Not yet implemented Mean Computes the mean of a feature with a constant or multiple other features. Link Link Not yet implemented Min Computes the minimum of a feature with a constant or multiple other features. Link Link Not yet implemented MinHashIndex Creates an integer bit array from a set of strings using the MinHash algorithm. Link Link Not yet implemented MinMaxScale Scales the input feature by the min/max resulting in a feature in [0, 1]. Link Link Not yet implemented Modulo Computes the modulo of a feature with the mod divisor being a constant or another feature. Link Link Not yet implemented Multiply Multiplies a single feature by a constant or multiples multiple features together. Link Link Not yet implemented NumericalIfStatement Performs a simple if else statement witha given operator. Value to check, result if true or false can be constants or features. Link Link Not yet implemented OneHotEncode Transforms a string to a one-hot array. Link Link Not yet implemented OrdinalArrayEncode Encodes strings in an array according to the order in which they appear. Only for 2D tensors. Link Link Not yet implemented Round Rounds a floating feature to the nearest integer using <code>ceil</code>, <code>floor</code> or a standard <code>round</code> op. Link Link Not yet implemented RoundToDecimal Rounds a floating feature to the nearest decimal precision. Link Link Not yet implemented SharedOneHotEncode Transforms a string to a one-hot array, using labels across multiple inputs to determine the one-hot size. Link Link Not yet implemented SharedStringIndex Transforms strings to indices via a vocabulary lookup, sharing the vocabulary across multiple inputs. Link Link Not yet implemented SingleFeatureArrayStandardScale Normalises by the mean and standard deviation calculated over all elements of all inputs, with ability to mask a specified value. Link Link Not yet implemented StandardScale Normalises by the mean and standard deviation, with ability to mask a specified value. Link Link Link StringAffix Prefixes and suffixes a string with provided constants. Link Link Not yet implemented StringArrayConstant Inserts provided string array constant into a column. Link Link Not yet implemented StringCase Applies an upper or lower casing operation to the feature. Link Link Not yet implemented StringConcatenate Joins string columns using the provided separator. Link Link Not yet implemented StringContains Checks for the existence of a constant or tensor-element substring within a feature. Link Link Not yet implemented StringContainsList Checks for the existence of any string from a list of string constants within a feature. Link Link Not yet implemented StringEqualsIfStatement Performs a simple if else statement on string equality. Value to check, result if true or false can be constants or features. Link Link Not yet implemented StringIndex Transforms strings to indices via a vocabulary lookup Link Link Not yet implemented StringListToString Concatenates a list of strings to a single string with a given delimiter. Link Link Not yet implemented StringMap Maps a list of string values to a list of other string values with a standard CASE WHEN statement. Can provide a default value for ELSE. Link Link Not yet implemented StringIsInList Checks if the feature is equal to at least one of the strings provided. Link Link Not yet implemented StringReplace Performs a regex replace operation on a feature with constant params or between multiple features Link Link Not yet implemented StringToStringList Splits a string by a separator, returning a list of parametrised length (with a default value for missing inputs). Link Link Not yet implemented SubStringDelimAtIndex Splits a string column using the provided delimiter, and returns the value at the index given. If the index is out of bounds, returns a given default value Link Link Not yet implemented Subtract Subtracts a constant from a single feature or subtracts multiple features from each other. Link Link Not yet implemented Sum Adds a constant to a single feature or sums multiple features together. Link Link Not yet implemented UnixTimestampToDateTime Converts a unix timestamp to a UTC datetime string. Link Link Not yet implemented"},{"location":"#mac-armx86_64-support","title":"Mac ARM/x86_64 Support","text":"<p>From <code>tensorflow&gt;=2.13.0</code> onwards, TensorFlow directly releases builds for Mac ARM chips. </p> <p>Kamae supports <code>tensorflow&gt;=2.9.1,&lt;2.19.0</code>, however, if you require <code>tensorflow&lt;2.13.0</code> and are using a Mac ARM chip, you will need to install <code>tensorflow-macos&lt;2.13.0</code> yourself.</p> <p>From <code>tensorflow&gt;=2.18.0</code> onwards, TensorFlow does not release builds for Mac x86_64 chips. If you are on an old Mac chip, please bear this in mind when using the library.</p>"},{"location":"#installation","title":"Installation","text":"<p>The Kamae package is pushed to PyPI, and can be installed using the command: <pre><code>pip install kamae\n</code></pre> Alternatively, the package can be installed from the source code by downloading the latest release .tar file from the Releases page and running the following command: <pre><code>pip install kamae-&lt;version&gt;.tar\n</code></pre></p>"},{"location":"#development","title":"Development","text":""},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#installing-python","title":"Installing Python","text":"<p>Local development is in Python 3.10. uv can install this for you, once you have run <code>make setup-uv</code>. Then run <code>make install</code></p> <p>The final package supports Python 3.8 -&gt; 3.12.</p>"},{"location":"#installing-pipx","title":"Installing <code>pipx</code>","text":"<p><code>pipx</code> is used to install <code>uv</code> and <code>pre-commit</code> in isolated environments.</p> <p>Installing <code>pipx</code> depends on your operating system. See the pipx installation instructions.</p>"},{"location":"#setting-up-the-project","title":"Setting up the project","text":"<p>Once python 3.10 and <code>pipx</code> are installed, run the below make command to set up the project: <pre><code>make setup\n</code></pre></p>"},{"location":"#helpful-commands","title":"Helpful Commands","text":"<p>A Makefile is provided to simplify common development tasks. The available commands can be listed by running: <pre><code>make help\n</code></pre></p> <p>In order to get setup for local development, you will need to install the project dependencies and pre-commit hooks. This can be done by running: <pre><code>make setup\n</code></pre></p> <p>Once the dependencies are installed, tests, formatting &amp; linting can be run by running:</p> <pre><code>make all\n</code></pre> <p>You can run an example of the package by running: <pre><code>make run-example\n</code></pre></p> <p>You can test the inference of a model served by TensorFlow Serving by running: <pre><code>make test-tf-serving\n</code></pre></p> <p>Lastly, you can run both an example and test the inference of a model (above two commands) in one command by running: <pre><code>make test-end-to-end\n</code></pre></p> <p>See the docs here for more details on testing inference.</p>"},{"location":"#dependencies","title":"Dependencies","text":"<p>For local development, dependency management is controlled with the uv package, which can be installed by following the instructions here.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>To contribute to the project, a branch should be created from the <code>main</code> branch, and a pull request should be opened when the changes are ready to be reviewed. Please follow these docs for contributing new transformers.</p>"},{"location":"#code-quality","title":"Code Quality","text":"<p>The project uses pre-commit hooks to enforce linting and formatting standards. You should install the pre-commit hooks before committing for the first time by running: <pre><code>uv run pre-commit install\n</code></pre></p> <p>Additionally, for a pull request to be accepted, the code must pass the unit tests found in the <code>tests/</code> directory. The full suite of formatting, linting, coverage checks, and tests can be run locally with the command: <pre><code>make all\n</code></pre></p>"},{"location":"#versioning","title":"Versioning","text":"<p>Versioning for the project is performed by the semantic-release package. When a pull request is merged into the <code>main</code> branch, the package version will be automatically updated based on the squashed commit message from the PR title.</p> <p>Commits prefixed with <code>fix:</code> will trigger a patch version update, <code>feat:</code> will trigger a minor version update, and <code>BREAKING CHANGE:</code> will trigger a major version update. Note <code>BREAKING CHANGE:</code> needs to be in the commit body/footer as detailed here. All other commit prefixes will trigger no version update. PR titles should therefore be prefixed accordingly.</p>"},{"location":"#contact","title":"Contact","text":"<p>For any questions or concerns please reach out to the team.</p>"},{"location":"achieving_shape_parity/","title":"Shape Parity in Kamae","text":"<p>Traditionally, the Spark DataFrame API had poor support for nested data structures, such as arrays and maps. Nowadays, Spark has improved its support for nested data structures, but it is still not as flexible as Tensorflow.</p> <p>In order to improve the compatibility between Spark and Tensorflow, Kamae provides a set of utils for transforming nested arrays that are used in all Spark-side transformers.</p> <p>This allows the user to maintain shape parity between the two frameworks.</p>"},{"location":"achieving_shape_parity/#how-to-achieve-shape-parity","title":"How to achieve shape parity","text":"<p>You can now have features of any level of nesting in your schema, provided they are only arrays. Not maps or structs. </p> <p>All transformers and estimators now natively support nested inputs, and can operate elementwise or across the full array in accordance with how  Tensorflow would operate on the same data.</p>"},{"location":"achieving_shape_parity/#restrictions","title":"Restrictions","text":"<ul> <li>The nested arrays must be homogenous. That is, all elements of the array must be of the same type.</li> <li>In the case of multiple input transformers, the nested arrays must be of the same size, or be a scalar. Scalars are broadcast to the size of the other array(s).</li> <li>The operation is performed either elementwise (in the case of a simple transform such as <code>log(x + 1)</code>) or across the innermost array (<code>axis=-1</code> in Tensorflow).</li> </ul>"},{"location":"achieving_type_parity/","title":"Type Parity in Kamae","text":"<p>By default, Spark and Tensorflow have very different behaviours when it comes to the data types of the outputs of their transforms.</p> <p>For example, computing <code>pyspark.sql.functions.log(x)</code> will always return a <code>DoubleType</code> column, even if <code>x</code> is an <code>IntegerType</code> or <code>FloatType</code> column. On the other hand, Tensorflow often attempts to maintain input and output datatypes, and so if you pass a <code>tf.float32</code> tensor into a <code>tf.math.log</code> operation, you will get a <code>tf.float32</code> tensor out.</p> <p>This inconsistency can cause issues if you plan to write the output of your Kamae pipeline to TFRecords for later training stages. In these cases you could be left with a mismatch between the expected and actual data types of your features.</p>"},{"location":"achieving_type_parity/#how-to-achieve-type-parity","title":"How to achieve type parity","text":"<p>Kamae provides <code>inputDtype</code> and <code>outputDtype</code> parameters to every Spark transformer/estimator that will cast  the inputs and outputs to the specified data type. This is mimicked on the Tensorflow side, so that casting is done in a consistent manner. These can be used to ensure:</p> <ol> <li>The input datatype is a compatible datatype for the transformer/estimator.</li> <li>For example the <code>LogTransformer</code> can only operate on floating types, but by specifying <code>inputDtype=\"float\"</code> you could pass in an <code>IntegerType</code> column and it would be cast to a <code>FloatType</code> column before the log operation is applied.</li> <li>The output datatype is a compatible datatype for the next stage of the pipeline.</li> </ol>"},{"location":"achieving_type_parity/#pitfalls","title":"Pitfalls","text":"<p>There is one special case where even providing <code>inputDtype</code> and <code>outputDtype</code> may still not achieve type parity. In the case where you intend to return a <code>string</code> from a numerical operation, setting <code>outputDtype=\"string\"</code> can have unexpected results.</p> <ul> <li>Casting a double to string in Tensorflow currently only preserves 6 significant figures, which can lead to loss of precision. In Spark all decimal places are preserved.</li> <li>Casting an integer to a string is different to casting a float (of the same integer value) to a string.</li> <li>For example, casting <code>1</code> to a string will return <code>\"1\"</code>. Casting <code>1.0</code> to a string will return <code>\"1.0\"</code>.</li> </ul> <p>Some operations in Spark always return DoubleType, whereas in Tensorflow they can return integers if the inputs are integers. If you then set the outputDtype to \"string\" you will get different results from the two frameworks.</p> <p>In these cases, it is recommended to set <code>outputDtype</code> to some intermediary numerical type (e.g. <code>float</code>) and then cast to string in a separate step. This separate step can be done using the <code>IdentityTransformer</code> with <code>outputDtype=\"string\"</code>.</p> <p>Lastly, it is worth noting that setting <code>inputDtype</code> and <code>outputDtype</code> will add some small overhead/latency to your resulting keras model, since we will perform a casting operation on the inputs. However this overhead should be minimal in comparison to the rest of the operations in your pipeline. </p>"},{"location":"achieving_type_parity/#rules","title":"Rules","text":"<p>Consolidating all the above into a set of rules to achieve type parity:</p> <ol> <li>Always set <code>outputDtype</code> on every transformer/estimator.</li> <li>If you require a string output from a numerical operation, set <code>outputDtype</code> to an intermediary numerical type and then cast to string in a separate step (using the <code>IdentityTransformer</code> with <code>outputDtype=\"string\"</code>).</li> </ol>"},{"location":"adding_transformer/","title":"Contributing a Keras layer and Spark/Scikit-learn transformer","text":"<p>Follow this guide to contribute a new transformer to the project.</p>"},{"location":"adding_transformer/#overview","title":"Overview","text":"<p>In order to contribute a new transformer, you will need to implement a Spark Transformer, a corresponding Keras layer, and a Spark Estimator if your transformer needs a fit method. We also require unit tests for all new classes, in particular parity tests ensuring your Spark Transformer and Keras layer produce the same output.</p> <p>You may wish to also implement a Scikit-learn transformer, however we deem the scikit-learn usage pattern to be experimental for now and so this is not required.</p>"},{"location":"adding_transformer/#naming","title":"Naming","text":"<p>In order to avoid name clashes and to keep consistency, we have a naming convention for all new classes.</p> <p>If an operation is called <code>&lt;X&gt;</code> then:</p> <ul> <li><code>&lt;X&gt;Estimator</code> = Spark estimator (if applicable)</li> <li><code>&lt;X&gt;Transformer</code> = Spark transformer</li> <li><code>&lt;X&gt;Layer</code> = Tensorflow/Keras layer</li> <li><code>&lt;X&gt;Params</code> = Spark params class</li> </ul> <p>We just keep the verb stem. E.g string indexing is StringIndexTransformer, not StringIndexerTransformer.</p> <p>The name of the file should then be <code>&lt;X&gt;.py</code>. E.g. <code>src/kame/spark/transformers/string_index.py</code> and <code>src/kame/tensorflow/layers/string_index.py</code>.</p> <p>Finally, if you need to create an estimator, then the estimator and its corresponding transformer should be in different files. E.g. <code>src/kame/spark/transformers/string_index.py</code> and <code>src/kame/spark/estimators/string_index.py</code>.</p>"},{"location":"adding_transformer/#keras-layer","title":"Keras layer","text":"<p>Your Keras layer should extend BaseLayer and implement the <code>_call</code> method. Furthermore, you will need to define the <code>compatible_dtypes</code> property which should return a list of compatible dtypes for the layer (or <code>None</code> if the layer is compatible with all dtypes). You should ensure your layer is serializable by implementing the <code>get_config</code> method.  You also need to add the decorator <code>@tf.keras.utils.register_keras_serializable(package=kamae.__name__)</code> to the class.</p>"},{"location":"adding_transformer/#example","title":"Example","text":"<pre><code>from typing import List, Optional\n\nimport tensorflow as tf\nimport kamae\n\nfrom .base import BaseLayer\n\n@tf.keras.utils.register_keras_serializable(package=kamae.__name__)\nclass MyLayer(BaseLayer):\n    def __init__(self, name, input_dtype, output_dtype, my_param, **kwargs):\n        # Ensure that the name, input_dtype, and output_dtype are passed to the super constructor\n        super().__init__(name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs)\n        self.my_param = my_param\n\n    @property\n    def compatible_dtypes(self) -&gt; Optional[List[tf.DType]]:\n        return [tf.float32, tf.float64]\n\n    def _call(self, inputs):\n        # do something with inputs\n        return outputs\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({'my_param': self.my_param})\n        return config\n</code></pre>"},{"location":"adding_transformer/#checklist","title":"Checklist","text":"<ul> <li> I have implemented a Keras layer that extends BaseLayer</li> <li> I have implemented the <code>_call</code> method of my Keras layer.</li> <li> I have defined the <code>compatible_dtypes</code> property of my Keras layer.</li> <li> I have added the decorator <code>@tf.keras.utils.register_keras_serializable(package=kamae.__name__)</code> to my Keras layer.</li> <li> I have ensured that my layer takes a <code>name</code>, <code>input_dtype</code>, and <code>output_dtype</code> as arguments to the constructor and that this is passed to the super constructor.</li> <li> My Keras layer is serializable. I have implemented the <code>get_config</code> method and added the decorator seen above to the class.</li> <li> I have unit tests of my implementation. </li> <li> I have a specific test of layer serialisation added here.</li> </ul>"},{"location":"adding_transformer/#spark-transformerestimator","title":"Spark Transformer/Estimator","text":"<p>Your Spark Transformer should extend BaseTransformer.  In this it should implement the <code>get_tf_layer</code> method, which should return an instance of your Keras layer. If your transformer needs a fit method, you should also implement a Spark Estimator (which extends BaseEstimator) whose fit method returns an instance of your transformer.</p> <p>Spark has a peculiar way of building constructors, in that the <code>__init__</code> calls a <code>setParams</code> method, which sets the parameters of the transformer. See the example below for how this works. All estimators and transformers follow this boilerplate code. The <code>setParams</code> method is implemented in the base transformer and estimator classes, so you do not need to implement it yourself. However, you do need to call it from your <code>__init__</code> method, as shown below. You also need to ensure that all custom parameters have a setter method, which is in the form <code>set&lt;ParamName&gt;</code>, as the <code>setParams</code> method will look for this method.</p> <p>Your transformer should use one (or more) of the input/output mixin classes from base.py - <code>SingleInputSingleOutputParams</code> - <code>SingleInputMultiOutputParams</code> - <code>MultiInputSingleOutputParams</code> - <code>MultiInputMultiOutputParams</code></p> <p>Only use more than one if you want to support two usages of your transformer, e.g. <code>MyTransformer(inputCol=\"a\", outputCol=\"b\")</code> and <code>MyTransformer(inputCols=[\"a\", \"b\"], outputCols=[\"c\", \"d\"])</code>. See for example the SumTransformer, which supports single input with a constant to add, or multiple inputs to sum.</p> <p>These mixins provide the <code>inputCol(s)</code> and <code>outputCol(s)</code> parameters, which are used to specify the input and output columns of your transformer.</p> <p>If your transformer requires more parameters that would need to be serialised to the Spark ML pipeline, you should add create a parameter class by extending the <code>Params</code> class here.</p> <p>Lastly, we have provided utils for transformers &amp; estimators to natively transform nested Spark array columns.  You can use one of the following functions from here according to your usecase if you need to add support for nested columns:</p> <ul> <li><code>single_input_single_output_scalar_transform</code></li> <li><code>single_input_single_output_array_transform</code></li> <li><code>single_input_single_output_scalar_udf_transform</code></li> <li><code>single_input_single_output_array_udf_transform</code></li> <li><code>multi_input_single_output_scalar_transform</code></li> <li><code>multi_input_single_output_array_transform</code></li> </ul>"},{"location":"adding_transformer/#example_1","title":"Example","text":"<p>Note that the methods are named <code>_fit</code> and <code>_transform</code>. <code>fit</code> and <code>transform</code> wrap these internal methods and should not be overridden.</p> <pre><code>from typing import List, Optional\n\nfrom pyspark import keyword_only\nfrom pyspark.ml.param import Param, Params, TypeConverters\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.types import DataType, StringType, BinaryType\n\nfrom kamae.spark.params import SingleInputSingleOutputParams\nfrom kamae.spark.transformers import BaseTransformer\nfrom kamae.spark.estimators import BaseEstimator\n\n\nclass MyCustomParams(Params):\n    myParam = Param(\n        Params._dummy(),\n        \"myParam\",\n        \"Description of myParam\",\n        typeConverter=TypeConverters.toFloat,\n    )\n\n    # Setter method must be in the form set&lt;ParamName&gt; otherwise \n    # the setParams method will not find the set method. \n    def setMyParam(self, value: float) -&gt; \"MyCustomParams\":\n        return self._set(myParam=value)\n\n    def getMyParam(self) -&gt; float:\n        return self.getOrDefault(self.myParam)\n\n\nclass MyEstimator(\n    BaseEstimator,\n    SingleInputSingleOutputParams,\n    MyCustomParams\n):\n\n    @keyword_only\n    def __init__(\n            self,\n            inputCol: Optional[str] = None,\n            outputCol: Optional[str] = None,\n            layerName: Optional[str] = None,\n            inputDtype: Optional[str] = None,\n            outputDtype: Optional[str] = None,\n            myParam: Optional[float] = None,\n    ) -&gt; None:\n        super().__init__()\n        kwargs = self._input_kwargs\n        self.setParams(**kwargs)\n\n    @property\n    def compatible_dtypes(self) -&gt; Optional[List[DataType]]:\n        return [StringType(), BinaryType()]\n\n    def _fit(self, dataset: DataFrame) -&gt; \"MyTransformer\":\n        # Do some fitting...\n        return MyTransformer(\n            inputCol=self.getInputCol(),\n            outputCol=self.getOutputCol(),\n            layerName=self.getLayerName(),\n            inputDtype=self.getInputDtype(),\n            outputDtype=self.getOutputDtype(),\n            myParam=self.getMyParam(),\n        )\n\n\nclass MyTransformer(\n    BaseTransformer,\n    SingleInputSingleOutputParams,\n    MyCustomParams\n):\n\n    @keyword_only\n    def __init__(\n            self,\n            inputCol: Optional[str] = None,\n            outputCol: Optional[str] = None,\n            layerName: Optional[str] = None,\n            inputDtype: Optional[str] = None,\n            outputDtype: Optional[str] = None,\n            myParam: Optional[float] = None,\n    ) -&gt; None:\n        super().__init__()\n        kwargs = self._input_kwargs\n        self.setParams(**kwargs)\n\n    @property\n    def compatible_dtypes(self) -&gt; Optional[List[DataType]]:\n        return [StringType(), BinaryType()]\n\n    def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n        # Ensure that the layer has the layer name, input dtype, and output dtype\n        # as arguments `name`, `input_dtype`, and `output_dtype` respectively.\n        return MyLayer(\n            name=self.getLayerName(),\n            input_dtype=self.getInputTFDtype(),\n            out_dtype=self.getOutputTFDtype(),\n            my_param=self.getMyParam(),\n        )\n\n    def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n        # Do some transformation...\n        return dataset.withColumn(\n            self.getOutputCol(),\n            output_of_transform,\n        )\n</code></pre>"},{"location":"adding_transformer/#checklist_1","title":"Checklist","text":"<ul> <li> I have implemented a Spark Transformer that extends BaseTransformer.</li> <li> If my transformer needs a fit method, I have implemented a Spark Estimator that extends BaseEstimator.</li> <li> I have followed the instructions for the <code>__init__</code> and <code>setParams</code> methods.</li> <li> I have used one (or more) of the input/output mixin classes from base.py.</li> <li> If my transformer requires more parameters that would need to be serialised to the Spark ML pipeline, I have added a parameter class by extending the <code>Params</code> class here.</li> <li> I have defined the <code>compatible_dtypes</code> property to specify the input/output data types that my transformer/estimator supports.</li> <li> I used a Keras subclassed layer for my <code>get_tf_layer</code> method.</li> <li> I have unit tests of my implementation. In particular, I have parity tests between the Spark and Keras implementations.</li> </ul>"},{"location":"adding_transformer/#scikit-learn-transformerestimator","title":"Scikit-learn Transformer/Estimator","text":"<p>If your transformer is a wrapper around an existing Scikit-Learn transformer, you should also extend the BaseTransformerMixin class. This will provide the required functionality to be incorporated into the Kamae framework. </p> <p>If you are writing a custom transformer, you should extend the BaseTransformer class. The only difference between these classes is that the <code>BaseTransformer</code> class also extends the <code>BaseEstimator</code> and <code>TransformerMixin</code> classes from scikit-learn. If you are wrapping an existing transformer, these are already extended by the transformer you are wrapping. See the StandardScaleEstimator for an example of a wrapper around an existing transformer. See the IdentityTransformer for an example of a custom transformer.</p> <p>Additionally, your transformer should use one (or more) of the input/output mixin classes from base.py - SingleInputSingleOutputMixin - SingleInputMultiOutputMixin - MultiInputSingleOutputMixin - MultiInputMultiOutputMixin</p> <p>Only use more than one if you want to support two usages of your transformer. We have no scikit-learn examples of this yet, only Spark. The behaviour is the same. See above to the Spark section to understand why you may want to do this.</p> <p>In scikit-learn, everything is an estimator. If your transformer does not require a fit method, just return <code>self</code> from the <code>fit</code> method. If your transformer does require a fit method, you should implement it within the <code>fit</code> method of your transformer.</p>"},{"location":"adding_transformer/#example_2","title":"Example","text":"<pre><code>import pandas as pd\nimport tensorflow as tf\nfrom kamae.sklearn.params import SingleInputSingleOutputMixin\nfrom kamae.sklearn.transformers import BaseTransformer\n\nclass MyTransformer(\n    BaseTransformer, SingleInputSingleOutputMixin\n):\n    def __init__(self, input_col: str, output_col: str, layer_name: str) -&gt; None:\n        super().__init__()\n        self.input_col = input_col\n        self.output_col = output_col\n        self.layer_name = layer_name\n\n    def fit(self, X: pd.DataFrame, y=None) -&gt; \"MyTransformer\":\n        return self\n\n    def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        X[self.output_col] = output_of_transform\n        return X\n\n    def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n        return MyLayer(\n            name=self.layer_name,\n        )\n</code></pre>"},{"location":"adding_transformer/#checklist_2","title":"Checklist","text":"<ul> <li> I have implemented a Scikit-learn Transformer that extends BaseTransformer (if custom) or BaseTransformerMixin (if wrapping an existing transformer).</li> <li> If my transformer needs a fit method, I have implemented it within the <code>fit</code> method of my transformer.</li> <li> I have used one (or more) of the input/output mixin classes from base.py.</li> <li> I used a Keras subclassed layer for my <code>get_tf_layer</code> method.</li> <li> I have unit tests of my implementation. In particular, I have parity tests between the scikit-learn and Keras implementations.</li> </ul>"},{"location":"chaining_models/","title":"Model Chaining: Combining Kamae and Trained Keras Models","text":"<p>Below we will explain how you can combine your Kamae pre/post processing model with a trained Keras model.</p>"},{"location":"chaining_models/#kamae-processing-models","title":"Kamae Processing Models","text":"<p>When building your Kamae model from the pipeline interfaces, you will call the <code>build_keras_model</code> method on the pipeline object. This method will return a Keras model that you can use to process your data.</p>"},{"location":"chaining_models/#accessing-model-inputs","title":"Accessing model inputs","text":"<p>The way in which you specify the <code>tf_input_schema</code> to this method can influence how you access your model inputs.</p>"},{"location":"chaining_models/#1-list-of-dictionary-config","title":"1. List of dictionary config.","text":"<p>This is the standard way of specifying the <code>tf_input_schema</code>.  In this case, you would pass the <code>tf_input_schema</code> as a list of dictionaries, where each dictionary specifies (at least) the name, shape and type of the input. These dictionaries will be passed directly into the <code>tf.keras.layers.Input</code> via ** kwargs, and so the names of the arguments will be the keys specified in the dictionary.</p> <p>In this case, when accessing your model inputs, you can use the <code>inputs</code> attribute of the model, which is a list of <code>tf.keras.Input</code> objects. You can access the <code>name</code> attribute of each of these objects to get the name of the input. These will match the names specified in the <code>tf_input_schema</code> dictionary.</p>"},{"location":"chaining_models/#2-list-of-tftypespec","title":"2. List of tf.TypeSpec.","text":"<p>If you have more complex inputs (e.g. a <code>RaggedTensor</code>) then you may find using <code>tf.TypeSpec</code> objects easier. In this case, you would pass the <code>tf_input_schema</code> as a list of <code>tf.TypeSpec</code> objects. Under the hood, these will be passed to the <code>tf.keras.layers.Input</code> via the <code>typespec</code> argument.</p> <p>However, in this case, accessing the inputs of your model via the <code>inputs</code> attribute will return inputs with missing names (i.e. <code>None</code>). This is detailed in this GitHub issue.</p> <p>In order to fix this you will need to zip the <code>input_names</code> attribute of your model with the <code>inputs</code> attribute, to assign the names to the inputs.</p> <pre><code>inputs_with_names = list(zip(model.input_names, model.inputs))\n</code></pre>"},{"location":"chaining_models/#accessing-model-outputs","title":"Accessing model outputs","text":"<p>With your Kamae model, you can access the outputs of the model via the <code>outputs</code> attribute of the model. We add an Identity layer to each output (to preserve the name of the output), but this means that the <code>name</code> attribute of the output will be <code>&lt;OUTPUT_NAME&gt;/Identity:0</code>.</p> <p>Therefore, you can either split these strings, or zip the <code>output_names</code> attribute of your model with the <code>outputs</code> attribute, to assign the names to the outputs.</p>"},{"location":"chaining_models/#combining-your-kamae-processing-model-with-a-trained-keras-model","title":"Combining your Kamae processing model with a trained Keras model","text":""},{"location":"chaining_models/#preprocessing-example","title":"Preprocessing example","text":"<p>Assuming we have two models, <code>prepro_model</code> and <code>trained_model</code> which we want to chain together, we can do the following:</p> <pre><code>import tensorflow as tf\n\n# Get the inputs of the prepro model\nprepro_inputs = prepro_model.inputs\n\n# If you need to access the names of the inputs, you can do the following\nprepro_inputs_dict = {\n    input_name: input \n    for input_name, input in zip(prepro_model.input_names, prepro_model.inputs)\n}\n\n# Get the outputs of the prepro model as a dictionary\nprepro_outputs_dict = {\n    output_name: output \n    for output_name, output in zip(prepro_model.output_names, prepro_model.outputs)\n}\n\n# Apply trained model to prepro outputs\ncombined_outputs = trained_model(prepro_outputs_dict)\n\n# Create a new model with the prepro inputs and combined outputs\ncombined_model = tf.keras.Model(inputs=prepro_inputs, outputs=combined_outputs)\n</code></pre>"},{"location":"chaining_models/#postprocessing-example","title":"Postprocessing example","text":"<p>Postprocessing works in a very similar way, you just change which model is applied to the other:</p> <pre><code>import tensorflow as tf\n\n# Get the inputs of the trained model\ntrained_inputs = trained_model.inputs\n\n# If you need to access the names of the inputs, you can do the following\ntrained_inputs_dict = {\n    input_name: input \n    for input_name, input in zip(trained_model.input_names, trained_model.inputs)\n}\n\n# Get the outputs of the trained model as a dictionary\ntrained_outputs_dict = {\n    output_name: output \n    for output_name, output in zip(trained_model.output_names, trained_model.outputs)\n}\n\n# Apply postpro model to trained outputs\ncombined_outputs = postpro_model(trained_outputs_dict)\n\n# Create a new model with the trained inputs and combined outputs\ncombined_model = tf.keras.Model(inputs=trained_inputs, outputs=combined_outputs)\n</code></pre>"},{"location":"testing_inference/","title":"Testing Inference","text":"<p>You can use the example script here and the make command <code>make test-tf-serving</code> to test the inference of a model served by TensorFlow Serving.</p> <p>The make command spins up a tensorflow serving Docker container and the script sends a request to the server to get a prediction.</p> <ul> <li>Depending on your model you will need to edit the inputs to the REST call in the script.</li> <li>If you are on an M1/M2 chip then you need to ensure you have turned off <code>Rosetta</code> in Docker Desktop settings as seen below:</li> </ul> <p>An E2E example can be run by using the <code>make test-end-to-end</code> command, hitting enter where necessary to accept the defaults.</p> <p></p>"},{"location":"reference/src/kamae/","title":"kamae","text":"<p>kamae</p> <p>kamae is a Python package comprising a set of reusable Keras transformation layers.</p>"},{"location":"reference/src/kamae/graph/","title":"graph","text":""},{"location":"reference/src/kamae/graph/pipeline_graph/","title":"pipeline_graph","text":""},{"location":"reference/src/kamae/graph/pipeline_graph/#src.kamae.graph.pipeline_graph.PipelineGraph","title":"PipelineGraph","text":"<pre><code>PipelineGraph(stage_dict)\n</code></pre> <p>PipelineGraph is a class that constructs a graph of the pipeline stages. This is used to determine the order in which the layers should be constructed. The graph is built by adding edges between layers that have the same input column as the output column of a previous layer. If the input is not an output of any other layer, then it is assumed to be an input to the pipeline.</p> <p>The graph is then topologically sorted to determine the order in which the layers should be constructed. Iterating through this order, the layers are constructed by calling the get_tf_layer method of each stage. The inputs to the layer are determined by the outputs of the previous layers.</p> <p>:param stage_dict: Dictionary of stages to add to the graph. :returns: None - class instance is initialized.</p> Source code in <code>src/kamae/graph/pipeline_graph.py</code> <pre><code>def __init__(self, stage_dict: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Initialize the PipelineGraph class with a dictionary of stage information.\n\n    :param stage_dict: Dictionary of stages to add to the graph.\n    :returns: None - class instance is initialized.\n    \"\"\"\n    self.stage_dict = stage_dict\n    self.graph = self.add_stage_edges(nx.DiGraph())\n    self.transform_order = [\n        node for node in nx.topological_sort(self.graph) if node in self.stage_dict\n    ]\n\n    # We keep a dictionary of layers to keep track of which have been reused.\n    # This allows us to easily get the output layers\n    self.layer_store = {}\n    self.inputs = {}\n</code></pre>"},{"location":"reference/src/kamae/graph/pipeline_graph/#src.kamae.graph.pipeline_graph.PipelineGraph.add_stage_edges","title":"add_stage_edges","text":"<pre><code>add_stage_edges(graph)\n</code></pre> <p>Add edges to the graph based on the inputs and outputs of each stage. Specifically for each stage, we connect the inputs of a stage to itself and the stage to its outputs.</p> <p>:param graph: NetworkX DAG to add edges to. :returns: Graph with edges added.</p> Source code in <code>src/kamae/graph/pipeline_graph.py</code> <pre><code>def add_stage_edges(self, graph: nx.DiGraph) -&gt; nx.DiGraph:\n    \"\"\"\n    Add edges to the graph based on the inputs and outputs of each stage.\n    Specifically for each stage, we connect the inputs of a stage to itself and the\n    stage to its outputs.\n\n    :param graph: NetworkX DAG to add edges to.\n    :returns: Graph with edges added.\n    \"\"\"\n    edges_to_add = []\n    for layer_name, layer_info in self.stage_dict.items():\n        # Add edges for all inputs\n        edges_to_add.extend(\n            [(input_name, layer_name) for input_name in layer_info[\"inputs\"]]\n        )\n        # Add edges for all outputs\n        edges_to_add.extend(\n            [(layer_name, output_name) for output_name in layer_info[\"outputs\"]]\n        )\n\n    graph.add_edges_from(edges_to_add)\n    return graph\n</code></pre>"},{"location":"reference/src/kamae/graph/pipeline_graph/#src.kamae.graph.pipeline_graph.PipelineGraph.build_keras_inputs","title":"build_keras_inputs","text":"<pre><code>build_keras_inputs(tf_input_schema)\n</code></pre> <p>Builds a Keras input layer for the given node.</p> <p>Specifically, we get a single node from the out edges of the current node. The out edges are a tuple of edges <code>(u, v)</code> where <code>u</code> is the current node and <code>v</code> is a node takes the current node (<code>u</code>) as input.</p> <p>Using <code>v</code> we can get the input dimension and type and therefore construct the keras input layer. We then store this layer as an input and update the layer store.</p> <p>:param tf_input_schema: List of tf.TypeSpec objects containing the input schema for the model or a list of dict config to be passed to the Input constructor. :returns: None - layer store is updated and input layer is stored in the inputs dict.</p> Source code in <code>src/kamae/graph/pipeline_graph.py</code> <pre><code>def build_keras_inputs(\n    self, tf_input_schema: Union[List[tf.TypeSpec], List[Dict[str, Any]]]\n) -&gt; None:\n    \"\"\"\n    Builds a Keras input layer for the given node.\n\n    Specifically, we get a single node from the out edges of the current node.\n    The out edges are a tuple of edges `(u, v)` where `u` is the current node and\n    `v` is a node takes the current node (`u`) as input.\n\n    Using `v` we can get the input dimension and type and therefore construct the\n    keras input layer. We then store this layer as an input and update the\n    layer store.\n\n    :param tf_input_schema: List of tf.TypeSpec objects containing the input schema\n    for the model or a list of dict config to be passed to the Input constructor.\n    :returns: None - layer store is updated and input layer is stored in the\n    inputs dict.\n    \"\"\"\n\n    if isinstance(tf_input_schema, list) and all(\n        isinstance(x, tf.TypeSpec) for x in tf_input_schema\n    ):\n        if keras_version &gt;= Version(\"3.0.0\"):\n            raise ValueError(\n                \"tf.TypeSpec is not supported in Keras 3, please use a dict config\"\n            )\n        input_config = [\n            {\n                \"name\": spec.name,\n                \"type_spec\": spec,\n            }\n            for spec in tf_input_schema\n        ]\n    elif isinstance(tf_input_schema, list) and all(\n        isinstance(x, dict) for x in tf_input_schema\n    ):\n        input_config = tf_input_schema\n    else:\n        raise ValueError(\"tf_input_schema must be a list of tf.TypeSpec or dict!\")\n\n    for conf in input_config:\n        name = conf.get(\"name\", None)\n        if name is None:\n            raise ValueError(\n                \"Input schema must have names for all inputs, but found None\"\n            )\n        input_layer = tf.keras.layers.Input(**conf)\n        self.inputs[name] = input_layer\n        self.update_layer_store_with_key(layer_key=name, layer_output=input_layer)\n</code></pre>"},{"location":"reference/src/kamae/graph/pipeline_graph/#src.kamae.graph.pipeline_graph.PipelineGraph.build_keras_model","title":"build_keras_model","text":"<pre><code>build_keras_model(tf_input_schema, output_names=None)\n</code></pre> <p>Builds a Keras model from the graph.</p> <p>:param tf_input_schema: List of tf.TypeSpec objects containing the input schema for the model. Each TypeSpec object must define a unique <code>name</code> attribute. These will be passed as is to the Keras Input layer. :param output_names: Optional list of output names for the Keras model. If provided, only the outputs specified are used as model outputs. :returns: Keras model to be applied to a tensors dictionary: {name: Tensor}.</p> Source code in <code>src/kamae/graph/pipeline_graph.py</code> <pre><code>def build_keras_model(\n    self,\n    tf_input_schema: Union[List[tf.TypeSpec], List[Dict[str, Any]]],\n    output_names: Optional[List[str]] = None,\n) -&gt; tf.keras.Model:\n    \"\"\"\n    Builds a Keras model from the graph.\n\n    :param tf_input_schema: List of tf.TypeSpec objects containing the input schema\n    for the model. Each TypeSpec object must define a unique `name` attribute.\n    These will be passed as is to the Keras Input layer.\n    :param output_names: Optional list of output names for the Keras model. If\n    provided, only the outputs specified are used as model outputs.\n    :returns: Keras model to be applied to a tensors dictionary: {name: Tensor}.\n    \"\"\"\n    # Build the input layers\n    self.build_keras_inputs(tf_input_schema=tf_input_schema)\n\n    for node in self.transform_order:\n        in_edges = list(self.graph.in_edges(node))\n        self.build_keras_transform_layer(node=node, in_edges=in_edges)\n\n    # All the layers are now stored in the layer store,\n    # with all inputs/outputs specified.\n    # We can now build the model by specifying the inputs and outputs.\n    sorted_inputs = {k: self.inputs[k] for k in sorted(self.inputs)}\n    return tf.keras.Model(\n        inputs=sorted_inputs,\n        outputs=self.get_model_outputs(output_names=output_names),\n    )\n</code></pre>"},{"location":"reference/src/kamae/graph/pipeline_graph/#src.kamae.graph.pipeline_graph.PipelineGraph.build_keras_transform_layer","title":"build_keras_transform_layer","text":"<pre><code>build_keras_transform_layer(\n    node, in_edges, hp_override=None\n)\n</code></pre> <p>Builds a Keras transformation layer for the given node. Gets the layer inputs using the in edges and then applies the layer to the inputs. Updates the layer store.</p> <p>:param node: Current node (name of layer). :param in_edges: List of in edges for the current node. :param hp_override: Optional dictionary of hyperparameters to override. Used for building Keras tuner model builder functions. :returns: None - layer store is updated.</p> Source code in <code>src/kamae/graph/pipeline_graph.py</code> <pre><code>def build_keras_transform_layer(\n    self,\n    node: str,\n    in_edges: List[Tuple[str, str]],\n    hp_override: Dict[str, Any] = None,\n) -&gt; None:\n    \"\"\"\n    Builds a Keras transformation layer for the given node.\n    Gets the layer inputs using the in edges and then applies the layer to\n    the inputs. Updates the layer store.\n\n    :param node: Current node (name of layer).\n    :param in_edges: List of in edges for the current node.\n    :param hp_override: Optional dictionary of hyperparameters to override.\n    Used for building Keras tuner model builder functions.\n    :returns: None - layer store is updated.\n    \"\"\"\n    layer_inputs = self.build_transform_layer_inputs(node=node, in_edges=in_edges)\n    layer = self.stage_dict[node][\"layer\"]\n    layer = self.override_hyperparameters(layer=layer, hp_override=hp_override)\n\n    if isinstance(layer, list):\n        # If we have a list of layers, we assume that each layer needs to operate\n        # on the corresponding input idx in the list of inputs.\n        layer_outputs = [\n            layer_elem(layer_input)\n            for layer_elem, layer_input in zip(layer, layer_inputs)\n        ]\n\n    else:\n        # If we have a single layer, we assume that it needs to operate on all\n        # the inputs.\n        layer_outputs = (\n            layer(layer_inputs) if len(layer_inputs) &gt; 1 else layer(*layer_inputs)\n        )\n\n    layer_output_names = self.stage_dict[node][\"outputs\"]\n    # Make layer outputs a list if it isn't already\n    layer_outputs = (\n        layer_outputs if isinstance(layer_outputs, list) else [layer_outputs]\n    )\n\n    # Zip the output names with the ouputs themselves\n    layer_outputs_with_name = {\n        layer_output_name: layer_output\n        for layer_output_name, layer_output in zip(\n            layer_output_names, layer_outputs\n        )\n    }\n\n    self.update_layer_store(layer_dict=layer_outputs_with_name)\n</code></pre>"},{"location":"reference/src/kamae/graph/pipeline_graph/#src.kamae.graph.pipeline_graph.PipelineGraph.build_transform_layer_inputs","title":"build_transform_layer_inputs","text":"<pre><code>build_transform_layer_inputs(node, in_edges)\n</code></pre> <p>Constructs all the layers that are connected to the current node. These are either input layers or the outputs of previous layers.</p> <p>The in edges are a tuple of edges <code>(u, v)</code> where <code>v</code> is the current node and <code>u</code> is a node that is an input to the current node (<code>v</code>).</p> <p>Using <code>u</code> we can tell if this is the output of another transformation or an input layer. If it's an input layer, we retrieve it from the inputs dictionary. If it's the output of another transformation, we retrieve it from the layer store.</p> <p>:param node: Current node (name of layer). :param in_edges: List of in edges for the current node. :returns: List of layer inputs for the current node/layer.</p> Source code in <code>src/kamae/graph/pipeline_graph.py</code> <pre><code>def build_transform_layer_inputs(\n    self, node: str, in_edges: List[Tuple[str, str]]\n) -&gt; List[tf.Tensor]:\n    \"\"\"\n    Constructs all the layers that are connected to the current node.\n    These are either input layers or the outputs of previous layers.\n\n    The in edges are a tuple of edges `(u, v)` where `v` is the current node and\n    `u` is a node that is an input to the current node (`v`).\n\n    Using `u` we can tell if this is the output of another transformation or an\n    input layer. If it's an input layer, we retrieve it from the inputs dictionary.\n    If it's the output of another transformation, we retrieve it from the\n    layer store.\n\n    :param node: Current node (name of layer).\n    :param in_edges: List of in edges for the current node.\n    :returns: List of layer inputs for the current node/layer.\n    \"\"\"\n    # Here we get all layer outputs that are connected to this node.\n    # We need these so we can apply the current node's layer to\n    # the output of the previous layers.\n    # Since we topologically sorted the nodes,\n    # all previous layers will have already been created.\n\n    # Get the in edge node names\n    in_edge_node_names = [in_edge[0] for in_edge in in_edges]\n    # For each in edge, find the output that maps to this node.\n    layer_output_from_in_edge = [\n        (stage_name, in_edge_node_name)\n        for in_edge_node_name in in_edge_node_names\n        for stage_name, stage_info in self.stage_dict.items()\n        if in_edge_node_name in stage_info[\"outputs\"]\n    ]\n    # Get any input layers that are connected to the node via the in edges\n    input_layers_from_in_edge = {\n        name: layer\n        for name, layer in self.inputs.items()\n        if name in in_edge_node_names\n    }\n\n    # For each layer output in edge,\n    # get its corresponding layer output from the layer store.\n    # Update the store to indicate we have reused this layer.\n    # Thus, it is not an output layer\n    in_edge_layers_inputs = {}\n    for name, output_name in layer_output_from_in_edge:\n        layer = self.get_layer_output_from_layer_store(output_name)\n        self.update_layer_store_with_key(layer_key=output_name, layer_output=layer)\n        in_edge_layers_inputs[output_name] = layer\n\n    # Sort the inputs according to the order set in the Spark transformers\n    input_dict = {**in_edge_layers_inputs, **input_layers_from_in_edge}\n    layer_inputs = self.sort_inputs(\n        layer_name=node,\n        input_dict=input_dict,\n    )\n    return layer_inputs\n</code></pre>"},{"location":"reference/src/kamae/graph/pipeline_graph/#src.kamae.graph.pipeline_graph.PipelineGraph.get_keras_hyperparam_from_config","title":"get_keras_hyperparam_from_config  <code>staticmethod</code>","text":"<pre><code>get_keras_hyperparam_from_config(hp, config)\n</code></pre> <p>Returns a keras hyperparameter object from a config dictionary.</p> <p>:param hp: keras_tuner.HyperParameters object passed through from the keras tuner model builder function. :param config: Config supplied by the user. :returns: keras_tuner.HyperParameters method for the given config.</p> Source code in <code>src/kamae/graph/pipeline_graph.py</code> <pre><code>@staticmethod\ndef get_keras_hyperparam_from_config(\n    hp: keras_tuner.HyperParameters, config: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Returns a keras hyperparameter object from a config dictionary.\n\n    :param hp: keras_tuner.HyperParameters object passed through from the\n    keras tuner model builder function.\n    :param config: Config supplied by the user.\n    :returns: keras_tuner.HyperParameters method for the given config.\n    \"\"\"\n\n    method_dict = {\n        \"int\": hp.Int,\n        \"choice\": hp.Choice,\n        \"boolean\": hp.Boolean,\n        \"float\": hp.Float,\n        \"fixed\": hp.Fixed,\n    }\n\n    hyperparam_dict = {}\n    for hyperparams in config:\n        try:\n            method = method_dict[hyperparams[\"method\"].lower()]\n        except KeyError:\n            raise ValueError(\n                f\"\"\"\n                Hyperparameter method {hyperparams['method']} not supported.\n                Must be one of {\",\".join(list(method_dict.keys()))}\"\"\"\n            )\n\n        hyperparam_dict.update(\n            {hyperparams[\"arg_name\"]: method(**hyperparams[\"kwargs\"])}\n        )\n    return hyperparam_dict\n</code></pre>"},{"location":"reference/src/kamae/graph/pipeline_graph/#src.kamae.graph.pipeline_graph.PipelineGraph.get_keras_tuner_model_builder","title":"get_keras_tuner_model_builder","text":"<pre><code>get_keras_tuner_model_builder(\n    tf_input_schema, hp_dict, output_names=None\n)\n</code></pre> <p>Returns a Keras tuner model builder function for the current graph. This allows the user to tune the hyperparameters of the preprocessing model. Useful for scenarios where the best preprocessing variables are not known a priori. For example, the num_bins to use for a HashIndexLayer.</p> <p>:param tf_input_schema: List of tf.TypeSpec objects containing the input schema for the model. Specifically the name, shape and dtype of each input. These will be passed as is to the Keras Input layer. :param hp_dict: Dictionary of possible hyperparameters for each layer. Should be of the format: {     \"\": [         {             \"arg_name\": ,             \"method\": , e.g. \"choice\"             \"kwargs\": {                              }         }     ] } :param output_names: Optional list of output names for the Keras model. If provided, only the outputs specified are used as model outputs. :returns: Model builder function that takes a keras_tuner.HyperParameters class and returns a model. Source code in <code>src/kamae/graph/pipeline_graph.py</code> <pre><code>def get_keras_tuner_model_builder(\n    self,\n    tf_input_schema: Union[List[tf.TypeSpec], List[Dict[str, Any]]],\n    hp_dict: Dict[str, List[Dict[str, Any]]],\n    output_names: Optional[List[str]] = None,\n) -&gt; Callable[[keras_tuner.HyperParameters], tf.keras.Model]:\n    \"\"\"\n    Returns a Keras tuner model builder function for the current graph.\n    This allows the user to tune the hyperparameters of the preprocessing model.\n    Useful for scenarios where the best preprocessing variables are not known\n    a priori. For example, the num_bins to use for a HashIndexLayer.\n\n    :param tf_input_schema: List of tf.TypeSpec objects containing the input schema\n    for the model. Specifically the name, shape and dtype of each input.\n    These will be passed as is to the Keras Input layer.\n    :param hp_dict: Dictionary of possible hyperparameters for each layer.\n    Should be of the format:\n    {\n        \"&lt;LAYER_NAME&gt;\": [\n            {\n                \"arg_name\": &lt;NAME_OF_LAYER_ARGUMENT&gt;,\n                \"method\": &lt;NAME_OF_KERAS_HYPERPARAMETER_METHOD&gt;, e.g. \"choice\"\n                \"kwargs\": {\n                    &lt;KWARGS_TO_PASS_TO_KERAS_HYPERPARAMETER_METHOD&gt;\n                }\n            }\n        ]\n    }\n    :param output_names: Optional list of output names for the Keras model. If\n    provided, only the outputs specified are used as model outputs.\n    :returns: Model builder function that takes a keras_tuner.HyperParameters class\n    and returns a model.\n    \"\"\"\n\n    transform_order = self.transform_order\n\n    def keras_model_builder(hp: keras_tuner.HyperParameters) -&gt; tf.keras.Model:\n        # We need to clear the layer store and inputs each time we build a model.\n        self.layer_store = {}\n        self.inputs = {}\n        # Build the input layers\n        self.build_keras_inputs(tf_input_schema=tf_input_schema)\n\n        for node in transform_order:\n            in_edges = list(self.graph.in_edges(node))\n\n            # Try and get the hyperparameter override.\n            try:\n                hp_override = self.get_keras_hyperparam_from_config(\n                    hp, hp_dict[node]\n                )\n            except KeyError:\n                hp_override = None\n            self.build_keras_transform_layer(\n                node=node, in_edges=in_edges, hp_override=hp_override\n            )\n\n        sorted_inputs = [self.inputs[k] for k in sorted(self.inputs)]\n        return tf.keras.Model(\n            inputs=sorted_inputs,\n            outputs=self.get_model_outputs(output_names=output_names),\n        )\n\n    return keras_model_builder\n</code></pre>"},{"location":"reference/src/kamae/graph/pipeline_graph/#src.kamae.graph.pipeline_graph.PipelineGraph.get_layer_output_from_layer_store","title":"get_layer_output_from_layer_store","text":"<pre><code>get_layer_output_from_layer_store(layer_output_name)\n</code></pre> <p>Given a layer name and index, get the output from the layer store.</p> <p>:param layer_output_name: Name of the layer output :returns: Tensor output of the layer</p> Source code in <code>src/kamae/graph/pipeline_graph.py</code> <pre><code>def get_layer_output_from_layer_store(self, layer_output_name: str) -&gt; tf.Tensor:\n    \"\"\"\n    Given a layer name and index, get the output from the layer store.\n\n    :param layer_output_name: Name of the layer output\n    :returns: Tensor output of the layer\n    \"\"\"\n    return self.layer_store[layer_output_name][\"output\"]\n</code></pre>"},{"location":"reference/src/kamae/graph/pipeline_graph/#src.kamae.graph.pipeline_graph.PipelineGraph.get_model_outputs","title":"get_model_outputs","text":"<pre><code>get_model_outputs(output_names=None)\n</code></pre> <p>Gets the outputs of the model. If output_names is provided, we use this to find the outputs for the model. Otherwise, the outputs are those that are not reused and not inputs. We also apply an identity layer to the outputs, so we can rename them with the same name as the output columns of the layer.</p> <p>:param output_names: Optional list of output names. If provided, the outputs are only allowed to be within this list. :returns: Dictionary of model output tensors.</p> Source code in <code>src/kamae/graph/pipeline_graph.py</code> <pre><code>def get_model_outputs(\n    self, output_names: Optional[List[str]] = None\n) -&gt; Dict[str, tf.Tensor]:\n    \"\"\"\n    Gets the outputs of the model. If output_names is provided, we use this to find\n    the outputs for the model. Otherwise, the outputs are those that are not reused\n    and not inputs. We also apply an identity layer to the outputs, so we\n    can rename them with the same name as the output columns of the layer.\n\n    :param output_names: Optional list of output names. If provided, the outputs\n    are only allowed to be within this list.\n    :returns: Dictionary of model output tensors.\n    \"\"\"\n    if output_names is None:\n        # If no specified output names then these are outputs that are not reused\n        # and not inputs.\n        output_names = [\n            k\n            for k, v in self.layer_store.items()\n            if not v[\"reused\"] and k not in self.inputs\n        ]\n    return {\n        # Do not wrap with identity if we are just passing through an input.\n        k: IdentityLayer(name=k)(v[\"output\"])\n        if k not in self.inputs\n        else v[\"output\"]\n        for k, v in self.layer_store.items()\n        if k in output_names\n    }\n</code></pre>"},{"location":"reference/src/kamae/graph/pipeline_graph/#src.kamae.graph.pipeline_graph.PipelineGraph.override_hyperparameters","title":"override_hyperparameters  <code>staticmethod</code>","text":"<pre><code>override_hyperparameters(layer, hp_override=None)\n</code></pre> <p>Overrides layer arguments with hyperparameters provided in the hyperparameter override dictionary.</p> <p>:param layer: Layer to override hyperparameters for. :param hp_override: Optional dictionary of hyperparameters to override. :returns: Layer with hyperparameters overridden.</p> Source code in <code>src/kamae/graph/pipeline_graph.py</code> <pre><code>@staticmethod\ndef override_hyperparameters(\n    layer: Union[tf.keras.layers.Layer, List[tf.keras.layers.Layer]],\n    hp_override: Dict[str, Any] = None,\n) -&gt; Union[tf.keras.layers.Layer, List[tf.keras.layers.Layer]]:\n    \"\"\"\n    Overrides layer arguments with hyperparameters provided in the\n    hyperparameter override dictionary.\n\n    :param layer: Layer to override hyperparameters for.\n    :param hp_override: Optional dictionary of hyperparameters to override.\n    :returns: Layer with hyperparameters overridden.\n    \"\"\"\n\n    def update_layer(\n        layer: tf.keras.layers.Layer, hp_override: Dict[str, Any]\n    ) -&gt; tf.keras.layers.Layer:\n        config = layer.get_config()\n        config.update(hp_override)\n        updated_layer = type(layer).from_config(config)\n        return updated_layer\n\n    if hp_override is None:\n        return layer\n    elif isinstance(layer, list):\n        overriden_layer = []\n        for layer_elem in layer:\n            overriden_layer.append(update_layer(layer_elem, hp_override))\n        return overriden_layer\n    else:\n        overriden_layer = update_layer(layer, hp_override)\n        return overriden_layer\n</code></pre>"},{"location":"reference/src/kamae/graph/pipeline_graph/#src.kamae.graph.pipeline_graph.PipelineGraph.sort_inputs","title":"sort_inputs","text":"<pre><code>sort_inputs(layer_name, input_dict)\n</code></pre> <p>Sorts the inputs for a given layer based on the order of the inputs in the stage dict. This is needed because layers with multiple inputs are not guaranteed to be in the correct order when built from the graph as the topological sort can be different to the order in the stage dict.</p> <p>:param layer_name: Name of the layer :param input_dict: Dictionary of inputs for the layer. :returns: Sorted list of inputs for the layer.</p> Source code in <code>src/kamae/graph/pipeline_graph.py</code> <pre><code>def sort_inputs(\n    self, layer_name: str, input_dict: Dict[str, tf.Tensor]\n) -&gt; List[tf.Tensor]:\n    \"\"\"\n    Sorts the inputs for a given layer based on the order of the inputs in the\n    stage dict. This is needed because layers with multiple inputs are not\n    guaranteed to be in the correct order when built from the graph as the\n    topological sort can be different to the order in the stage dict.\n\n    :param layer_name: Name of the layer\n    :param input_dict: Dictionary of inputs for the layer.\n    :returns: Sorted list of inputs for the layer.\n    \"\"\"\n    stage_inputs = self.stage_dict[layer_name][\"inputs\"]\n    return [input_dict[i] for i in stage_inputs]\n</code></pre>"},{"location":"reference/src/kamae/graph/pipeline_graph/#src.kamae.graph.pipeline_graph.PipelineGraph.update_layer_store","title":"update_layer_store","text":"<pre><code>update_layer_store(layer_dict)\n</code></pre> <p>Given a dictionary of layer output names and tensor outputs, update the layer store.</p> <p>:param layer_dict: Dictionary of layer names and outputs. :returns: None - layer store is updated.</p> Source code in <code>src/kamae/graph/pipeline_graph.py</code> <pre><code>def update_layer_store(self, layer_dict: Dict[str, tf.Tensor]) -&gt; None:\n    \"\"\"\n    Given a dictionary of layer output names and tensor outputs,\n    update the layer store.\n\n    :param layer_dict: Dictionary of layer names and outputs.\n    :returns: None - layer store is updated.\n    \"\"\"\n    for name, output in layer_dict.items():\n        self.update_layer_store_with_key(layer_key=name, layer_output=output)\n</code></pre>"},{"location":"reference/src/kamae/graph/pipeline_graph/#src.kamae.graph.pipeline_graph.PipelineGraph.update_layer_store_with_key","title":"update_layer_store_with_key","text":"<pre><code>update_layer_store_with_key(layer_key, layer_output)\n</code></pre> <p>Updates the layer store at a specific key with the layer output and whether it was reused. A layer is deemed to be reused if it is already present in the layer store.</p> <p>:param layer_key: Key to update the layer store with. :param layer_output: Layer output to update. :returns: None - layer store is updated.</p> Source code in <code>src/kamae/graph/pipeline_graph.py</code> <pre><code>def update_layer_store_with_key(\n    self, layer_key: str, layer_output: tf.Tensor\n) -&gt; None:\n    \"\"\"\n    Updates the layer store at a specific key with the layer output and whether\n    it was reused. A layer is deemed to be reused if it is already present in\n    the layer store.\n\n    :param layer_key: Key to update the layer store with.\n    :param layer_output: Layer output to update.\n    :returns: None - layer store is updated.\n    \"\"\"\n    if layer_key in self.layer_store:\n        self.layer_store[layer_key] = {\"output\": layer_output, \"reused\": True}\n    else:\n        self.layer_store[layer_key] = {\"output\": layer_output, \"reused\": False}\n</code></pre>"},{"location":"reference/src/kamae/sklearn/","title":"sklearn","text":""},{"location":"reference/src/kamae/sklearn/estimators/","title":"estimators","text":""},{"location":"reference/src/kamae/sklearn/estimators/standard_scale/","title":"standard_scale","text":""},{"location":"reference/src/kamae/sklearn/estimators/standard_scale/#src.kamae.sklearn.estimators.standard_scale.StandardScaleEstimator","title":"StandardScaleEstimator","text":"<pre><code>StandardScaleEstimator(input_col, output_col, layer_name)\n</code></pre> <p>               Bases: <code>StandardScaler</code>, <code>BaseTransformerMixin</code>, <code>SingleInputSingleOutputMixin</code></p> <p>Standard Scikit-Learn Estimator for use in Scikit-Learn pipelines. Wrapper over the existing implementation of the StandardScaler in Scikit-Learn, however operates on array columns and returns array columns. This is to align with the Spark implementation of the StandardScaler.</p> <p>Standardize features by removing the mean and scaling to unit variance.</p> <p>The standard score of a sample <code>x</code> is calculated as:</p> <pre><code>z = (x - u) / s\n</code></pre> <p>where <code>u</code> is the mean of the training samples and <code>s</code> is the standard deviation of the training samples</p> <p>:param input_col: Input column name. :param output_col: Output column name. :param layer_name: Name of the layer. Used as the name of the tensorflow layer</p> Source code in <code>src/kamae/sklearn/estimators/standard_scale.py</code> <pre><code>def __init__(self, input_col: str, output_col: str, layer_name: str) -&gt; None:\n    \"\"\"\n    Intializes a StandardScale estimator.\n\n    :param input_col: Input column name.\n    :param output_col: Output column name.\n    :param layer_name: Name of the layer. Used as the name of the tensorflow layer\n    \"\"\"\n    super().__init__(with_mean=True, with_std=True)\n    self.input_col = input_col\n    self.output_col = output_col\n    self.layer_name = layer_name\n</code></pre>"},{"location":"reference/src/kamae/sklearn/estimators/standard_scale/#src.kamae.sklearn.estimators.standard_scale.StandardScaleEstimator.fit","title":"fit","text":"<pre><code>fit(X, y=None, **kwargs)\n</code></pre> <p>Fits the transformer to the data. Since the scikit-learn StandardScaler takes scalar values, we need to convert the numpy array to a list of scalars. This is to mimic the behavior of the Spark StandardScaler.</p> <p>In this, the input to our transformer is an array, and the output is a scaled array.</p> <p>:param X: Pandas dataframe to fit the transformer to. :param y: Not used, present here for API consistency by convention. :returns: Fit pipeline.</p> Source code in <code>src/kamae/sklearn/estimators/standard_scale.py</code> <pre><code>def fit(\n    self, X: pd.DataFrame, y: None = None, **kwargs: Any\n) -&gt; \"StandardScaleEstimator\":\n    \"\"\"\n    Fits the transformer to the data. Since the scikit-learn StandardScaler\n    takes scalar values, we need to convert the numpy array to a list of scalars.\n    This is to mimic the behavior of the Spark StandardScaler.\n\n    In this, the input to our transformer is an array, and the output is a scaled\n    array.\n\n    :param X: Pandas dataframe to fit the transformer to.\n    :param y: Not used, present here for API consistency by convention.\n    :returns: Fit pipeline.\n    \"\"\"\n    # Get array column as a list of scalars\n    feature_array = X[self.input_col].tolist()\n    super().fit(X=feature_array, y=y, sample_weight=None)\n    return self\n</code></pre>"},{"location":"reference/src/kamae/sklearn/estimators/standard_scale/#src.kamae.sklearn.estimators.standard_scale.StandardScaleEstimator.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the standard scaler transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter  that performs the standardization.</p> Source code in <code>src/kamae/sklearn/estimators/standard_scale.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the standard scaler transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter\n     that performs the standardization.\n    \"\"\"\n    return StandardScaleLayer(\n        name=self.layer_name, mean=self.mean_, variance=self.var_\n    )\n</code></pre>"},{"location":"reference/src/kamae/sklearn/estimators/standard_scale/#src.kamae.sklearn.estimators.standard_scale.StandardScaleEstimator.transform","title":"transform","text":"<pre><code>transform(X, y=None)\n</code></pre> <p>Transforms the data using the transformer. Standardises the array <code>input_col</code>, creating a new standardised <code>output_col</code>.</p> <p>:param X: Pandas dataframe to transform. :param y: Not used, present here for API consistency by convention. :returns: Transformed data.</p> Source code in <code>src/kamae/sklearn/estimators/standard_scale.py</code> <pre><code>def transform(self, X: pd.DataFrame, y: None = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Transforms the data using the transformer. Standardises the array `input_col`,\n    creating a new standardised `output_col`.\n\n    :param X: Pandas dataframe to transform.\n    :param y: Not used, present here for API consistency by convention.\n    :returns: Transformed data.\n    \"\"\"\n    # Get array column as a list of scalars\n    feature_array = X[self.input_col].tolist()\n    # Transform the list of scalars\n    transformed_list_of_scalars = super().transform(feature_array)\n    # Set the output column to an array of the transformed list of scalars\n    X[self.output_col] = list(transformed_list_of_scalars)\n    return X\n</code></pre>"},{"location":"reference/src/kamae/sklearn/params/","title":"params","text":""},{"location":"reference/src/kamae/sklearn/params/base/","title":"base","text":""},{"location":"reference/src/kamae/sklearn/params/base/#src.kamae.sklearn.params.base.MultiInputMixin","title":"MultiInputMixin","text":"<p>Mixin class containing set methods for the multiple input columns scenario.</p>"},{"location":"reference/src/kamae/sklearn/params/base/#src.kamae.sklearn.params.base.MultiInputMixin.input_cols","title":"input_cols  <code>property</code> <code>writable</code>","text":"<pre><code>input_cols\n</code></pre> <p>Gets the input column names.</p> <p>:returns: List of strings of input column names.</p>"},{"location":"reference/src/kamae/sklearn/params/base/#src.kamae.sklearn.params.base.MultiInputMultiOutputMixin","title":"MultiInputMultiOutputMixin","text":"<p>               Bases: <code>MultiInputMixin</code>, <code>MultiOutputMixin</code></p> <p>Mixin for a layer that takes multiple inputs and returns multiple outputs</p>"},{"location":"reference/src/kamae/sklearn/params/base/#src.kamae.sklearn.params.base.MultiInputSingleOutputMixin","title":"MultiInputSingleOutputMixin","text":"<p>               Bases: <code>MultiInputMixin</code>, <code>SingleOutputMixin</code></p> <p>Mixin for a layer that takes multiple inputs and returns a single output</p>"},{"location":"reference/src/kamae/sklearn/params/base/#src.kamae.sklearn.params.base.MultiOutputMixin","title":"MultiOutputMixin","text":"<p>               Bases: <code>LayerNameMixin</code></p> <p>Mixin class containing set methods for the multiple output columns scenario.</p>"},{"location":"reference/src/kamae/sklearn/params/base/#src.kamae.sklearn.params.base.MultiOutputMixin.output_cols","title":"output_cols  <code>property</code> <code>writable</code>","text":"<pre><code>output_cols\n</code></pre> <p>Gets the output column names.</p> <p>:returns: List of strings of output column names.</p>"},{"location":"reference/src/kamae/sklearn/params/base/#src.kamae.sklearn.params.base.MultiOutputMixin.layer_name","title":"layer_name","text":"<pre><code>layer_name(value)\n</code></pre> <p>Sets the layer name to the given string value. Throws an error if the value is the same as any of the output column names, as this causes issues when constructing the pipeline graph.</p> <p>:param value: String to set the layer_name parameter to. :returns: None, layer_name is set to the given value.</p> Source code in <code>src/kamae/sklearn/params/base.py</code> <pre><code>@LayerNameMixin.layer_name.setter\ndef layer_name(self, value: str) -&gt; None:\n    \"\"\"\n    Sets the layer name to the given string value.\n    Throws an error if the value is the same as any of the output column names,\n    as this causes issues when constructing the pipeline graph.\n\n    :param value: String to set the layer_name parameter to.\n    :returns: None, layer_name is set to the given value.\n    \"\"\"\n    if hasattr(self, \"output_cols\") and any(\n        [output_col == value for output_col in self.output_cols]\n    ):\n        raise ValueError(\n            f\"\"\"Layer name {value} cannot be the same\n            as any of the output column names {\", \".join(self.output_cols)}\"\"\"\n        )\n    self._layer_name = value if value is not None else self.__repr__()\n</code></pre>"},{"location":"reference/src/kamae/sklearn/params/base/#src.kamae.sklearn.params.base.SingleInputMixin","title":"SingleInputMixin","text":"<p>Mixin class containing set methods for the single input column scenario.</p>"},{"location":"reference/src/kamae/sklearn/params/base/#src.kamae.sklearn.params.base.SingleInputMixin.input_col","title":"input_col  <code>property</code> <code>writable</code>","text":"<pre><code>input_col\n</code></pre> <p>Gets the input column name.</p> <p>:returns: Input column name.</p>"},{"location":"reference/src/kamae/sklearn/params/base/#src.kamae.sklearn.params.base.SingleInputMultiOutputMixin","title":"SingleInputMultiOutputMixin","text":"<p>               Bases: <code>SingleInputMixin</code>, <code>MultiOutputMixin</code></p> <p>Mixin for a layer that takes a single input and returns multiple outputs</p>"},{"location":"reference/src/kamae/sklearn/params/base/#src.kamae.sklearn.params.base.SingleInputSingleOutputMixin","title":"SingleInputSingleOutputMixin","text":"<p>               Bases: <code>SingleInputMixin</code>, <code>SingleOutputMixin</code></p> <p>Mixin for a layer that takes a single input and returns a single output</p>"},{"location":"reference/src/kamae/sklearn/params/base/#src.kamae.sklearn.params.base.SingleOutputMixin","title":"SingleOutputMixin","text":"<p>               Bases: <code>LayerNameMixin</code></p> <p>Mixin class containing set methods for the single output column scenario.</p>"},{"location":"reference/src/kamae/sklearn/params/base/#src.kamae.sklearn.params.base.SingleOutputMixin.output_col","title":"output_col  <code>property</code> <code>writable</code>","text":"<pre><code>output_col\n</code></pre> <p>Gets the output column name.</p> <p>:returns: List of strings of output column names.</p>"},{"location":"reference/src/kamae/sklearn/params/base/#src.kamae.sklearn.params.base.SingleOutputMixin.layer_name","title":"layer_name","text":"<pre><code>layer_name(value)\n</code></pre> <p>Sets the layer name to the given string value. Throws an error if the value is the same as the output column name, as this causes issues when constructing the pipeline graph.</p> <p>:param value: String to set the layer_name parameter to. :returns: None, layer_name is set to the given value.</p> Source code in <code>src/kamae/sklearn/params/base.py</code> <pre><code>@LayerNameMixin.layer_name.setter\ndef layer_name(self, value: str) -&gt; None:\n    \"\"\"\n    Sets the layer name to the given string value.\n    Throws an error if the value is the same as the output column name,\n    as this causes issues when constructing the pipeline graph.\n\n    :param value: String to set the layer_name parameter to.\n    :returns: None, layer_name is set to the given value.\n    \"\"\"\n    if hasattr(self, \"output_col\") and self.output_col == value:\n        raise ValueError(\n            f\"\"\"Layer name {value} cannot be the same\n            as the output column name {self.output_col}\"\"\"\n        )\n    self._layer_name = value if value is not None else self.__repr__()\n</code></pre>"},{"location":"reference/src/kamae/sklearn/params/name/","title":"name","text":""},{"location":"reference/src/kamae/sklearn/params/name/#src.kamae.sklearn.params.name.LayerNameMixin","title":"LayerNameMixin","text":"<p>Mixin class for a layer name.</p>"},{"location":"reference/src/kamae/sklearn/params/name/#src.kamae.sklearn.params.name.LayerNameMixin.layer_name","title":"layer_name  <code>property</code>","text":"<pre><code>layer_name\n</code></pre> <p>Gets the layer name.</p> <p>:returns: String of layer name.</p>"},{"location":"reference/src/kamae/sklearn/params/utils/","title":"utils","text":""},{"location":"reference/src/kamae/sklearn/params/utils/#src.kamae.sklearn.params.utils.InputOutputExtractor","title":"InputOutputExtractor","text":"<p>Mixin class containing methods for extracting input and output column names.</p>"},{"location":"reference/src/kamae/sklearn/params/utils/#src.kamae.sklearn.params.utils.InputOutputExtractor.get_layer_inputs_outputs","title":"get_layer_inputs_outputs","text":"<pre><code>get_layer_inputs_outputs()\n</code></pre> <p>Gets the input &amp; output information of the layer. Returns a tuple of lists, the first containing the input column names and the second containing the output column names.</p> <p>:returns: Tuple of lists containing the input and output column names.</p> Source code in <code>src/kamae/sklearn/params/utils.py</code> <pre><code>def get_layer_inputs_outputs(self) -&gt; Tuple[List[str], List[str]]:\n    \"\"\"\n    Gets the input &amp; output information of the layer. Returns a tuple of lists,\n    the first containing the input column names and the second containing the\n    output column names.\n\n    :returns: Tuple of lists containing the input and output column names.\n    \"\"\"\n\n    if hasattr(self, \"input_cols\") and getattr(self, \"input_cols\") is not None:\n        inputs = self.input_cols\n    elif hasattr(self, \"input_col\") and getattr(self, \"input_col\") is not None:\n        inputs = [self.input_col]\n    else:\n        inputs = []\n\n    if hasattr(self, \"output_cols\") and getattr(self, \"output_cols\") is not None:\n        outputs = self.output_cols\n    elif hasattr(self, \"output_col\") and getattr(self, \"output_col\") is not None:\n        outputs = [self.output_col]\n    else:\n        outputs = []\n\n    return inputs, outputs\n</code></pre>"},{"location":"reference/src/kamae/sklearn/pipeline/","title":"pipeline","text":""},{"location":"reference/src/kamae/sklearn/pipeline/pipeline/","title":"pipeline","text":""},{"location":"reference/src/kamae/sklearn/pipeline/pipeline/#src.kamae.sklearn.pipeline.pipeline.KamaeSklearnPipeline","title":"KamaeSklearnPipeline","text":"<pre><code>KamaeSklearnPipeline(steps, *, memory=None, verbose=False)\n</code></pre> <p>               Bases: <code>Pipeline</code></p> <p>KamaeSklearnPipeline is a subclass of sklearn.pipeline.Pipeline that is used to chain together BaseTransformers. It maintains the same functionality as sklearn.pipeline.Pipeline e.g. serialisation.</p> <p>:param steps: List of tuples containing the name and LayerTransformer :param memory: str or object with the joblib.Memory interface, default=None Used to cache the fitted transformers of the pipeline. The last step will never be cached, even if it is a transformer. By default, no caching is performed. If a string is given, it is the path to the caching directory. Enabling caching triggers a clone of the transformers before fitting. Therefore, the transformer instance given to the pipeline cannot be inspected directly. Use the attribute <code>named_steps</code> or <code>steps</code> to inspect estimators within the pipeline. Caching the transformers is advantageous when fitting is time consuming. :param verbose: If True, the time elapsed while fitting each step will be printed as it is completed.</p> Source code in <code>src/kamae/sklearn/pipeline/pipeline.py</code> <pre><code>def __init__(\n    self,\n    steps: List[Tuple[str, BaseTransformer]],\n    *,\n    memory: Optional[Union[str, joblib.Memory]] = None,\n    verbose: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initializes a KamaeSklearnPipeline object.\n\n    :param steps: List of tuples containing the name and LayerTransformer\n    :param memory: str or object with the joblib.Memory interface, default=None\n    Used to cache the fitted transformers of the pipeline. The last step\n    will never be cached, even if it is a transformer. By default, no\n    caching is performed. If a string is given, it is the path to the\n    caching directory. Enabling caching triggers a clone of the transformers\n    before fitting. Therefore, the transformer instance given to the\n    pipeline cannot be inspected directly. Use the attribute ``named_steps``\n    or ``steps`` to inspect estimators within the pipeline. Caching the\n    transformers is advantageous when fitting is time consuming.\n    :param verbose: If True, the time elapsed while fitting each step\n    will be printed as it is completed.\n    \"\"\"\n    super().__init__(steps, memory=memory, verbose=verbose)\n</code></pre>"},{"location":"reference/src/kamae/sklearn/pipeline/pipeline/#src.kamae.sklearn.pipeline.pipeline.KamaeSklearnPipeline.build_keras_model","title":"build_keras_model","text":"<pre><code>build_keras_model(tf_input_schema, output_names=None)\n</code></pre> <p>Builds a keras model from the pipeline using the PipelineGraph helper class.</p> <p>:param tf_input_schema: List of dictionaries containing the input schema for the model. Specifically the name, shape and dtype of each input. These will be passed as is to the Keras Input layer. :param output_names: Optional list of output names for the Keras model. If provided, only the outputs specified are used as model outputs. :returns: Keras model.</p> Source code in <code>src/kamae/sklearn/pipeline/pipeline.py</code> <pre><code>def build_keras_model(\n    self,\n    tf_input_schema: Union[List[tf.TypeSpec], List[Dict[str, Any]]],\n    output_names: Optional[List[str]] = None,\n) -&gt; tf.keras.Model:\n    \"\"\"\n    Builds a keras model from the pipeline using the PipelineGraph\n    helper class.\n\n    :param tf_input_schema: List of dictionaries containing the input schema for\n    the model. Specifically the name, shape and dtype of each input.\n    These will be passed as is to the Keras Input layer.\n    :param output_names: Optional list of output names for the Keras model. If\n    provided, only the outputs specified are used as model outputs.\n    :returns: Keras model.\n    \"\"\"\n    stage_dict = {\n        step[1].layer_name: step[1].construct_layer_info() for step in self.steps\n    }\n    pipeline_graph = PipelineGraph(stage_dict=stage_dict)\n    return pipeline_graph.build_keras_model(\n        tf_input_schema=tf_input_schema, output_names=output_names\n    )\n</code></pre>"},{"location":"reference/src/kamae/sklearn/pipeline/pipeline/#src.kamae.sklearn.pipeline.pipeline.KamaeSklearnPipeline.get_all_tf_layers","title":"get_all_tf_layers","text":"<pre><code>get_all_tf_layers()\n</code></pre> <p>Gets a list of all tensorflow layers in the pipeline model.</p> <p>:returns: List of tensorflow layers within the pipeline model.</p> Source code in <code>src/kamae/sklearn/pipeline/pipeline.py</code> <pre><code>def get_all_tf_layers(self) -&gt; List[tf.keras.layers.Layer]:\n    \"\"\"\n    Gets a list of all tensorflow layers in the pipeline model.\n\n    :returns: List of tensorflow layers within the pipeline model.\n    \"\"\"\n    return [step[1].get_tf_layer() for step in self.steps]\n</code></pre>"},{"location":"reference/src/kamae/sklearn/pipeline/pipeline/#src.kamae.sklearn.pipeline.pipeline.KamaeSklearnPipeline.get_keras_tuner_model_builder","title":"get_keras_tuner_model_builder","text":"<pre><code>get_keras_tuner_model_builder(\n    tf_input_schema, hp_dict, output_names=None\n)\n</code></pre> <p>Builds a keras tuner model builder (function) from the pipeline model using the PipelineGraph helper class.</p> <p>:param tf_input_schema: List of dictionaries containing the input schema for the model. Specifically the name, shape and dtype of each input. These will be passed as is to the Keras Input layer. :param hp_dict: Dictionary containing the hyperparameters for the model. :param output_names: Optional list of output names for the Keras model. If provided, only the outputs specified are used as model outputs. :returns: Keras tuner model builder (function).</p> Source code in <code>src/kamae/sklearn/pipeline/pipeline.py</code> <pre><code>def get_keras_tuner_model_builder(\n    self,\n    tf_input_schema: Union[List[tf.TypeSpec], List[Dict[str, Any]]],\n    hp_dict: Dict[str, List[Dict[str, Any]]],\n    output_names: Optional[List[str]] = None,\n) -&gt; Callable[[kt.HyperParameters], tf.keras.Model]:\n    \"\"\"\n    Builds a keras tuner model builder (function) from the pipeline model\n    using the PipelineGraph helper class.\n\n    :param tf_input_schema: List of dictionaries containing the input schema for\n    the model. Specifically the name, shape and dtype of each input.\n    These will be passed as is to the Keras Input layer.\n    :param hp_dict: Dictionary containing the hyperparameters for the model.\n    :param output_names: Optional list of output names for the Keras model. If\n    provided, only the outputs specified are used as model outputs.\n    :returns: Keras tuner model builder (function).\n    \"\"\"\n    stage_dict = {\n        step[1].layer_name: step[1].construct_layer_info() for step in self.steps\n    }\n    pipeline_graph = PipelineGraph(stage_dict=stage_dict)\n    return pipeline_graph.get_keras_tuner_model_builder(\n        tf_input_schema=tf_input_schema, hp_dict=hp_dict, output_names=output_names\n    )\n</code></pre>"},{"location":"reference/src/kamae/sklearn/transformers/","title":"transformers","text":""},{"location":"reference/src/kamae/sklearn/transformers/array_concatenate/","title":"array_concatenate","text":""},{"location":"reference/src/kamae/sklearn/transformers/array_concatenate/#src.kamae.sklearn.transformers.array_concatenate.ArrayConcatenateTransformer","title":"ArrayConcatenateTransformer","text":"<pre><code>ArrayConcatenateTransformer(\n    input_cols, output_col, layer_name\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>MultiInputSingleOutputMixin</code></p> <p>Vector Assembler Scikit-Learn Transformer for use in Scikit-Learn pipelines. This transformer assembles multiple columns into a single array column.</p> Source code in <code>src/kamae/sklearn/transformers/array_concatenate.py</code> <pre><code>def __init__(self, input_cols: List[str], output_col: str, layer_name: str) -&gt; None:\n    super().__init__()\n    self.input_cols = input_cols\n    self.output_col = output_col\n    self.layer_name = layer_name\n</code></pre>"},{"location":"reference/src/kamae/sklearn/transformers/array_concatenate/#src.kamae.sklearn.transformers.array_concatenate.ArrayConcatenateTransformer.fit","title":"fit","text":"<pre><code>fit(X, y=None)\n</code></pre> <p>Fits the transformer to the data. Does nothing since this is transformer not an estimator.</p> <p>:param X: Pandas dataframe to fit the transformer to. :param y: Not used, present here for API consistency by convention. :returns: Fit pipeline, in this case the transformer itself.</p> Source code in <code>src/kamae/sklearn/transformers/array_concatenate.py</code> <pre><code>def fit(self, X: pd.DataFrame, y: None = None) -&gt; \"ArrayConcatenateTransformer\":\n    \"\"\"\n    Fits the transformer to the data. Does nothing since\n    this is transformer not an estimator.\n\n    :param X: Pandas dataframe to fit the transformer to.\n    :param y: Not used, present here for API consistency by convention.\n    :returns: Fit pipeline, in this case the transformer itself.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"reference/src/kamae/sklearn/transformers/array_concatenate/#src.kamae.sklearn.transformers.array_concatenate.ArrayConcatenateTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer that concatenates the input tensors.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that concatenates the input tensors.</p> Source code in <code>src/kamae/sklearn/transformers/array_concatenate.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer that concatenates the input tensors.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter\n    that concatenates the input tensors.\n    \"\"\"\n    return ArrayConcatenateLayer(name=self.layer_name, axis=-1)\n</code></pre>"},{"location":"reference/src/kamae/sklearn/transformers/array_concatenate/#src.kamae.sklearn.transformers.array_concatenate.ArrayConcatenateTransformer.transform","title":"transform","text":"<pre><code>transform(X, y=None)\n</code></pre> <p>Transform the input dataset. Creates a new column named outputCol which is a concatenated array of all input columns.</p> <p>:param X: Pandas dataframe to transform. :param y: Not used, present here for API consistency by convention. :returns: Transformed data.</p> Source code in <code>src/kamae/sklearn/transformers/array_concatenate.py</code> <pre><code>def transform(self, X: pd.DataFrame, y: None = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform the input dataset. Creates a new column named outputCol which is a\n    concatenated array of all input columns.\n\n    :param X: Pandas dataframe to transform.\n    :param y: Not used, present here for API consistency by convention.\n    :returns: Transformed data.\n    \"\"\"\n\n    # Check which columns are arrays, this gives a dict like:\n    # {'col1': True, 'col2': False, 'col3': True}\n    is_col_an_array_dict = (\n        X.head(1)[self.input_cols]\n        .applymap(lambda x: pd.api.types.is_list_like(x))\n        .to_dict(orient=\"records\")[0]\n    )\n\n    new_input_cols = []\n    for col_name, col_an_array in is_col_an_array_dict.items():\n        if col_an_array:\n            # If the column is an array then we need to create a\n            # numpy array of arrays\n            # TODO: Can we make this more this efficient?\n            values = X[col_name].to_numpy()\n            new_input_cols.append(np.array([np.array(x) for x in values]))\n        else:\n            # If the column is not an array then we just need to extend\n            # the numpy array to have an extra dimension. This is so we can concat\n            # the arrays later.\n            values = X[col_name].to_numpy()\n            new_input_cols.append(values[:, None])\n\n    # Concatenate the arrays, this creates an N x M array\n    # where N is the number of rows, M is the number of features\n    concatenated_array = np.concatenate(new_input_cols, axis=-1)\n    # Add this to the dataframe, convert the numpy array to a list\n    # of 1-D numpy arrays\n    X[self.output_col] = list(concatenated_array)\n\n    return X\n</code></pre>"},{"location":"reference/src/kamae/sklearn/transformers/array_split/","title":"array_split","text":""},{"location":"reference/src/kamae/sklearn/transformers/array_split/#src.kamae.sklearn.transformers.array_split.ArraySplitTransformer","title":"ArraySplitTransformer","text":"<pre><code>ArraySplitTransformer(input_col, output_cols, layer_name)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputMultiOutputMixin</code></p> <p>VectorSlicer Scikit-Learn Transformer for use in Scikit-Learn pipelines. This transformer slices an array column into multiple columns.</p> Source code in <code>src/kamae/sklearn/transformers/array_split.py</code> <pre><code>def __init__(self, input_col: str, output_cols: List[str], layer_name: str) -&gt; None:\n    super().__init__()\n    self.input_col = input_col\n    self.output_cols = output_cols\n    self.layer_name = layer_name\n</code></pre>"},{"location":"reference/src/kamae/sklearn/transformers/array_split/#src.kamae.sklearn.transformers.array_split.ArraySplitTransformer.fit","title":"fit","text":"<pre><code>fit(X, y=None)\n</code></pre> <p>Fits the transformer to the data. Does nothing since this is transformer not an estimator.</p> <p>:param X: Pandas dataframe to fit the transformer to. :param y: Not used, present here for API consistency by convention. :returns: Fit pipeline, in this case the transformer itself.</p> Source code in <code>src/kamae/sklearn/transformers/array_split.py</code> <pre><code>def fit(self, X: pd.DataFrame, y: None = None) -&gt; \"ArraySplitTransformer\":\n    \"\"\"\n    Fits the transformer to the data. Does nothing since\n    this is transformer not an estimator.\n\n    :param X: Pandas dataframe to fit the transformer to.\n    :param y: Not used, present here for API consistency by convention.\n    :returns: Fit pipeline, in this case the transformer itself.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"reference/src/kamae/sklearn/transformers/array_split/#src.kamae.sklearn.transformers.array_split.ArraySplitTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for that unstacks the input tensor and reshapes to the original shape.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that slices the input tensors.</p> Source code in <code>src/kamae/sklearn/transformers/array_split.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for that unstacks the input tensor and reshapes\n    to the original shape.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter\n    that slices the input tensors.\n    \"\"\"\n    return ArraySplitLayer(name=self.layer_name, axis=-1)\n</code></pre>"},{"location":"reference/src/kamae/sklearn/transformers/array_split/#src.kamae.sklearn.transformers.array_split.ArraySplitTransformer.transform","title":"transform","text":"<pre><code>transform(X, y=None)\n</code></pre> <p>Transforms the input dataset. Creates a new column for each output column equal to the value of the input column at the given index.</p> <p>:param X: Pandas dataframe to transform. :param y: Not used, present here for API consistency by convention. :returns: Transformed data.</p> Source code in <code>src/kamae/sklearn/transformers/array_split.py</code> <pre><code>def transform(self, X: pd.DataFrame, y: None = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column for each output column equal\n    to the value of the input column at the given index.\n\n    :param X: Pandas dataframe to transform.\n    :param y: Not used, present here for API consistency by convention.\n    :returns: Transformed data.\n    \"\"\"\n    X[self.output_cols] = pd.DataFrame(X[self.input_col].tolist(), index=X.index)\n    return X\n</code></pre>"},{"location":"reference/src/kamae/sklearn/transformers/base/","title":"base","text":""},{"location":"reference/src/kamae/sklearn/transformers/base/#src.kamae.sklearn.transformers.base.BaseTransformer","title":"BaseTransformer","text":"<pre><code>BaseTransformer(**kwargs)\n</code></pre> <p>               Bases: <code>BaseTransformerMixin</code>, <code>BaseEstimator</code>, <code>TransformerMixin</code>, <code>ABC</code></p> <p>Abstract class for all scikit-learn transformers. Specifically, this class extends the required scikit-learn classes BaseEstimator and TransformerMixin adding in the kamae BaseTransformerMixin which defines the methods needed to work with the kamae pipeline graph.</p> <p>The reason we keep this separate from the BaseTransformerMixin (which is not done for Spark) is because on the scikit-learn side we want to allow the ability to inherit from existing scikit-learn classes (such as the StandardScaler). In these cases the existing class already inherits from BaseEstimator and TransformerMixin and so only needs the BaseTransformerMixin (to add kamae specific functionality). If you try and inherit these classes twice (once from the existing scikit-learn class and once from BaseTransformer) you will get an error. Therefore, we keep these separate.</p> <p>If you are building an entirely new transformer, then you can inherit from this class directly, to save you from having to inherit from BaseEstimator and TransformerMixin.</p> <p>In Spark, all existing (core) implementations are built in Scala and ported to Python. In this case, the ability to re-use existing Spark transformers is very difficult and not worth the effort. You can see that for the StandardScaleEstimator the logic does not depend on the existing Spark StandardScaler.</p> <p>Therefore, we have a single BaseTransformer class for use by all Spark transformers.</p> Source code in <code>src/kamae/sklearn/transformers/base.py</code> <pre><code>def __init__(self, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Initializes the transformer.\n    \"\"\"\n    super().__init__()\n</code></pre>"},{"location":"reference/src/kamae/sklearn/transformers/base/#src.kamae.sklearn.transformers.base.BaseTransformerMixin","title":"BaseTransformerMixin","text":"<pre><code>BaseTransformerMixin(**kwargs)\n</code></pre> <p>               Bases: <code>ABC</code>, <code>LayerNameMixin</code>, <code>InputOutputExtractor</code></p> <p>Mixin abstract class defining methods needed for all kamae scikit-learn transformers.</p> Source code in <code>src/kamae/sklearn/transformers/base.py</code> <pre><code>def __init__(self, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Initializes the transformer.\n    \"\"\"\n    super().__init__()\n</code></pre>"},{"location":"reference/src/kamae/sklearn/transformers/base/#src.kamae.sklearn.transformers.base.BaseTransformerMixin.construct_layer_info","title":"construct_layer_info","text":"<pre><code>construct_layer_info()\n</code></pre> <p>Constructs the layer info dictionary. Contains the layer name, the tensorflow layer, and the inputs and outputs. This is used when constructing the pipeline graph.</p> <p>:returns: Dictionary containing layer information such as name, tensorflow layer, inputs, and outputs.</p> Source code in <code>src/kamae/sklearn/transformers/base.py</code> <pre><code>def construct_layer_info(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Constructs the layer info dictionary.\n    Contains the layer name, the tensorflow layer, and the inputs and outputs.\n    This is used when constructing the pipeline graph.\n\n    :returns: Dictionary containing layer information such as\n    name, tensorflow layer, inputs, and outputs.\n    \"\"\"\n    inputs, outputs = self.get_layer_inputs_outputs()\n    return {\n        \"name\": self.layer_name,\n        \"layer\": self.get_tf_layer(),\n        \"inputs\": inputs,\n        \"outputs\": outputs,\n    }\n</code></pre>"},{"location":"reference/src/kamae/sklearn/transformers/base/#src.kamae.sklearn.transformers.base.BaseTransformerMixin.get_tf_layer","title":"get_tf_layer  <code>abstractmethod</code>","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer to be used in the model. This is the only abstract method that must be implemented. :returns: Tensorflow Layer</p> Source code in <code>src/kamae/sklearn/transformers/base.py</code> <pre><code>@abstractmethod\ndef get_tf_layer(self) -&gt; Union[tf.keras.layers.Layer, List[tf.keras.layers.Layer]]:\n    \"\"\"\n    Gets the tensorflow layer to be used in the model.\n    This is the only abstract method that must be implemented.\n    :returns: Tensorflow Layer\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/src/kamae/sklearn/transformers/identity/","title":"identity","text":""},{"location":"reference/src/kamae/sklearn/transformers/identity/#src.kamae.sklearn.transformers.identity.IdentityTransformer","title":"IdentityTransformer","text":"<pre><code>IdentityTransformer(input_col, output_col, layer_name)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputMixin</code></p> <p>Identity Scikit-Learn Transformer for use in Scikit-Learn pipelines. This transformer simply passes the input to the output unchanged. Used for cases where you want to keep the input the same.</p> <p>:param input_col: Input column name. :param output_col: Output column name. :param layer_name: Name of the layer. Used as the name of the tensorflow layer in the keras model. :returns: None - class instantialized.</p> Source code in <code>src/kamae/sklearn/transformers/identity.py</code> <pre><code>def __init__(self, input_col: str, output_col: str, layer_name: str) -&gt; None:\n    \"\"\"\n    Intializes an IdentityTransformer transformer.\n\n    :param input_col: Input column name.\n    :param output_col: Output column name.\n    :param layer_name: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model.\n    :returns: None - class instantialized.\n    \"\"\"\n    super().__init__()\n    self.input_col = input_col\n    self.output_col = output_col\n    self.layer_name = layer_name\n</code></pre>"},{"location":"reference/src/kamae/sklearn/transformers/identity/#src.kamae.sklearn.transformers.identity.IdentityTransformer.fit","title":"fit","text":"<pre><code>fit(X, y=None)\n</code></pre> <p>Fits the transformer to the data. Does nothing since this is an identity transformer.</p> <p>:param X: Pandas dataframe to fit the transformer to. :param y: Not used, present here for API consistency by convention. :returns: Fit pipeline, in this case the transformer itself.</p> Source code in <code>src/kamae/sklearn/transformers/identity.py</code> <pre><code>def fit(self, X: pd.DataFrame, y: None = None) -&gt; \"IdentityTransformer\":\n    \"\"\"\n    Fits the transformer to the data. Does nothing since\n    this is an identity transformer.\n\n    :param X: Pandas dataframe to fit the transformer to.\n    :param y: Not used, present here for API consistency by convention.\n    :returns: Fit pipeline, in this case the transformer itself.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"reference/src/kamae/sklearn/transformers/identity/#src.kamae.sklearn.transformers.identity.IdentityTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the identity transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs an Identity operation.</p> Source code in <code>src/kamae/sklearn/transformers/identity.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the identity transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs an Identity operation.\n    \"\"\"\n    return IdentityLayer(\n        name=self.layer_name,\n    )\n</code></pre>"},{"location":"reference/src/kamae/sklearn/transformers/identity/#src.kamae.sklearn.transformers.identity.IdentityTransformer.transform","title":"transform","text":"<pre><code>transform(X, y=None)\n</code></pre> <p>Transforms the data using the transformer. Creates a new column with name <code>output_col</code>, which is the same as the <code>input_col</code>.</p> <p>:param X: Pandas dataframe to transform. :param y: Not used, present here for API consistency by convention. :returns: Transformed data.</p> Source code in <code>src/kamae/sklearn/transformers/identity.py</code> <pre><code>def transform(self, X: pd.DataFrame, y: None = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Transforms the data using the transformer. Creates a new column with name\n    `output_col`, which is the same as the `input_col`.\n\n    :param X: Pandas dataframe to transform.\n    :param y: Not used, present here for API consistency by convention.\n    :returns: Transformed data.\n    \"\"\"\n    X[self.output_col] = X[self.input_col]\n    return X\n</code></pre>"},{"location":"reference/src/kamae/sklearn/transformers/log/","title":"log","text":""},{"location":"reference/src/kamae/sklearn/transformers/log/#src.kamae.sklearn.transformers.log.LogTransformer","title":"LogTransformer","text":"<pre><code>LogTransformer(\n    input_col, output_col, layer_name, alpha=None\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputMixin</code></p> <p>Log Scikit-Learn Transformer for use in Scikit-Learn pipelines. This transformer applies a log(alpha + x) transform to the input column.</p> <ul> <li>alpha: 1</li> </ul> <p>:param input_col: Input column name. :param output_col: Output column name. :param layer_name: Name of the layer. Used as the name of the tensorflow layer :param alpha: Value to use in log transform: log(alpha + x). Default is 1. :returns: None - class instantialized.</p> Source code in <code>src/kamae/sklearn/transformers/log.py</code> <pre><code>def __init__(\n    self,\n    input_col: str,\n    output_col: str,\n    layer_name: str,\n    alpha: Optional[float] = None,\n) -&gt; None:\n    \"\"\"\n    Intializes a LogTransformLayer transformer. Sets the default values of:\n\n    - alpha: 1\n\n    :param input_col: Input column name.\n    :param output_col: Output column name.\n    :param layer_name: Name of the layer. Used as the name of the tensorflow layer\n    :param alpha: Value to use in log transform: log(alpha + x). Default is 1.\n    :returns: None - class instantialized.\n    \"\"\"\n    super().__init__()\n    self.input_col = input_col\n    self.output_col = output_col\n    self.layer_name = layer_name\n    self.alpha = float(alpha) if alpha is not None else 1.0\n</code></pre>"},{"location":"reference/src/kamae/sklearn/transformers/log/#src.kamae.sklearn.transformers.log.LogTransformer.fit","title":"fit","text":"<pre><code>fit(X, y=None)\n</code></pre> <p>Fits the transformer. Does nothing since this is just a transformer.</p> <p>:param X: Pandas dataframe to fit the transformer to. :param y: Not used, present here for API consistency by convention. :returns: Fit pipeline, in this case the transformer itself.</p> Source code in <code>src/kamae/sklearn/transformers/log.py</code> <pre><code>def fit(self, X: pd.DataFrame, y: None = None) -&gt; \"LogTransformer\":\n    \"\"\"\n    Fits the transformer. Does nothing since this is just a transformer.\n\n    :param X: Pandas dataframe to fit the transformer to.\n    :param y: Not used, present here for API consistency by convention.\n    :returns: Fit pipeline, in this case the transformer itself.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"reference/src/kamae/sklearn/transformers/log/#src.kamae.sklearn.transformers.log.LogTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer that performs the log transform.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that performs the log(alpha + x) operation.</p> Source code in <code>src/kamae/sklearn/transformers/log.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer that performs the log transform.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter\n    that performs the log(alpha + x) operation.\n    \"\"\"\n    alpha = self.alpha\n    return LogLayer(name=self.layer_name, alpha=alpha)\n</code></pre>"},{"location":"reference/src/kamae/sklearn/transformers/log/#src.kamae.sklearn.transformers.log.LogTransformer.transform","title":"transform","text":"<pre><code>transform(X, y=None)\n</code></pre> <p>Transforms the data using the transformer. Creates a new column with name <code>output_col</code>, which applies log(alpha + x) transform to the <code>input_col</code>.</p> <p>:param X: Pandas dataframe to transform. :param y: Not used, present here for API consistency by convention. :returns: Transformed data.</p> Source code in <code>src/kamae/sklearn/transformers/log.py</code> <pre><code>def transform(self, X: pd.DataFrame, y: None = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Transforms the data using the transformer. Creates a new column with name\n    `output_col`, which applies log(alpha + x) transform to the `input_col`.\n\n    :param X: Pandas dataframe to transform.\n    :param y: Not used, present here for API consistency by convention.\n    :returns: Transformed data.\n    \"\"\"\n    X[self.output_col] = np.log(X[self.input_col] + self.alpha)\n    return X\n</code></pre>"},{"location":"reference/src/kamae/spark/","title":"spark","text":""},{"location":"reference/src/kamae/spark/common/","title":"common","text":""},{"location":"reference/src/kamae/spark/common/spark_operation/","title":"spark_operation","text":""},{"location":"reference/src/kamae/spark/common/spark_operation/#src.kamae.spark.common.spark_operation.SparkOperation","title":"SparkOperation","text":"<pre><code>SparkOperation()\n</code></pre> <p>               Bases: <code>ABC</code>, <code>HasLayerName</code>, <code>HasInputDtype</code>, <code>HasOutputDtype</code>, <code>InputOutputExtractor</code></p> <p>Abstract class used in Spark transformers and estimators. Provides common utils for param setting, input/output dtype casting, and layer name setting.</p> Source code in <code>src/kamae/spark/common/spark_operation.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"\n    Initializes the spark operation class.\n    \"\"\"\n    super().__init__()\n    self._setDefault(layerName=self.uid, inputDtype=None, outputDtype=None)\n    self.tmp_column_suffix = self.generate_tmp_column_suffix()\n</code></pre>"},{"location":"reference/src/kamae/spark/common/spark_operation/#src.kamae.spark.common.spark_operation.SparkOperation.compatible_dtypes","title":"compatible_dtypes  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the spark operation. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the spark operation.</p>"},{"location":"reference/src/kamae/spark/common/spark_operation/#src.kamae.spark.common.spark_operation.SparkOperation._cast","title":"_cast  <code>staticmethod</code>","text":"<pre><code>_cast(column, column_dtype, cast_dtype)\n</code></pre> <p>Casts the input column to the specified datatype.</p> <p>:param column: Pyspark column to cast. :param column_dtype: The datatype class of the column. :param cast_dtype: The datatype string to cast the column to. :returns: Pyspark column cast to the specified datatype.</p> Source code in <code>src/kamae/spark/common/spark_operation.py</code> <pre><code>@staticmethod\ndef _cast(column: Column, column_dtype: DataType, cast_dtype: str) -&gt; Column:\n    \"\"\"\n    Casts the input column to the specified datatype.\n\n    :param column: Pyspark column to cast.\n    :param column_dtype: The datatype class of the column.\n    :param cast_dtype: The datatype string to cast the column to.\n    :returns: Pyspark column cast to the specified datatype.\n    \"\"\"\n    # There is an edge case where we can have a negatively signed 0.\n    # This will not match tensorflow, so we will ensure the sign of zeros is\n    # always positive.\n    if isinstance(column_dtype, NumericType):\n        # We need to cast back to the original type after multiplying by the sign\n        # since the multiplication by sign will return a double. Therefore, we can\n        # have cases where the original type is an int, it is cast to a double, and\n        # then we want a string. So we have 1 -&gt; 1.0 -&gt; \"1.0\" rather than \"1\".\n        # TODO: I really don't like this, there must be a better way to do this.\n        func = lambda x: F.when(  # noqa: E731\n            x == F.lit(0),\n            (x * F.signum(x)).cast(column_dtype.simpleString()).cast(cast_dtype),\n        ).otherwise(x.cast(cast_dtype))\n    else:\n        func = lambda x: x.cast(cast_dtype)  # noqa: E731\n    return single_input_single_output_scalar_transform(\n        input_col=column, input_col_datatype=column_dtype, func=func\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/common/spark_operation/#src.kamae.spark.common.spark_operation.SparkOperation._cast_input_columns","title":"_cast_input_columns","text":"<pre><code>_cast_input_columns(input_columns, input_column_datatypes)\n</code></pre> <p>Casts the input columns to the given inputDtype, if specified. All columns are cast to this. This might not be ideal, there may be layers where some inputs are expected to be different types. In these cases, the subclass should implement the cast_input_columns method.</p> <p>:param input_columns: List of input columns to cast. :param input_column_datatypes: List of input column datatypes. :returns: List of tuple; first element is input columns cast to the inputDtype, second element is a boolean indicating whether the column was cast.</p> Source code in <code>src/kamae/spark/common/spark_operation.py</code> <pre><code>def _cast_input_columns(\n    self, input_columns: List[Column], input_column_datatypes: List[DataType]\n) -&gt; List[Tuple[Column, bool]]:\n    \"\"\"\n    Casts the input columns to the given inputDtype, if specified. All columns are\n    cast to this. This might not be ideal, there may be layers where some inputs are\n    expected to be different types. In these cases, the subclass should\n    implement the cast_input_columns method.\n\n    :param input_columns: List of input columns to cast.\n    :param input_column_datatypes: List of input column datatypes.\n    :returns: List of tuple; first element is input columns cast to the inputDtype,\n    second element is a boolean indicating whether the column was cast.\n    \"\"\"\n    return self._cast_input_output_columns(\n        columns=input_columns, column_datatypes=input_column_datatypes, ingress=True\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/common/spark_operation/#src.kamae.spark.common.spark_operation.SparkOperation._cast_input_output_columns","title":"_cast_input_output_columns","text":"<pre><code>_cast_input_output_columns(\n    columns, column_datatypes, ingress\n)\n</code></pre> <p>Casts either the input and output columns to the given inputDtype or outputDtype, if specified. Ingress is a boolean that indicates whether we are casting the input (True) or output (False) columns.</p> <p>:param columns: List of input/output columns to cast. :param column_datatypes: List of input/output column datatypes. :param ingress: Boolean indicating whether we are casting the input (True) or output (False) columns. :returns: List of tuple; first element is input/output columns cast to the inputDtype/outputDtype, second element is a boolean indicating whether the column was cast.</p> Source code in <code>src/kamae/spark/common/spark_operation.py</code> <pre><code>def _cast_input_output_columns(\n    self, columns: List[Column], column_datatypes: List[DataType], ingress: bool\n) -&gt; List[Tuple[Column, bool]]:\n    \"\"\"\n    Casts either the input and output columns to the given inputDtype or\n    outputDtype, if specified. Ingress is a boolean that indicates whether we are\n    casting the input (True) or output (False) columns.\n\n    :param columns: List of input/output columns to cast.\n    :param column_datatypes: List of input/output column datatypes.\n    :param ingress: Boolean indicating whether we are casting the input (True) or\n    output (False) columns.\n    :returns: List of tuple; first element is input/output columns cast to the\n    inputDtype/outputDtype, second element is a boolean indicating whether the\n    column was cast.\n    \"\"\"\n    if ingress:\n        cast_dtype = self.getInputDtype()\n        if (\n            cast_dtype is not None\n            and self.compatible_dtypes is not None\n            and cast_dtype\n            not in [dtype.simpleString() for dtype in self.compatible_dtypes]\n        ):\n            raise ValueError(\n                f\"\"\"inputDtype {cast_dtype} is not a compatible dtype for\n                transformer with uid {self.uid}.\n                Compatible dtypes are{[\n                    dtype.simpleString() for dtype in self.compatible_dtypes\n                ]}.\"\"\"\n            )\n    else:\n        cast_dtype = self.getOutputDtype()\n\n    if cast_dtype is not None:\n        return [\n            (\n                self._cast(\n                    column=column, column_dtype=column_dtype, cast_dtype=cast_dtype\n                ),\n                True,\n            )\n            if get_element_type(column_dtype).simpleString() != cast_dtype\n            else (\n                column,\n                False,\n            )\n            for column, column_dtype in zip(columns, column_datatypes)\n        ]\n    return [(column, False) for column in columns]\n</code></pre>"},{"location":"reference/src/kamae/spark/common/spark_operation/#src.kamae.spark.common.spark_operation.SparkOperation._cast_output_columns","title":"_cast_output_columns","text":"<pre><code>_cast_output_columns(\n    output_columns, output_column_datatypes\n)\n</code></pre> <p>Casts the output columns to the given outputDtype, if specified. All columns are cast to this. This might not be ideal, there may be layers where some outputs are expected to be different types. In these cases, the subclass should implement the cast_output_columns method.</p> <p>:param output_columns: List of output columns to cast. :param output_column_datatypes: List of output column datatypes. :returns: List of tuple; first element is output columns cast to the outputDtype, second element is a boolean indicating whether the column was cast.</p> Source code in <code>src/kamae/spark/common/spark_operation.py</code> <pre><code>def _cast_output_columns(\n    self, output_columns: List[Column], output_column_datatypes: List[DataType]\n) -&gt; List[Tuple[Column, bool]]:\n    \"\"\"\n    Casts the output columns to the given outputDtype, if specified. All columns are\n    cast to this. This might not be ideal, there may be layers where some outputs\n    are expected to be different types. In these cases, the subclass should\n    implement the cast_output_columns method.\n\n    :param output_columns: List of output columns to cast.\n    :param output_column_datatypes: List of output column datatypes.\n    :returns: List of tuple; first element is output columns cast to the\n    outputDtype, second element is a boolean indicating whether the column was cast.\n    \"\"\"\n    return self._cast_input_output_columns(\n        columns=output_columns,\n        column_datatypes=output_column_datatypes,\n        ingress=False,\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/common/spark_operation/#src.kamae.spark.common.spark_operation.SparkOperation._check_input_dtypes_compatible","title":"_check_input_dtypes_compatible","text":"<pre><code>_check_input_dtypes_compatible(dataset, column_names)\n</code></pre> <p>Checks if the input tensors are compatible with the compatible_dtypes of the layer.</p> <p>:param dataset: The input dataset. :param column_names: The names of the input columns. :raises ValueError: If the input columns are not compatible with the compatible_dtypes of the transformer. :returns: None</p> Source code in <code>src/kamae/spark/common/spark_operation.py</code> <pre><code>def _check_input_dtypes_compatible(\n    self, dataset: DataFrame, column_names: List[str]\n) -&gt; None:\n    \"\"\"\n    Checks if the input tensors are compatible with the compatible_dtypes of the\n    layer.\n\n    :param dataset: The input dataset.\n    :param column_names: The names of the input columns.\n    :raises ValueError: If the input columns are not compatible with the\n    compatible_dtypes of the transformer.\n    :returns: None\n    \"\"\"\n    for c in column_names:\n        tmp_column_name = self._resolve_tmp_from_true_column_name(\n            c, suffix=self.tmp_column_suffix\n        )\n        # Either the tmp column name or the original is present.\n        check_column_name = (\n            tmp_column_name if tmp_column_name in dataset.columns else c\n        )\n        col_dtype = self.get_column_datatype(dataset, check_column_name)\n        underlying_col_dtype = get_element_type(col_dtype)\n        if (\n            self.compatible_dtypes is not None\n            and underlying_col_dtype not in self.compatible_dtypes\n        ):\n            raise TypeError(\n                f\"\"\"Input column with name {c} and dtype\n                {underlying_col_dtype.simpleString()} is not a compatible dtype for\n                transformer with uid {self.uid}.\n                Compatible dtypes are {[\n                    dtype.simpleString() for dtype in self.compatible_dtypes\n                ]}.\"\"\"\n            )\n</code></pre>"},{"location":"reference/src/kamae/spark/common/spark_operation/#src.kamae.spark.common.spark_operation.SparkOperation._create_casted_input_output_columns","title":"_create_casted_input_output_columns","text":"<pre><code>_create_casted_input_output_columns(dataset, ingress)\n</code></pre> <p>Recreates the input or output columns, creating temporary columns with the casted input columns if necessary. This is done to avoid overwriting the original columns and creating inconsistencies if multiple transforms use the same inputs.</p> <p>:param dataset: The input dataset. :param ingress: Whether the input columns are being cast or the output columns. :returns: The dataset with the cast input/output columns.</p> Source code in <code>src/kamae/spark/common/spark_operation.py</code> <pre><code>def _create_casted_input_output_columns(\n    self, dataset: DataFrame, ingress: bool\n) -&gt; DataFrame:\n    \"\"\"\n    Recreates the input or output columns, creating temporary columns with the\n    casted input columns if necessary. This is done to avoid overwriting the\n    original columns and creating inconsistencies if multiple transforms use the\n    same inputs.\n\n    :param dataset: The input dataset.\n    :param ingress: Whether the input columns are being cast or the output columns.\n    :returns: The dataset with the cast input/output columns.\n    \"\"\"\n    col_names = self._get_single_or_multi_col(ingress=ingress)\n    columns = [F.col(c) for c in col_names]\n    col_datatypes = [dataset.schema[column].dataType for column in col_names]\n    casted_cols = (\n        self._cast_input_columns(columns, col_datatypes)\n        if ingress\n        else self._cast_output_columns(columns, col_datatypes)\n    )\n    for column_name, column_tuple in zip(col_names, casted_cols):\n        column = column_tuple[0]\n        column_is_cast = column_tuple[1]\n        if ingress:\n            if column_is_cast:\n                # If we are casting inputs, we create new columns temporarily\n                # to avoid overwriting the original columns and creating\n                # inconsistencies if multiple transforms use the same inputs\n                tmp_input_name = self._resolve_tmp_from_true_column_name(\n                    column_name, suffix=self.tmp_column_suffix\n                )\n                dataset = dataset.withColumn(tmp_input_name, column)\n        else:\n            # Output casting can just be done directly, since this is already a new\n            # column created by the transform itself\n            if column_is_cast:\n                dataset = dataset.withColumn(column_name, column)\n    return dataset\n</code></pre>"},{"location":"reference/src/kamae/spark/common/spark_operation/#src.kamae.spark.common.spark_operation.SparkOperation._resolve_tmp_from_true_column_name","title":"_resolve_tmp_from_true_column_name  <code>staticmethod</code>","text":"<pre><code>_resolve_tmp_from_true_column_name(\n    column_name, suffix, reverse=False\n)\n</code></pre> <p>Resolves the temporary column name from the true column name or vice versa.</p> <p>Used to rename input columns when casting is necessary to avoid overwriting the original columns.</p> <p>:param column_name: The current column name. :param suffix: The suffix to append to the column name. :param reverse: Whether to resolve the temporary column name from the true column name or vice versa.</p> Source code in <code>src/kamae/spark/common/spark_operation.py</code> <pre><code>@staticmethod\ndef _resolve_tmp_from_true_column_name(\n    column_name: str, suffix: str, reverse: bool = False\n) -&gt; str:\n    \"\"\"\n    Resolves the temporary column name from the true column name or vice versa.\n\n    Used to rename input columns when casting is necessary to avoid overwriting\n    the original columns.\n\n    :param column_name: The current column name.\n    :param suffix: The suffix to append to the column name.\n    :param reverse: Whether to resolve the temporary column name from the true\n    column name or vice versa.\n    \"\"\"\n    return (\n        f\"{column_name}_{suffix}\"\n        if not reverse\n        else column_name.replace(f\"_{suffix}\", \"\")\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/common/spark_operation/#src.kamae.spark.common.spark_operation.SparkOperation.drop_tmp_casted_input_columns","title":"drop_tmp_casted_input_columns","text":"<pre><code>drop_tmp_casted_input_columns(dataset)\n</code></pre> <p>Drops the temporary columns from the dataset that are created when casting the input columns.</p> <p>:param dataset: The input dataset. :returns: The dataset with the temporary columns dropped.</p> Source code in <code>src/kamae/spark/common/spark_operation.py</code> <pre><code>def drop_tmp_casted_input_columns(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Drops the temporary columns from the dataset that are created when casting the\n    input columns.\n\n    :param dataset: The input dataset.\n    :returns: The dataset with the temporary columns dropped.\n    \"\"\"\n    for column_name in self._get_single_or_multi_col(ingress=True):\n        tmp_input_name = self._resolve_tmp_from_true_column_name(\n            column_name, suffix=self.tmp_column_suffix\n        )\n        if tmp_input_name in dataset.columns:\n            dataset = dataset.drop(tmp_input_name)\n    return dataset\n</code></pre>"},{"location":"reference/src/kamae/spark/common/spark_operation/#src.kamae.spark.common.spark_operation.SparkOperation.generate_tmp_column_suffix","title":"generate_tmp_column_suffix  <code>staticmethod</code>","text":"<pre><code>generate_tmp_column_suffix(str_len=25)\n</code></pre> <p>Returns a random string of length <code>str_len</code> to append to temporary columns.</p> <p>This ensures that there is minimal collision between temporary columns created and the original columns.</p> <p>:param str_len: Length of the random string. :returns: Random string of length <code>str_len</code>.</p> Source code in <code>src/kamae/spark/common/spark_operation.py</code> <pre><code>@staticmethod\ndef generate_tmp_column_suffix(str_len: int = 25) -&gt; str:\n    \"\"\"\n    Returns a random string of length `str_len` to append to temporary columns.\n\n    This ensures that there is minimal collision between temporary columns created\n    and the original columns.\n\n    :param str_len: Length of the random string.\n    :returns: Random string of length `str_len`.\n    \"\"\"\n    return \"\".join(choice(ascii_uppercase) for _ in range(str_len))\n</code></pre>"},{"location":"reference/src/kamae/spark/common/spark_operation/#src.kamae.spark.common.spark_operation.SparkOperation.setParams","title":"setParams","text":"<pre><code>setParams(**kwargs)\n</code></pre> <p>Sets all given keyword parameters.</p> <p>:returns: Class instance.</p> Source code in <code>src/kamae/spark/common/spark_operation.py</code> <pre><code>@keyword_only\ndef setParams(self, **kwargs: Any) -&gt; \"SparkOperation\":\n    \"\"\"\n    Sets all given keyword parameters.\n\n    :returns: Class instance.\n    \"\"\"\n    input_kwargs = self._input_kwargs\n    if (\n        \"inputCol\" in input_kwargs\n        and \"inputCols\" in input_kwargs\n        and input_kwargs[\"inputCol\"] is not None\n        and input_kwargs[\"inputCols\"] is not None\n    ):\n        raise ValueError(\"Only one of inputCol or inputCols can be set, not both.\")\n    for param_name, param_value in input_kwargs.items():\n        # Only if the param value is not None, do we set the param\n        if param_value is not None:\n            # Get the setter method for the parameter\n            setter_method_name = f\"set{param_name[0].upper()}{param_name[1:]}\"\n            setter_method = getattr(self, setter_method_name)\n            # Set the parameter\n            setter_method(param_value)\n    return self\n</code></pre>"},{"location":"reference/src/kamae/spark/common/spark_operation/#src.kamae.spark.common.spark_operation.SparkOperation.set_input_columns_to_from_casted","title":"set_input_columns_to_from_casted","text":"<pre><code>set_input_columns_to_from_casted(\n    dataset, suffix, reverse=False\n)\n</code></pre> <p>Sets the input columns to the temporary casted columns or back if reverse is True.</p> <p>:param dataset: The input dataset. Used solely to understand if a tmp column has been created. These are only created if the column needed to be cast. :param suffix: The suffix to append to the column name. :param reverse: Whether to set the input columns back to the original columns. :returns: None</p> Source code in <code>src/kamae/spark/common/spark_operation.py</code> <pre><code>def set_input_columns_to_from_casted(\n    self, dataset: DataFrame, suffix: str, reverse: bool = False\n) -&gt; None:\n    \"\"\"\n    Sets the input columns to the temporary casted columns or back if reverse is\n    True.\n\n    :param dataset: The input dataset. Used solely to understand if a tmp column\n    has been created. These are only created if the column needed to be cast.\n    :param suffix: The suffix to append to the column name.\n    :param reverse: Whether to set the input columns back to the original columns.\n    :returns: None\n    \"\"\"\n    col_names = self._get_single_or_multi_col(ingress=True)\n    renamed_columns = [\n        self._resolve_tmp_from_true_column_name(column_name, suffix, reverse)\n        for column_name in col_names\n    ]\n    # Check if renaming is necessary. If the renamed column is not present in the\n    # dataset, we should not set the input columns to the renamed columns.\n    new_col_names = []\n    for original_column_name, renamed_column_name in zip(\n        col_names, renamed_columns\n    ):\n        if renamed_column_name in dataset.columns:\n            new_col_names.append(renamed_column_name)\n        else:\n            new_col_names.append(original_column_name)\n\n    if self.hasParam(\"inputCols\") and self.isDefined(\"inputCols\"):\n        self._set(inputCols=new_col_names)\n    elif self.hasParam(\"inputCol\") and self.isDefined(\"inputCol\"):\n        self._set(inputCol=new_col_names[0])\n    else:\n        raise ValueError(\"No input columns to set\")\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/","title":"estimators","text":""},{"location":"reference/src/kamae/spark/estimators/base/","title":"base","text":""},{"location":"reference/src/kamae/spark/estimators/base/#src.kamae.spark.estimators.base.BaseEstimator","title":"BaseEstimator","text":"<pre><code>BaseEstimator()\n</code></pre> <p>               Bases: <code>Estimator</code>, <code>SparkOperation</code></p> Source code in <code>src/kamae/spark/estimators/base.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"\n    Initializes the estimator.\n    \"\"\"\n    super().__init__()\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/base/#src.kamae.spark.estimators.base.BaseEstimator.construct_layer_info","title":"construct_layer_info","text":"<pre><code>construct_layer_info()\n</code></pre> <p>Constructs the layer info dictionary. Contains the layer name, the tensorflow layer, and the inputs and outputs. This is used when constructing the pipeline graph.</p> <p>layer is set to None because estimators do not have a defined tf computation, we will not use this information in the pipeline graph (for estimators).</p> <p>:returns: Dictionary containing layer information such as name, tensorflow layer, inputs, and outputs.</p> Source code in <code>src/kamae/spark/estimators/base.py</code> <pre><code>def construct_layer_info(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Constructs the layer info dictionary.\n    Contains the layer name, the tensorflow layer, and the inputs and outputs.\n    This is used when constructing the pipeline graph.\n\n    layer is set to None because estimators do not have a defined tf computation,\n    we will not use this information in the pipeline graph (for estimators).\n\n    :returns: Dictionary containing layer information such as\n    name, tensorflow layer, inputs, and outputs.\n    \"\"\"\n    inputs, outputs = self.get_layer_inputs_outputs()\n    return {\n        \"name\": self.getOrDefault(\"layerName\"),\n        # Estimators do not have a defined tf computation, only transformers do.\n        \"layer\": None,\n        \"inputs\": inputs,\n        \"outputs\": outputs,\n    }\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/base/#src.kamae.spark.estimators.base.BaseEstimator.fit","title":"fit","text":"<pre><code>fit(dataset, params=None)\n</code></pre> <p>Overrides the fit method of the parent class to add casting of input columns to the preferred data type.</p> <p>:param dataset: Input dataset. :param params: Optional additional parameters. :returns: Fitted transformer companion object.</p> Source code in <code>src/kamae/spark/estimators/base.py</code> <pre><code>def fit(\n    self,\n    dataset: DataFrame,\n    params: Optional[Union[\"ParamMap\", List[\"ParamMap\"], Tuple[\"ParamMap\"]]] = None,\n) -&gt; BaseTransformer:\n    \"\"\"\n    Overrides the fit method of the parent class to add casting of input columns\n    to the preferred data type.\n\n    :param dataset: Input dataset.\n    :param params: Optional additional parameters.\n    :returns: Fitted transformer companion object.\n    \"\"\"\n    try:\n        dataset = self._create_casted_input_output_columns(\n            dataset=dataset, ingress=True\n        )\n        self._check_input_dtypes_compatible(\n            dataset, self._get_single_or_multi_col(ingress=True)\n        )\n\n        # Set estimator input columns to casted columns\n        self.set_input_columns_to_from_casted(\n            dataset=dataset,\n            suffix=self.tmp_column_suffix,\n        )\n\n        # Replicate the logic from the existing abstract estimator fit method\n        transformer = super().fit(dataset, params)\n\n        # Reset input columns from casted columns\n        self.set_input_columns_to_from_casted(\n            dataset=dataset,\n            suffix=self.tmp_column_suffix,\n            reverse=True,\n        )\n\n        # Reset input columns for transformer\n        transformer.set_input_columns_to_from_casted(\n            dataset=dataset,\n            suffix=self.tmp_column_suffix,\n            reverse=True,\n        )\n\n        return transformer\n\n    except Exception as e:\n        param_dict = {\n            param[0].name: param[1] for param in self.extractParamMap().items()\n        }\n        raise e.__class__(\n            f\"Error in estimator: {self.uid} with params: {param_dict}\"\n        ).with_traceback(e.__traceback__)\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/conditional_standard_scale/","title":"conditional_standard_scale","text":""},{"location":"reference/src/kamae/spark/estimators/conditional_standard_scale/#src.kamae.spark.estimators.conditional_standard_scale.ConditionalStandardScaleEstimator","title":"ConditionalStandardScaleEstimator","text":"<pre><code>ConditionalStandardScaleEstimator(\n    inputCol=None,\n    outputCol=None,\n    layerName=None,\n    scalingFunction=\"standard\",\n    inputDtype=None,\n    outputDtype=None,\n    maskCols=None,\n    maskOperators=None,\n    maskValues=None,\n    relevanceCol=None,\n    skipZeros=False,\n    epsilon=0,\n    nanFillValue=None,\n)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>SingleInputSingleOutputParams</code>, <code>ConditionalStandardScaleEstimatorParams</code>, <code>StandardScaleSkipZerosParams</code>, <code>NanFillValueParams</code></p> <p>Conditional standard scaler estimator for use in Spark pipelines. This is used to calculate the mean and standard deviation with masking, and then to standardize/transform the input column using the mean and standard deviation, optionally skipping the standardization of inputs equal to zero. The mask columns, mask values and mask conditions are used to calculate the moments only when all the masking conditions are satisfied. The skip_zeros parameter allows to apply both the scaling and the transformation only when input is not equal to zero. If equal to zero, it will remain zero in the output value as it was in the input value. It is also possible to specify a non-standard scaling function using the scalingFunction parameter. When fit is called it returns a ConditionalStandardScaleTransformer which can be used to standardize/transform the input data.</p> <p>WARNING: If the input is an array, we assume that the array has a constant shape across all rows.</p> <p>:param inputCol: Input column name to standardize. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param scalingFunction: The scaling function to use: 'standard', 'binary'. :param maskCols: Columns on which to apply the mask values. Defaults to None. :param maskOperators: Operators to use in each masking condition: eq, neq, lt, gt, leq, geq. Defaults to 'neq'. :param maskValues: Values applied to the maskCols which makes the value ignored in the calculation of the moments. Defaults to None. :param relevanceCol: The name of the relevance column to use during spark fit function. :param skipZeros: If True, during spark transform and keras inference, do not apply the scaling when the values to scale are equal to zero. :param epsilon: Small value to add to conditional check of zeros. Valid only when skipZeros is True. Defaults to 0. :param nanFillValue: Value to fill NaNs with after scaling. It is important to use it if epsilon filters out all the values. Defaults to None. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/estimators/conditional_standard_scale.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    layerName: Optional[str] = None,\n    scalingFunction: str = \"standard\",\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    maskCols: Optional[List[str]] = None,\n    maskOperators: Optional[List[str]] = None,\n    maskValues: Optional[List[float]] = None,\n    relevanceCol: Optional[str] = None,\n    skipZeros: bool = False,\n    epsilon: float = 0,\n    nanFillValue: Optional[float] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes a ConditionalStandardScaleEstimator estimator.\n    Sets all parameters to given inputs.\n\n    :param inputCol: Input column name to standardize.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param scalingFunction: The scaling function to use: 'standard', 'binary'.\n    :param maskCols: Columns on which to apply the mask values. Defaults to None.\n    :param maskOperators: Operators to use in each masking condition:\n    eq, neq, lt, gt, leq, geq. Defaults to 'neq'.\n    :param maskValues: Values applied to the maskCols which makes the value\n    ignored in the calculation of the moments. Defaults to None.\n    :param relevanceCol: The name of the relevance column to use during spark\n    fit function.\n    :param skipZeros: If True, during spark transform and keras inference,\n    do not apply the scaling when the values to scale are equal to zero.\n    :param epsilon: Small value to add to conditional check of zeros. Valid only\n    when skipZeros is True. Defaults to 0.\n    :param nanFillValue: Value to fill NaNs with after scaling. It is important\n    to use it if epsilon filters out all the values. Defaults to None.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(\n        scalingFunction=\"standard\",\n        maskCols=None,\n        maskOperators=None,\n        maskValues=None,\n        relevanceCol=None,\n        skipZeros=False,\n        epsilon=0,\n        nanFillValue=None,\n    )\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/conditional_standard_scale/#src.kamae.spark.estimators.conditional_standard_scale.ConditionalStandardScaleEstimator.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible data types for transformer. :returns: List of compatible data types.</p>"},{"location":"reference/src/kamae/spark/estimators/conditional_standard_scale/#src.kamae.spark.estimators.conditional_standard_scale.ConditionalStandardScaleEstimator._fit","title":"_fit","text":"<pre><code>_fit(dataset)\n</code></pre> <p>Fits the ConditionalStandardScaleEstimator estimator to the given dataset. Calculates the mean and standard deviation of the input feature column and returns a StandardScaleTransformer with the mean and standard deviation set.</p> <p>:param dataset: Pyspark dataframe to fit the estimator to. :returns: ConditionalStandardScaleEstimator instance with mean &amp; standard deviation set.</p> Source code in <code>src/kamae/spark/estimators/conditional_standard_scale.py</code> <pre><code>def _fit(self, dataset: DataFrame) -&gt; \"ConditionalStandardScaleTransformer\":\n    \"\"\"\n    Fits the ConditionalStandardScaleEstimator estimator to the given dataset.\n    Calculates the mean and standard deviation of the input feature column and\n    returns a StandardScaleTransformer with the mean and standard deviation set.\n\n    :param dataset: Pyspark dataframe to fit the estimator to.\n    :returns: ConditionalStandardScaleEstimator instance with\n    mean &amp; standard deviation set.\n    \"\"\"\n    input_column_dtype = self.get_column_datatype(dataset, self.getInputCol())\n    if not isinstance(input_column_dtype, ArrayType):\n        input_col = F.array(F.col(self.getInputCol()))\n        input_column_dtype = ArrayType(input_column_dtype)\n    else:\n        input_col = F.col(self.getInputCol())\n\n    # Masks are applied to the dataset before calculating the moments\n    self._validate_mask_ops()\n    if self.getMaskCols() is not None:\n        mask_cols = self.getMaskCols()\n        for i in range(len(mask_cols)):\n            mask_col = mask_cols[i]\n            if mask_col not in dataset.columns:\n                raise ValueError(f\"Mask column {mask_col} not found in dataset.\")\n            mask_op = get_condition_operator(self.getMaskOperators()[i])\n            mask_val = self.getMaskValues()[i]\n            dataset = dataset.filter(mask_op(F.col(mask_col), mask_val))\n\n    # Collect a single row to driver and get the length.\n    # We assume all subsequent rows have the same length.\n    row = dataset.select(input_col).first()\n    if row is None:\n        raise ValueError(\"No data left after application of mask conditions.\")\n    array_size = np.array((row[0])).shape[-1]\n\n    # Calculate the moments\n    if self.getScalingFunction().lower() == \"standard\":\n        return self._fit_standard(\n            dataset, input_col, input_column_dtype, array_size\n        )\n    elif self.getScalingFunction().lower() == \"binary\":\n        return self._fit_binary(dataset, input_col, input_column_dtype, array_size)\n    else:\n        raise ValueError(f\"Unknown scaling function: {self.getScalingFunction()}.\")\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/conditional_standard_scale/#src.kamae.spark.estimators.conditional_standard_scale.ConditionalStandardScaleEstimator._fit_binary","title":"_fit_binary","text":"<pre><code>_fit_binary(\n    dataset, input_column, input_column_dtype, array_size\n)\n</code></pre> <p>Fits the ConditionalStandardScaleEstimator estimator with the binary scaling function. In this case, the relevance column name must be set. This should be used only when input is a binary variable and the label can be transformed into a classification task (if relevance &gt; 0 then 1 else 0).</p> <p>With this function, the mean and stddev are:     mean = 1 - (f/n)     stddev = sqrt((f * pow(1-(f/n), 2) + (n-f) * pow(0-(f/n), 2)) / (n-1)) where:     n = sum(when(x==1, 1, 0))     f = sum(when(x==1 &amp;&amp; relevance &gt; 0, 1, 0))</p> <p>:param dataset: Pyspark dataframe to fit the estimator to. :param array_size: The size of the array to standardize. :returns: ConditionalStandardScaleEstimator instance with mean &amp; standard deviation set.</p> Source code in <code>src/kamae/spark/estimators/conditional_standard_scale.py</code> <pre><code>def _fit_binary(\n    self,\n    dataset: DataFrame,\n    input_column: Column,\n    input_column_dtype: DataType,\n    array_size: int,\n) -&gt; \"ConditionalStandardScaleTransformer\":\n    \"\"\"\n    Fits the ConditionalStandardScaleEstimator estimator with\n    the binary scaling function. In this case, the relevance\n    column name must be set. This should be used only when input\n    is a binary variable and the label can be transformed into a\n    classification task (if relevance &gt; 0 then 1 else 0).\n\n    With this function, the mean and stddev are:\n        mean = 1 - (f/n)\n        stddev = sqrt((f * pow(1-(f/n), 2) + (n-f) * pow(0-(f/n), 2)) / (n-1))\n    where:\n        n = sum(when(x==1, 1, 0))\n        f = sum(when(x==1 &amp;&amp; relevance &gt; 0, 1, 0))\n\n    :param dataset: Pyspark dataframe to fit the estimator to.\n    :param array_size: The size of the array to standardize.\n    :returns: ConditionalStandardScaleEstimator instance with\n    mean &amp; standard deviation set.\n    \"\"\"\n    if self.getRelevanceCol() is None:\n        raise ValueError(\"Relevance column must be set for binary scaling.\")\n\n    # Construct the elements to calculate the moments\n    element_struct = construct_nested_elements_for_scaling(\n        column=input_column,\n        column_datatype=input_column_dtype,\n        array_dim=array_size,\n    )\n\n    count_cols = [\n        F.sum(\n            F.when(\n                F.col(f\"element_struct.element_{i}\") == F.lit(1),\n                1,\n            ).otherwise(0)\n        ).alias(f\"count_{i}\")\n        for i in range(1, array_size + 1)\n    ]\n    count_ones_cols = [\n        F.sum(\n            F.when(\n                (F.col(f\"element_struct.element_{i}\") == F.lit(1))\n                &amp; (F.col(self.getRelevanceCol()) &gt; 0),\n                1,\n            ).otherwise(0)\n        ).alias(f\"count_ones_{i}\")\n        for i in range(1, array_size + 1)\n    ]\n\n    # apply the aggregations\n    metric_cols = count_cols + count_ones_cols\n    metrics_dict = (\n        dataset.withColumn(\"element_struct\", element_struct)\n        .agg(*metric_cols)\n        .first()\n        .asDict()\n    )\n    count = [metrics_dict[f\"count_{i}\"] for i in range(1, array_size + 1)]\n    count_ones = [metrics_dict[f\"count_ones_{i}\"] for i in range(1, array_size + 1)]\n    if self.getNanFillValue() is not None:\n        fill_val = self.getNanFillValue()\n        count = [fill_val if c is None else c for c in count]\n        count_ones = [fill_val if c is None else c for c in count_ones]\n    mean, stddev = self._get_binary_moments(np.array(count_ones), np.array(count))\n    return ConditionalStandardScaleTransformer(\n        inputCol=self.getInputCol(),\n        outputCol=self.getOutputCol(),\n        layerName=self.getLayerName(),\n        inputDtype=self.getInputDtype(),\n        outputDtype=self.getOutputDtype(),\n        mean=mean.tolist(),\n        stddev=stddev.tolist(),\n        skipZeros=self.getSkipZeros(),\n        epsilon=self.getEpsilon(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/conditional_standard_scale/#src.kamae.spark.estimators.conditional_standard_scale.ConditionalStandardScaleEstimator._fit_standard","title":"_fit_standard","text":"<pre><code>_fit_standard(\n    dataset, input_column, input_column_dtype, array_size\n)\n</code></pre> <p>Fits the ConditionalStandardScaleEstimator estimator with the standard scaling function. This should be the default function for the scaling operation.</p> <p>:param dataset: Pyspark dataframe to fit the estimator to. :param array_size: The size of the array to standardize. :returns: ConditionalStandardScaleEstimator instance with mean &amp; standard deviation set.</p> Source code in <code>src/kamae/spark/estimators/conditional_standard_scale.py</code> <pre><code>def _fit_standard(\n    self,\n    dataset: DataFrame,\n    input_column: Column,\n    input_column_dtype: DataType,\n    array_size: int,\n) -&gt; \"ConditionalStandardScaleTransformer\":\n    \"\"\"\n    Fits the ConditionalStandardScaleEstimator estimator with\n    the standard scaling function.\n    This should be the default function for the scaling operation.\n\n    :param dataset: Pyspark dataframe to fit the estimator to.\n    :param array_size: The size of the array to standardize.\n    :returns: ConditionalStandardScaleEstimator instance with\n    mean &amp; standard deviation set.\n    \"\"\"\n    # Construct the elements to calculate the moments\n    element_struct = construct_nested_elements_for_scaling(\n        column=input_column,\n        column_datatype=input_column_dtype,\n        array_dim=array_size,\n    )\n    # Defaults\n    mean_cols = [\n        F.mean(F.col(f\"element_struct.element_{i}\")).alias(f\"mean_{i}\")\n        for i in range(1, array_size + 1)\n    ]\n    stddev_cols = [\n        F.stddev_pop(F.col(f\"element_struct.element_{i}\")).alias(f\"stddev_{i}\")\n        for i in range(1, array_size + 1)\n    ]\n    # Use relevance column and skip zeros (with epsilon)\n    if self.getSkipZeros() and (self.getRelevanceCol() is not None):\n        eps = self.getEpsilon()\n        mean_cols = [\n            F.mean(\n                F.when(\n                    # x != (0 +- eps)\n                    (F.abs(F.col(f\"element_struct.element_{i}\")) &gt; F.lit(eps))\n                    &amp; (F.col(self.getRelevanceCol()) &gt; 0),\n                    F.col(f\"element_struct.element_{i}\"),\n                )\n            ).alias(f\"mean_{i}\")\n            for i in range(1, array_size + 1)\n        ]\n        stddev_cols = [\n            F.stddev_pop(\n                F.when(\n                    # x != (0 +- eps)\n                    (F.abs(F.col(f\"element_struct.element_{i}\")) &gt; F.lit(eps))\n                    &amp; (F.col(self.getRelevanceCol()) &gt; 0),\n                    F.col(f\"element_struct.element_{i}\"),\n                )\n            ).alias(f\"stddev_{i}\")\n            for i in range(1, array_size + 1)\n        ]\n    # Use relevance column\n    elif self.getRelevanceCol() is not None:\n        mean_cols = [\n            F.mean(\n                F.when(\n                    (F.col(self.getRelevanceCol()) &gt; 0),\n                    F.col(f\"element_struct.element_{i}\"),\n                )\n            ).alias(f\"mean_{i}\")\n            for i in range(1, array_size + 1)\n        ]\n        stddev_cols = [\n            F.stddev_pop(\n                F.when(\n                    (F.col(self.getRelevanceCol()) &gt; 0),\n                    F.col(f\"element_struct.element_{i}\"),\n                )\n            ).alias(f\"stddev_{i}\")\n            for i in range(1, array_size + 1)\n        ]\n    # Skip zeros on fit (with epsilon)\n    elif self.getSkipZeros():\n        eps = self.getEpsilon()\n        mean_cols = [\n            F.mean(\n                F.when(\n                    # x != (0 +- eps)\n                    F.abs(F.col(f\"element_struct.element_{i}\")) &gt; F.lit(eps),\n                    F.col(f\"element_struct.element_{i}\"),\n                ),\n            ).alias(f\"mean_{i}\")\n            for i in range(1, array_size + 1)\n        ]\n        stddev_cols = [\n            F.stddev_pop(\n                F.when(\n                    # x != (0 +- eps)\n                    F.abs(F.col(f\"element_struct.element_{i}\")) &gt; F.lit(eps),\n                    F.col(f\"element_struct.element_{i}\"),\n                ),\n            ).alias(f\"stddev_{i}\")\n            for i in range(1, array_size + 1)\n        ]\n    # apply the aggregations\n    metric_cols = mean_cols + stddev_cols\n    mean_and_stddev_dict = (\n        dataset.withColumn(\"element_struct\", element_struct)\n        .agg(*metric_cols)\n        .first()\n        .asDict()\n    )\n    mean = [mean_and_stddev_dict[f\"mean_{i}\"] for i in range(1, array_size + 1)]\n    stddev = [mean_and_stddev_dict[f\"stddev_{i}\"] for i in range(1, array_size + 1)]\n    if self.getNanFillValue() is not None:\n        fill_val = self.getNanFillValue()\n        mean = [fill_val if m is None else m for m in mean]\n        stddev = [fill_val if s is None else s for s in stddev]\n    return ConditionalStandardScaleTransformer(\n        inputCol=self.getInputCol(),\n        outputCol=self.getOutputCol(),\n        layerName=self.getLayerName(),\n        inputDtype=self.getInputDtype(),\n        outputDtype=self.getOutputDtype(),\n        mean=mean,\n        stddev=stddev,\n        skipZeros=self.getSkipZeros(),\n        epsilon=self.getEpsilon(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/conditional_standard_scale/#src.kamae.spark.estimators.conditional_standard_scale.ConditionalStandardScaleEstimator._get_binary_moments","title":"_get_binary_moments","text":"<pre><code>_get_binary_moments(f, n)\n</code></pre> <p>Calculates the moments for a binary variable.</p> <p>:param f: Number of samples where feature is one/true. :param n: Number of samples. :returns:     - mean - The mean of the distribution.     - stddev - The stddev of the distribution.</p> Source code in <code>src/kamae/spark/estimators/conditional_standard_scale.py</code> <pre><code>def _get_binary_moments(self, f: NDArray, n: NDArray) -&gt; (NDArray, NDArray):\n    \"\"\"\n    Calculates the moments for a binary variable.\n\n    :param f: Number of samples where feature is one/true.\n    :param n: Number of samples.\n    :returns:\n        - mean - The mean of the distribution.\n        - stddev - The stddev of the distribution.\n    \"\"\"\n    true_ratio = np.where(n &lt;= 0, 0, f / n)\n    variance = np.where(\n        (n - 1) &lt;= 0,\n        0,\n        (f * pow(1 - true_ratio, 2) + (n - f) * pow(0 - true_ratio, 2)) / (n - 1),\n    )\n    mean = 1 - true_ratio\n    stddev = np.sqrt(variance)\n    return mean, stddev\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/conditional_standard_scale/#src.kamae.spark.estimators.conditional_standard_scale.ConditionalStandardScaleEstimator._validate_mask_ops","title":"_validate_mask_ops","text":"<pre><code>_validate_mask_ops()\n</code></pre> <p>Validates the mask operators. Mask columns, operators and values must be set together and must have the same length.</p> Source code in <code>src/kamae/spark/estimators/conditional_standard_scale.py</code> <pre><code>def _validate_mask_ops(self) -&gt; None:\n    \"\"\"\n    Validates the mask operators.\n    Mask columns, operators and values must be set together\n    and must have the same length.\n    \"\"\"\n    arr1 = self.getMaskCols()\n    arr2 = self.getMaskOperators()\n    arr3 = self.getMaskValues()\n    if any(arr is not None for arr in [arr1, arr2, arr3]):\n        if any(arr is None or len(arr) != len(arr1) for arr in [arr1, arr2, arr3]):\n            raise ValueError(\n                \"Mask columns, operators and values must be set together \"\n                \"and must have the same length.\"\n            )\n    return\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/conditional_standard_scale/#src.kamae.spark.estimators.conditional_standard_scale.ConditionalStandardScaleEstimatorParams","title":"ConditionalStandardScaleEstimatorParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing conditional standard scale parameters, needed for single feature array scaler layers.</p>"},{"location":"reference/src/kamae/spark/estimators/conditional_standard_scale/#src.kamae.spark.estimators.conditional_standard_scale.ConditionalStandardScaleEstimatorParams.getMaskCols","title":"getMaskCols","text":"<pre><code>getMaskCols()\n</code></pre> <p>Gets the maskCols parameter. :returns: List of string values of the mask column.</p> Source code in <code>src/kamae/spark/estimators/conditional_standard_scale.py</code> <pre><code>def getMaskCols(self) -&gt; List[str]:\n    \"\"\"\n    Gets the maskCols parameter.\n    :returns: List of string values of the mask column.\n    \"\"\"\n    return self.getOrDefault(self.maskCols)\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/conditional_standard_scale/#src.kamae.spark.estimators.conditional_standard_scale.ConditionalStandardScaleEstimatorParams.getMaskOperators","title":"getMaskOperators","text":"<pre><code>getMaskOperators()\n</code></pre> <p>Gets the maskOperators parameter.</p> <p>:returns: List of string values describing the operators to use in conditions: - eq - neq - lt - gt - leq - geq</p> Source code in <code>src/kamae/spark/estimators/conditional_standard_scale.py</code> <pre><code>def getMaskOperators(self) -&gt; List[str]:\n    \"\"\"\n    Gets the maskOperators parameter.\n\n    :returns: List of string values describing the operators to use in conditions:\n    - eq\n    - neq\n    - lt\n    - gt\n    - leq\n    - geq\n    \"\"\"\n    return self.getOrDefault(self.maskOperators)\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/conditional_standard_scale/#src.kamae.spark.estimators.conditional_standard_scale.ConditionalStandardScaleEstimatorParams.getMaskValues","title":"getMaskValues","text":"<pre><code>getMaskValues()\n</code></pre> <p>Gets the maskValues parameter.</p> <p>:returns: List of float values of the mask values.</p> Source code in <code>src/kamae/spark/estimators/conditional_standard_scale.py</code> <pre><code>def getMaskValues(self) -&gt; List[float]:\n    \"\"\"\n    Gets the maskValues parameter.\n\n    :returns: List of float values of the mask values.\n    \"\"\"\n    return self.getOrDefault(self.maskValues)\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/conditional_standard_scale/#src.kamae.spark.estimators.conditional_standard_scale.ConditionalStandardScaleEstimatorParams.getRelevanceCol","title":"getRelevanceCol","text":"<pre><code>getRelevanceCol()\n</code></pre> <p>Gets the relevanceCol parameter.</p> <p>:returns: String value of the relevance column.</p> Source code in <code>src/kamae/spark/estimators/conditional_standard_scale.py</code> <pre><code>def getRelevanceCol(self) -&gt; str:\n    \"\"\"\n    Gets the relevanceCol parameter.\n\n    :returns: String value of the relevance column.\n    \"\"\"\n    return self.getOrDefault(self.relevanceCol)\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/conditional_standard_scale/#src.kamae.spark.estimators.conditional_standard_scale.ConditionalStandardScaleEstimatorParams.getScalingFunction","title":"getScalingFunction","text":"<pre><code>getScalingFunction()\n</code></pre> <p>Gets the scalingFunction parameter.</p> <p>:returns: Boolean value of the scalingFunction value.</p> Source code in <code>src/kamae/spark/estimators/conditional_standard_scale.py</code> <pre><code>def getScalingFunction(self) -&gt; str:\n    \"\"\"\n    Gets the scalingFunction parameter.\n\n    :returns: Boolean value of the scalingFunction value.\n    \"\"\"\n    return self.getOrDefault(self.scalingFunction)\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/conditional_standard_scale/#src.kamae.spark.estimators.conditional_standard_scale.ConditionalStandardScaleEstimatorParams.setMaskCols","title":"setMaskCols","text":"<pre><code>setMaskCols(value)\n</code></pre> <p>Sets the maskCols parameter.</p> <p>:param value: Columns to use as the mask columns. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/estimators/conditional_standard_scale.py</code> <pre><code>def setMaskCols(\n    self, value: List[str]\n) -&gt; \"ConditionalStandardScaleEstimatorParams\":\n    \"\"\"\n    Sets the maskCols parameter.\n\n    :param value: Columns to use as the mask columns.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(maskCols=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/conditional_standard_scale/#src.kamae.spark.estimators.conditional_standard_scale.ConditionalStandardScaleEstimatorParams.setMaskOperators","title":"setMaskOperators","text":"<pre><code>setMaskOperators(value)\n</code></pre> <p>Sets the maskOperators parameter.</p> <p>:param value: String value describing the operator to use in condition: - eq - neq - lt - gt - leq - geq :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/estimators/conditional_standard_scale.py</code> <pre><code>def setMaskOperators(\n    self, value: List[str]\n) -&gt; \"ConditionalStandardScaleEstimatorParams\":\n    \"\"\"\n    Sets the maskOperators parameter.\n\n    :param value: String value describing the operator to use in condition:\n    - eq\n    - neq\n    - lt\n    - gt\n    - leq\n    - geq\n    :returns: Instance of class mixed in.\n    \"\"\"\n    allowed_operators = [\"eq\", \"neq\", \"lt\", \"gt\", \"leq\", \"geq\"]\n    for v in value:\n        if v not in allowed_operators:\n            raise ValueError(\n                f\"conditionOperator must be one of {allowed_operators}, \"\n                f\"but got {value}\"\n            )\n    return self._set(maskOperators=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/conditional_standard_scale/#src.kamae.spark.estimators.conditional_standard_scale.ConditionalStandardScaleEstimatorParams.setMaskValues","title":"setMaskValues","text":"<pre><code>setMaskValues(value)\n</code></pre> <p>Sets the maskValues parameter.</p> <p>:param value: List of float values to use as the mask value. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/estimators/conditional_standard_scale.py</code> <pre><code>def setMaskValues(\n    self, value: List[float]\n) -&gt; \"ConditionalStandardScaleEstimatorParams\":\n    \"\"\"\n    Sets the maskValues parameter.\n\n    :param value: List of float values to use as the mask value.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(maskValues=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/conditional_standard_scale/#src.kamae.spark.estimators.conditional_standard_scale.ConditionalStandardScaleEstimatorParams.setRelevanceCol","title":"setRelevanceCol","text":"<pre><code>setRelevanceCol(value)\n</code></pre> <p>Sets the relevanceCol parameter.</p> <p>:param value: String value to use as the relevance column. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/estimators/conditional_standard_scale.py</code> <pre><code>def setRelevanceCol(self, value: str) -&gt; \"ConditionalStandardScaleEstimatorParams\":\n    \"\"\"\n    Sets the relevanceCol parameter.\n\n    :param value: String value to use as the relevance column.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(relevanceCol=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/conditional_standard_scale/#src.kamae.spark.estimators.conditional_standard_scale.ConditionalStandardScaleEstimatorParams.setScalingFunction","title":"setScalingFunction","text":"<pre><code>setScalingFunction(value)\n</code></pre> <p>Sets the scalingFunction parameter.</p> <p>:param value: String value to indicate which scaling function to use. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/estimators/conditional_standard_scale.py</code> <pre><code>def setScalingFunction(\n    self,\n    value: str,\n) -&gt; \"ConditionalStandardScaleEstimatorParams\":\n    \"\"\"\n    Sets the scalingFunction parameter.\n\n    :param value: String value to indicate which scaling function to use.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if value.lower() == \"standard\":\n        return self._set(scalingFunction=value.lower())\n    elif value.lower() == \"binary\":\n        return self._set(scalingFunction=value.lower())\n    else:\n        raise ValueError(f\"Unknown scaling function: {value}.\")\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/impute/","title":"impute","text":""},{"location":"reference/src/kamae/spark/estimators/impute/#src.kamae.spark.estimators.impute.ImputeEstimator","title":"ImputeEstimator","text":"<pre><code>ImputeEstimator(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    maskValue=None,\n    imputeMethod=None,\n)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>SingleInputSingleOutputParams</code>, <code>MaskValueParams</code>, <code>ImputeMethodParams</code></p> <p>Imputation estimator for use in Spark pipelines. This estimator is used to calculate the chosen statistic of the input feature column. When fit is called it returns a ImputeTransformer which can be used to impute either the mean or median of a column. Rows are not included in the calculation of the statistic when they are either null or equal to the supplied mask value.</p> <p>:param inputCol: Input column name to standardize. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer  in the keras model. If not set, we use the uid of the Spark transformer. :param maskValue: Value which to ignore, in addition to nulls, when computing imputation statistic. This is also the value that is imputed over in TF at inference. :param imputeMethod: Method by which to compute the value to be imputed. Valid values are \"mean\" or \"median\". :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/estimators/impute.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    maskValue: Optional[Union[float, int, str]] = None,\n    imputeMethod: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes a ImputeEstimator estimator.\n    Sets all parameters to given inputs.\n\n    :param inputCol: Input column name to standardize.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n     in the keras model. If not set, we use the uid of the Spark transformer.\n    :param maskValue: Value which to ignore, in addition to nulls, when\n    computing imputation statistic.\n    This is also the value that is imputed over in TF at inference.\n    :param imputeMethod: Method by which to compute the value to be imputed.\n    Valid values are \"mean\" or \"median\".\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(imputeMethod=\"mean\")\n    self.valid_impute_methods = [\"mean\", \"median\"]\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/impute/#src.kamae.spark.estimators.impute.ImputeEstimator.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/estimators/impute/#src.kamae.spark.estimators.impute.ImputeEstimator._fit","title":"_fit","text":"<pre><code>_fit(dataset)\n</code></pre> <p>Fits the ImputeEstimator estimator to the given dataset. Calculates the imputation statistic of the input feature column and returns an ImputeTransformer with the statistic set.</p> <p>:param dataset: Pyspark dataframe to fit the estimator to. :returns: ImputeTransformer instance with impute value set.</p> Source code in <code>src/kamae/spark/estimators/impute.py</code> <pre><code>def _fit(self, dataset: DataFrame) -&gt; \"ImputeTransformer\":\n    \"\"\"\n    Fits the ImputeEstimator estimator to the given dataset.\n    Calculates the imputation statistic of the input feature column and\n    returns an ImputeTransformer with the statistic set.\n\n    :param dataset: Pyspark dataframe to fit the estimator to.\n    :returns: ImputeTransformer instance with impute value set.\n    \"\"\"\n    imputeMethod = self.getImputeMethod()\n\n    if imputeMethod == \"mean\":\n        estimator_fn = F.mean\n    elif imputeMethod == \"median\":\n        estimator_fn = F.median\n\n    input_column_type = self.get_column_datatype(dataset, self.getInputCol())\n    input_col_an_array = isinstance(input_column_type, ArrayType)\n\n    # If the column input is an array then we need to flatten and explode it to\n    # calculate the impute value\n    if input_col_an_array:\n        # Flatten the array to a single array\n        flattened_array_col = flatten_nested_arrays(\n            column=F.col(self.getInputCol()), column_data_type=input_column_type\n        )\n        processed_input_col = F.explode(flattened_array_col).alias(\n            self.uid + \"_input_col\"\n        )\n    else:\n        processed_input_col = F.col(self.getInputCol()).alias(\n            self.uid + \"_input_col\"\n        )\n\n    imputeValue = (\n        dataset.select(processed_input_col)\n        .select(\n            F.when(\n                (F.col(self.uid + \"_input_col\") == F.lit(self.getMaskValue()))\n                | (F.col(self.uid + \"_input_col\").isNull()),\n                None,\n            )\n            .otherwise(F.col(self.uid + \"_input_col\"))\n            .alias(\"input_col\")\n        )\n        .agg(estimator_fn(\"input_col\"))\n        .collect()[0][0]\n    )\n\n    return ImputeTransformer(\n        inputCol=self.getInputCol(),\n        outputCol=self.getOutputCol(),\n        layerName=self.getLayerName(),\n        inputDtype=self.getInputDtype(),\n        outputDtype=self.getOutputDtype(),\n        imputeValue=imputeValue,\n        maskValue=self.getMaskValue(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/min_max_scale/","title":"min_max_scale","text":""},{"location":"reference/src/kamae/spark/estimators/min_max_scale/#src.kamae.spark.estimators.min_max_scale.MinMaxScaleEstimator","title":"MinMaxScaleEstimator","text":"<pre><code>MinMaxScaleEstimator(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    maskValue=None,\n)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>SingleInputSingleOutputParams</code>, <code>MaskValueParams</code></p> <p>Min max estimator for use in Spark pipelines. This estimator is used to calculate the min and max of the input feature column. When fit is called it returns a MinMaxScaleTransformer which can be used to standardize/transform additional features.</p> <p>WARNING: If the input is an array, we assume that the array has a constant shape across all rows.</p> <p>:param inputCol: Input column name to standardize. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer  in the keras model. If not set, we use the uid of the Spark transformer. :param maskValue: Value to use for masking. If set, these values will be ignored during the computation of the min and max values. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/estimators/min_max_scale.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    maskValue: Optional[float] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes a MinMaxScaleEstimator estimator.\n    Sets all parameters to given inputs.\n\n    :param inputCol: Input column name to standardize.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n     in the keras model. If not set, we use the uid of the Spark transformer.\n    :param maskValue: Value to use for masking. If set, these values will be ignored\n    during the computation of the min and max values.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(maskValue=None)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/min_max_scale/#src.kamae.spark.estimators.min_max_scale.MinMaxScaleEstimator.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/estimators/min_max_scale/#src.kamae.spark.estimators.min_max_scale.MinMaxScaleEstimator._fit","title":"_fit","text":"<pre><code>_fit(dataset)\n</code></pre> <p>Fits the MinMaxScaleEstimator estimator to the given dataset. Calculates the min and max of the input feature column and returns a MinMaxScaleTransformer with the min and max set.</p> <p>:param dataset: Pyspark dataframe to fit the estimator to. :returns: MinMaxScaleTransformer instance with min &amp; max set.</p> Source code in <code>src/kamae/spark/estimators/min_max_scale.py</code> <pre><code>def _fit(self, dataset: DataFrame) -&gt; \"MinMaxScaleTransformer\":\n    \"\"\"\n    Fits the MinMaxScaleEstimator estimator to the given dataset.\n    Calculates the min and max of the input feature column and\n    returns a MinMaxScaleTransformer with the min and max set.\n\n    :param dataset: Pyspark dataframe to fit the estimator to.\n    :returns: MinMaxScaleTransformer instance with min &amp; max set.\n    \"\"\"\n    input_column_type = self.get_column_datatype(dataset, self.getInputCol())\n    if not isinstance(input_column_type, ArrayType):\n        input_col = F.array(F.col(self.getInputCol()))\n        input_column_type = ArrayType(input_column_type)\n    else:\n        input_col = F.col(self.getInputCol())\n\n    # Collect a single row to driver and get the length.\n    # We assume all subsequent rows have the same length.\n    array_size = np.array((dataset.select(input_col).first()[0])).shape[-1]\n\n    element_struct = construct_nested_elements_for_scaling(\n        column=input_col,\n        column_datatype=input_column_type,\n        array_dim=array_size,\n    )\n\n    min_cols = [\n        F.min(\n            F.when(\n                F.col(f\"element_struct.element_{i}\") == F.lit(self.getMaskValue()),\n                F.lit(None),\n            ).otherwise(F.col(f\"element_struct.element_{i}\"))\n        ).alias(f\"min_{i}\")\n        for i in range(1, array_size + 1)\n    ]\n\n    max_cols = [\n        F.max(\n            F.when(\n                F.col(f\"element_struct.element_{i}\") == F.lit(self.getMaskValue()),\n                F.lit(None),\n            ).otherwise(F.col(f\"element_struct.element_{i}\"))\n        ).alias(f\"max_{i}\")\n        for i in range(1, array_size + 1)\n    ]\n\n    metric_cols = min_cols + max_cols\n\n    min_and_max_dict = (\n        dataset.select(element_struct).agg(*metric_cols).first().asDict()\n    )\n    min_vals = [min_and_max_dict[f\"min_{i}\"] for i in range(1, array_size + 1)]\n    max_vals = [min_and_max_dict[f\"max_{i}\"] for i in range(1, array_size + 1)]\n\n    return MinMaxScaleTransformer(\n        inputCol=self.getInputCol(),\n        outputCol=self.getOutputCol(),\n        layerName=self.getLayerName(),\n        inputDtype=self.getInputDtype(),\n        outputDtype=self.getOutputDtype(),\n        min=min_vals,\n        max=max_vals,\n        maskValue=self.getMaskValue(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/one_hot_encode/","title":"one_hot_encode","text":""},{"location":"reference/src/kamae/spark/estimators/one_hot_encode/#src.kamae.spark.estimators.one_hot_encode.OneHotEncodeEstimator","title":"OneHotEncodeEstimator","text":"<pre><code>OneHotEncodeEstimator(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    stringOrderType=\"frequencyDesc\",\n    maskToken=None,\n    numOOVIndices=1,\n    dropUnseen=False,\n    maxNumLabels=None,\n)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>DropUnseenParams</code>, <code>SingleInputSingleOutputParams</code>, <code>StringIndexParams</code></p> <p>One-hot encoder Spark Estimator for use in Spark pipelines. This estimator is used to collect all the string labels for a given column. When fit is called it returns a OneHotEncodeTransformer which can be used to create one-hot arrays from additional feature columns using the same string labels.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column(s) to after transforming. :param: stringOrderType: How to order the string indices. Options are 'frequencyAsc', 'frequencyDesc', 'alphabeticalAsc', 'alphabeticalDesc'. Defaults to 'frequencyDesc'. :param maskToken: Token to use for masking. If set, the token will be indexed as 0. :param numOOVIndices: Number of out of vocabulary indices to use. The out of vocabulary indices are used to represent unseen labels and are placed at the beginning of the one-hot encoding. Defaults to 1. :param dropUnseen: Whether to drop unseen label indices. If set to True, the transformer will not add an extra dimension for unseen labels in the one-hot encoding. Defaults to False. :param maxNumLabels: Optional value to limit the size of the vocabulary. Defaults to None to consider the full list. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/estimators/one_hot_encode.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    stringOrderType: str = \"frequencyDesc\",\n    maskToken: Optional[str] = None,\n    numOOVIndices: int = 1,\n    dropUnseen: bool = False,\n    maxNumLabels: Optional[int] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes the OneHotEncoder estimator.\n    Sets all parameters to given inputs.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column(s) to after\n    transforming.\n    :param: stringOrderType: How to order the string indices.\n    Options are 'frequencyAsc', 'frequencyDesc', 'alphabeticalAsc',\n    'alphabeticalDesc'. Defaults to 'frequencyDesc'.\n    :param maskToken: Token to use for masking.\n    If set, the token will be indexed as 0.\n    :param numOOVIndices: Number of out of vocabulary indices to use. The\n    out of vocabulary indices are used to represent unseen labels and are\n    placed at the beginning of the one-hot encoding. Defaults to 1.\n    :param dropUnseen: Whether to drop unseen label indices. If set to True,\n    the transformer will not add an extra dimension for unseen labels in the\n    one-hot encoding. Defaults to False.\n    :param maxNumLabels: Optional value to limit the size of the vocabulary.\n    Defaults to None to consider the full list.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(\n        stringOrderType=\"frequencyDesc\",\n        numOOVIndices=1,\n        dropUnseen=False,\n        maskToken=None,\n        maxNumLabels=None,\n    )\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/one_hot_encode/#src.kamae.spark.estimators.one_hot_encode.OneHotEncodeEstimator.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/estimators/one_hot_encode/#src.kamae.spark.estimators.one_hot_encode.OneHotEncodeEstimator._fit","title":"_fit","text":"<pre><code>_fit(dataset)\n</code></pre> <p>Fits the OneHotEncodeEstimator estimator to the given dataset. Returns a OneHotEncodeTransformer which can be used to one-hot columns using the collected string labels.</p> <p>It re-uses the StringIndexEstimator to collect the string labels.</p> <p>:param dataset: Pyspark dataframe to fit the estimator to. :returns: OneHotEncodeTransformer instance with collected string labels.</p> Source code in <code>src/kamae/spark/estimators/one_hot_encode.py</code> <pre><code>def _fit(self, dataset: DataFrame) -&gt; \"OneHotEncodeTransformer\":\n    \"\"\"\n    Fits the OneHotEncodeEstimator estimator to the given dataset.\n    Returns a OneHotEncodeTransformer which can be used to one-hot columns using\n    the collected string labels.\n\n    It re-uses the StringIndexEstimator to collect the string labels.\n\n    :param dataset: Pyspark dataframe to fit the estimator to.\n    :returns: OneHotEncodeTransformer instance with collected string labels.\n    \"\"\"\n    column_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n    labels = collect_labels_array(\n        dataset=dataset,\n        column=F.col(self.getInputCol()),\n        column_datatype=column_datatype,\n        string_order_type=self.getStringOrderType(),\n        mask_token=self.getMaskToken(),\n        max_num_labels=self.getMaxNumLabels(),\n    )\n\n    self.setLabelsArray(labels)\n\n    return OneHotEncodeTransformer(\n        inputCol=self.getInputCol(),\n        outputCol=self.getOutputCol(),\n        layerName=self.getLayerName(),\n        inputDtype=self.getInputDtype(),\n        outputDtype=self.getOutputDtype(),\n        labelsArray=self.getLabelsArray(),\n        stringOrderType=self.getStringOrderType(),\n        maskToken=self.getMaskToken(),\n        numOOVIndices=self.getNumOOVIndices(),\n        dropUnseen=self.getDropUnseen(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/shared_one_hot_encode/","title":"shared_one_hot_encode","text":""},{"location":"reference/src/kamae/spark/estimators/shared_one_hot_encode/#src.kamae.spark.estimators.shared_one_hot_encode.SharedOneHotEncodeEstimator","title":"SharedOneHotEncodeEstimator","text":"<pre><code>SharedOneHotEncodeEstimator(\n    inputCols=None,\n    outputCols=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    stringOrderType=\"frequencyDesc\",\n    maskToken=None,\n    numOOVIndices=1,\n    dropUnseen=False,\n    maxNumLabels=None,\n)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>MultiInputMultiOutputParams</code>, <code>DropUnseenParams</code>, <code>StringIndexParams</code></p> <p>Shared One-hot encoder Spark Estimator for use in Spark pipelines. This estimator is used to collect all the string labels for a given set of input columns. When fit is called it returns a SharedOneHotEncodeTransformer which can be used to create one-hot arrays from additional feature columns using the same string labels.</p> <p>:param inputCols: List of input column names. :param outputCols: List of output column names. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer returned by the fit method. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column(s) to after transforming. :param: stringOrderType: How to order the string indices. Options are 'frequencyAsc', 'frequencyDesc', 'alphabeticalAsc', 'alphabeticalDesc'. Defaults to 'frequencyDesc'. :param maskToken: Token to use for masking. If set, the token will be indexed as 0. :param numOOVIndices: Number of out of vocabulary indices to use. The out of vocabulary indices are used to represent unseen labels and are placed at the beginning of the one-hot encoding. Defaults to 1. :param dropUnseen: Whether to drop unseen label indices. If set to True, the transformer will not add an extra dimension for unseen labels in the one-hot encoding. Defaults to False. :param maxNumLabels: Optional value to limit the size of the vocabulary. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/estimators/shared_one_hot_encode.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCols: Optional[List[str]] = None,\n    outputCols: Optional[List[str]] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    stringOrderType: str = \"frequencyDesc\",\n    maskToken: Optional[str] = None,\n    numOOVIndices: int = 1,\n    dropUnseen: bool = False,\n    maxNumLabels: Optional[int] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes the SharedOneHotEncoder estimator.\n    Sets all parameters to given inputs.\n\n    :param inputCols: List of input column names.\n    :param outputCols: List of output column names.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer returned\n    by the fit method.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column(s) to after\n    transforming.\n    :param: stringOrderType: How to order the string indices.\n    Options are 'frequencyAsc', 'frequencyDesc', 'alphabeticalAsc',\n    'alphabeticalDesc'. Defaults to 'frequencyDesc'.\n    :param maskToken: Token to use for masking.\n    If set, the token will be indexed as 0.\n    :param numOOVIndices: Number of out of vocabulary indices to use. The\n    out of vocabulary indices are used to represent unseen labels and are\n    placed at the beginning of the one-hot encoding. Defaults to 1.\n    :param dropUnseen: Whether to drop unseen label indices. If set to True,\n    the transformer will not add an extra dimension for unseen labels in the\n    one-hot encoding. Defaults to False.\n    :param maxNumLabels: Optional value to limit the size of the vocabulary.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(\n        stringOrderType=\"frequencyDesc\",\n        numOOVIndices=1,\n        dropUnseen=False,\n        maskToken=None,\n        maxNumLabels=None,\n    )\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/shared_one_hot_encode/#src.kamae.spark.estimators.shared_one_hot_encode.SharedOneHotEncodeEstimator.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/estimators/shared_one_hot_encode/#src.kamae.spark.estimators.shared_one_hot_encode.SharedOneHotEncodeEstimator._fit","title":"_fit","text":"<pre><code>_fit(dataset)\n</code></pre> <p>Fits the OneHotEncodeEstimator estimator to the given dataset. Returns a OneHotEncodeTransformer which can be used to one-hot columns using the collected string labels.</p> <p>:param dataset: Pyspark dataframe to fit the estimator to. :returns: OneHotEncodeTransformer instance with collected string labels.</p> Source code in <code>src/kamae/spark/estimators/shared_one_hot_encode.py</code> <pre><code>def _fit(self, dataset: DataFrame) -&gt; \"SharedOneHotEncodeTransformer\":\n    \"\"\"\n    Fits the OneHotEncodeEstimator estimator to the given dataset.\n    Returns a OneHotEncodeTransformer which can be used to one-hot columns using\n    the collected string labels.\n\n    :param dataset: Pyspark dataframe to fit the estimator to.\n    :returns: OneHotEncodeTransformer instance with collected string labels.\n    \"\"\"\n    column_datatypes = [\n        self.get_column_datatype(dataset=dataset, column_name=i)\n        for i in self.getInputCols()\n    ]\n    labels = collect_labels_array_from_multiple_columns(\n        dataset=dataset,\n        columns=[F.col(i) for i in self.getInputCols()],\n        column_datatypes=column_datatypes,\n        string_order_type=self.getStringOrderType(),\n        mask_token=self.getMaskToken(),\n        max_num_labels=self.getMaxNumLabels(),\n    )\n    self.setLabelsArray(labels)\n\n    return SharedOneHotEncodeTransformer(\n        inputCols=self.getInputCols(),\n        outputCols=self.getOutputCols(),\n        layerName=self.getLayerName(),\n        inputDtype=self.getInputDtype(),\n        outputDtype=self.getOutputDtype(),\n        labelsArray=self.getLabelsArray(),\n        stringOrderType=self.getStringOrderType(),\n        maskToken=self.getMaskToken(),\n        numOOVIndices=self.getNumOOVIndices(),\n        dropUnseen=self.getDropUnseen(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/shared_string_index/","title":"shared_string_index","text":""},{"location":"reference/src/kamae/spark/estimators/shared_string_index/#src.kamae.spark.estimators.shared_string_index.SharedStringIndexEstimator","title":"SharedStringIndexEstimator","text":"<pre><code>SharedStringIndexEstimator(\n    inputCols=None,\n    outputCols=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    stringOrderType=\"frequencyDesc\",\n    maskToken=None,\n    numOOVIndices=1,\n    maxNumLabels=None,\n)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>MultiInputMultiOutputParams</code>, <code>StringIndexParams</code></p> <p>Shared vocab String indexer Spark Estimator for use in Spark pipelines. This estimator is used to collect all the string labels across multiple columns and keeps a shared list of string labels. When fit is called it returns a SharedStringIndexerLayerModel which can be used to index additional feature columns using the same string labels.</p> <p>:param inputCols: Input column names. :param outputCols: Output column names. :param inputDtype: Input data type to cast input columns to before transforming. :param outputDtype: Output data type to cast the output columns to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param: stringOrderType: How to order the string indices. Options are 'frequencyAsc', 'frequencyDesc', 'alphabeticalAsc', 'alphabeticalDesc'. :param maskToken: Token to use for masking. If set, the token will be indexed as 0. :param numOOVIndices: Number of out of vocabulary indices to use. :param maxNumLabels: Optional value to limit the size of the vocabulary. Default is 1. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/estimators/shared_string_index.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCols: Optional[List[str]] = None,\n    outputCols: Optional[List[str]] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    stringOrderType: str = \"frequencyDesc\",\n    maskToken: Optional[str] = None,\n    numOOVIndices: int = 1,\n    maxNumLabels: Optional[int] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes the SharedStringIndexEstimator estimator.\n    Sets all parameters to given inputs.\n\n    :param inputCols: Input column names.\n    :param outputCols: Output column names.\n    :param inputDtype: Input data type to cast input columns to before\n    transforming.\n    :param outputDtype: Output data type to cast the output columns to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param: stringOrderType: How to order the string indices.\n    Options are 'frequencyAsc', 'frequencyDesc', 'alphabeticalAsc',\n    'alphabeticalDesc'.\n    :param maskToken: Token to use for masking.\n    If set, the token will be indexed as 0.\n    :param numOOVIndices: Number of out of vocabulary indices to use.\n    :param maxNumLabels: Optional value to limit the size of the vocabulary.\n    Default is 1.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(\n        stringOrderType=\"frequencyDesc\",\n        numOOVIndices=1,\n        maskToken=None,\n        maxNumLabels=None,\n    )\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/shared_string_index/#src.kamae.spark.estimators.shared_string_index.SharedStringIndexEstimator.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/estimators/shared_string_index/#src.kamae.spark.estimators.shared_string_index.SharedStringIndexEstimator._fit","title":"_fit","text":"<pre><code>_fit(dataset)\n</code></pre> <p>Fits the SharedStringIndexEstimator estimator to the given dataset. Returns a SharedStringIndexerLayerModel which can be used to index columns using the collected string labels.</p> <p>:param dataset: Pyspark dataframe to fit the estimator to. :returns: SharedStringIndexerLayerModel instance with collected string labels.</p> Source code in <code>src/kamae/spark/estimators/shared_string_index.py</code> <pre><code>def _fit(self, dataset: DataFrame) -&gt; \"SharedStringIndexTransformer\":\n    \"\"\"\n    Fits the SharedStringIndexEstimator estimator to the given dataset.\n    Returns a SharedStringIndexerLayerModel which can be used to index columns using\n    the collected string labels.\n\n    :param dataset: Pyspark dataframe to fit the estimator to.\n    :returns: SharedStringIndexerLayerModel instance with collected string labels.\n    \"\"\"\n\n    column_datatypes = [\n        self.get_column_datatype(dataset=dataset, column_name=i)\n        for i in self.getInputCols()\n    ]\n    labels = collect_labels_array_from_multiple_columns(\n        dataset=dataset,\n        columns=[F.col(i) for i in self.getInputCols()],\n        column_datatypes=column_datatypes,\n        string_order_type=self.getStringOrderType(),\n        mask_token=self.getMaskToken(),\n        max_num_labels=self.getMaxNumLabels(),\n    )\n    self.setLabelsArray(labels)\n\n    return SharedStringIndexTransformer(\n        inputCols=self.getInputCols(),\n        outputCols=self.getOutputCols(),\n        layerName=self.getLayerName(),\n        inputDtype=self.getInputDtype(),\n        outputDtype=self.getOutputDtype(),\n        labelsArray=self.getLabelsArray(),\n        stringOrderType=self.getStringOrderType(),\n        numOOVIndices=self.getNumOOVIndices(),\n        maskToken=self.getMaskToken(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/single_feature_array_standard_scale/","title":"single_feature_array_standard_scale","text":""},{"location":"reference/src/kamae/spark/estimators/single_feature_array_standard_scale/#src.kamae.spark.estimators.single_feature_array_standard_scale.SingleFeatureArrayStandardScaleEstimator","title":"SingleFeatureArrayStandardScaleEstimator","text":"<pre><code>SingleFeatureArrayStandardScaleEstimator(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    maskValue=None,\n)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>SingleInputSingleOutputParams</code>, <code>MaskValueParams</code></p> <p>Single feature array standard scaler estimator for use in Spark pipelines. This estimator is used to calculate the mean and standard deviation of the input feature column when it is an array where all the elements represent the same feature. An example would be a sequence of trip durations or booking windows in a traveller's session. When fit is called it returns a StandardScaleTransformer which can be used to standardize/transform additional features, where the mean and standard deviation are calculated across all elements in all the arrays.</p> <p>:param inputCol: Input column name to standardize. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer  in the keras model. If not set, we use the uid of the Spark transformer. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/estimators/single_feature_array_standard_scale.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    maskValue: Optional[float] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes a SingleFeatureArrayStandardScaleEstimator estimator.\n    Sets all parameters to given inputs.\n\n    :param inputCol: Input column name to standardize.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n     in the keras model. If not set, we use the uid of the Spark transformer.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(maskValue=None)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/single_feature_array_standard_scale/#src.kamae.spark.estimators.single_feature_array_standard_scale.SingleFeatureArrayStandardScaleEstimator.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/estimators/single_feature_array_standard_scale/#src.kamae.spark.estimators.single_feature_array_standard_scale.SingleFeatureArrayStandardScaleEstimator._fit","title":"_fit","text":"<pre><code>_fit(dataset)\n</code></pre> <p>Fits the SingleFeatureArrayStandardScaleEstimator estimator to the given dataset. Calculates the mean and standard deviation of the input feature column and returns a StandardScaleTransformer with the mean and standard deviation set.</p> <p>All rows are assumed to be of the same length. The mask value which is set in the estimator is used to ignore certain values in the process of calculating the mean and stddev. :param dataset: Pyspark dataframe to fit the estimator to. :returns: StandardScaleTransformer instance with mean &amp; standard deviation set.</p> Source code in <code>src/kamae/spark/estimators/single_feature_array_standard_scale.py</code> <pre><code>def _fit(self, dataset: DataFrame) -&gt; \"StandardScaleTransformer\":\n    \"\"\"\n    Fits the SingleFeatureArrayStandardScaleEstimator estimator to the given\n    dataset. Calculates the mean and standard deviation of the input feature column\n    and returns a StandardScaleTransformer with the mean and standard deviation set.\n\n    All rows are assumed to be of the same length. The mask value which is set in\n    the estimator is used to ignore certain values in the process of calculating\n    the mean and stddev.\n    :param dataset: Pyspark dataframe to fit the estimator to.\n    :returns: StandardScaleTransformer instance with mean &amp; standard deviation set.\n    \"\"\"\n\n    input_column_type = self.get_column_datatype(dataset, self.getInputCol())\n    if not isinstance(input_column_type, ArrayType):\n        raise ValueError(\n            f\"\"\"Input column {self.getInputCol()} must be of ArrayType.\n                    Got {input_column_type} instead.\"\"\"\n        )\n\n    # Collect a single row to driver and get the length.\n    # We assume all subsequent rows have the same length.\n    array_size = np.array((dataset.select(self.getInputCol()).first()[0])).shape[-1]\n\n    # Flatten the array to a single array.\n    # Will do nothing if the array is not nested.\n    flattened_array_col = flatten_nested_arrays(\n        column=F.col(self.getInputCol()), column_data_type=input_column_type\n    )\n\n    mean_and_stddev_dict: Dict[str, float] = (\n        dataset.select(F.explode(flattened_array_col).alias(self.getInputCol()))\n        .withColumn(\n            \"mask\",\n            F.when(\n                F.col(self.getInputCol()) == F.lit(self.getMaskValue()), 1\n            ).otherwise(0),\n        )\n        .filter(F.col(\"mask\") == F.lit(0))\n        .agg(\n            F.mean(self.getInputCol()).alias(\"mean\"),\n            F.stddev_pop(self.getInputCol()).alias(\"stddev\"),\n        )\n        .first()\n        .asDict()\n    )\n    mean: List[float] = [mean_and_stddev_dict[\"mean\"] for _ in range(array_size)]\n    stddev: List[float] = [\n        mean_and_stddev_dict[\"stddev\"] for _ in range(array_size)\n    ]\n\n    return StandardScaleTransformer(\n        inputCol=self.getInputCol(),\n        outputCol=self.getOutputCol(),\n        layerName=self.getLayerName(),\n        inputDtype=self.getInputDtype(),\n        outputDtype=self.getOutputDtype(),\n        mean=mean,\n        stddev=stddev,\n        maskValue=self.getMaskValue(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/standard_scale/","title":"standard_scale","text":""},{"location":"reference/src/kamae/spark/estimators/standard_scale/#src.kamae.spark.estimators.standard_scale.StandardScaleEstimator","title":"StandardScaleEstimator","text":"<pre><code>StandardScaleEstimator(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    maskValue=None,\n)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>SingleInputSingleOutputParams</code>, <code>MaskValueParams</code></p> <p>Standard scaler estimator for use in Spark pipelines. This estimator is used to calculate the mean and standard deviation of the input feature column. When fit is called it returns a StandardScaleTransformer which can be used to standardize/transform additional features.</p> <p>WARNING: If the input is an array, we assume that the array has a constant shape across all rows.</p> <p>:param inputCol: Input column name to standardize. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer  in the keras model. If not set, we use the uid of the Spark transformer. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/estimators/standard_scale.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    maskValue: Optional[float] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes a StandardScaleEstimator estimator.\n    Sets all parameters to given inputs.\n\n    :param inputCol: Input column name to standardize.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n     in the keras model. If not set, we use the uid of the Spark transformer.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(maskValue=None)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/standard_scale/#src.kamae.spark.estimators.standard_scale.StandardScaleEstimator.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/estimators/standard_scale/#src.kamae.spark.estimators.standard_scale.StandardScaleEstimator._fit","title":"_fit","text":"<pre><code>_fit(dataset)\n</code></pre> <p>Fits the StandardScaleEstimator estimator to the given dataset. Calculates the mean and standard deviation of the input feature column and returns a StandardScaleTransformer with the mean and standard deviation set.</p> <p>:param dataset: Pyspark dataframe to fit the estimator to. :returns: StandardScaleTransformer instance with mean &amp; standard deviation set.</p> Source code in <code>src/kamae/spark/estimators/standard_scale.py</code> <pre><code>def _fit(self, dataset: DataFrame) -&gt; \"StandardScaleTransformer\":\n    \"\"\"\n    Fits the StandardScaleEstimator estimator to the given dataset.\n    Calculates the mean and standard deviation of the input feature column and\n    returns a StandardScaleTransformer with the mean and standard deviation set.\n\n    :param dataset: Pyspark dataframe to fit the estimator to.\n    :returns: StandardScaleTransformer instance with mean &amp; standard deviation set.\n    \"\"\"\n    input_column_type = self.get_column_datatype(dataset, self.getInputCol())\n    if not isinstance(input_column_type, ArrayType):\n        input_col = F.array(F.col(self.getInputCol()))\n        input_column_type = ArrayType(input_column_type)\n    else:\n        input_col = F.col(self.getInputCol())\n\n    # Collect a single row to driver and get the length.\n    # We assume all subsequent rows have the same length.\n    array_size = np.array((dataset.select(input_col).first()[0])).shape[-1]\n\n    element_struct = construct_nested_elements_for_scaling(\n        column=input_col,\n        column_datatype=input_column_type,\n        array_dim=array_size,\n    )\n\n    mean_cols = [\n        F.mean(\n            F.when(\n                F.col(f\"element_struct.element_{i}\") == F.lit(self.getMaskValue()),\n                F.lit(None),\n            ).otherwise(F.col(f\"element_struct.element_{i}\"))\n        ).alias(f\"mean_{i}\")\n        for i in range(1, array_size + 1)\n    ]\n\n    stddev_cols = [\n        F.stddev_pop(\n            F.when(\n                F.col(f\"element_struct.element_{i}\") == F.lit(self.getMaskValue()),\n                F.lit(None),\n            ).otherwise(F.col(f\"element_struct.element_{i}\"))\n        ).alias(f\"stddev_{i}\")\n        for i in range(1, array_size + 1)\n    ]\n\n    metric_cols = mean_cols + stddev_cols\n\n    mean_and_stddev_dict = (\n        dataset.select(element_struct).agg(*metric_cols).first().asDict()\n    )\n    mean = [mean_and_stddev_dict[f\"mean_{i}\"] for i in range(1, array_size + 1)]\n    stddev = [mean_and_stddev_dict[f\"stddev_{i}\"] for i in range(1, array_size + 1)]\n\n    return StandardScaleTransformer(\n        inputCol=self.getInputCol(),\n        outputCol=self.getOutputCol(),\n        layerName=self.getLayerName(),\n        inputDtype=self.getInputDtype(),\n        outputDtype=self.getOutputDtype(),\n        mean=mean,\n        stddev=stddev,\n        maskValue=self.getMaskValue(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/string_index/","title":"string_index","text":""},{"location":"reference/src/kamae/spark/estimators/string_index/#src.kamae.spark.estimators.string_index.StringIndexEstimator","title":"StringIndexEstimator","text":"<pre><code>StringIndexEstimator(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    stringOrderType=None,\n    maskToken=None,\n    numOOVIndices=1,\n    maxNumLabels=None,\n)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>SingleInputSingleOutputParams</code>, <code>StringIndexParams</code></p> <p>String indexer Spark Estimator for use in Spark pipelines. This estimator is used to collect all the string labels for a given column. When fit is called it returns a StringIndexerLayerModel which can be used to index additional feature columns using the same string labels.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param: stringOrderType: How to order the string indices. Options are 'frequencyAsc', 'frequencyDesc', 'alphabeticalAsc', 'alphabeticalDesc'. :param maskToken: Token to use for masking. If set, the token will be indexed as 0. :param numOOVIndices: Number of out of vocabulary indices to use. :param maxNumLabels: Optional value to limit the size of the vocabulary. Default is 1. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/estimators/string_index.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    stringOrderType: Optional[str] = None,\n    maskToken: Optional[str] = None,\n    numOOVIndices: int = 1,\n    maxNumLabels: Optional[int] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes the StringIndexEstimator estimator.\n    Sets all parameters to given inputs.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param: stringOrderType: How to order the string indices.\n    Options are 'frequencyAsc', 'frequencyDesc', 'alphabeticalAsc',\n    'alphabeticalDesc'.\n    :param maskToken: Token to use for masking.\n    If set, the token will be indexed as 0.\n    :param numOOVIndices: Number of out of vocabulary indices to use.\n    :param maxNumLabels: Optional value to limit the size of the vocabulary.\n    Default is 1.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(\n        stringOrderType=\"frequencyDesc\",\n        numOOVIndices=1,\n        maskToken=None,\n        maxNumLabels=None,\n    )\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/estimators/string_index/#src.kamae.spark.estimators.string_index.StringIndexEstimator.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/estimators/string_index/#src.kamae.spark.estimators.string_index.StringIndexEstimator._fit","title":"_fit","text":"<pre><code>_fit(dataset)\n</code></pre> <p>Fits the StringIndexEstimator estimator to the given dataset. Returns a StringIndexerLayerModel which can be used to index columns using the collected string labels.</p> <p>:param dataset: Pyspark dataframe to fit the estimator to. :returns: StringIndexerLayerModel instance with collected string labels.</p> Source code in <code>src/kamae/spark/estimators/string_index.py</code> <pre><code>def _fit(self, dataset: DataFrame) -&gt; \"StringIndexTransformer\":\n    \"\"\"\n    Fits the StringIndexEstimator estimator to the given dataset.\n    Returns a StringIndexerLayerModel which can be used to index columns using\n    the collected string labels.\n\n    :param dataset: Pyspark dataframe to fit the estimator to.\n    :returns: StringIndexerLayerModel instance with collected string labels.\n    \"\"\"\n    column_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n    labels = collect_labels_array(\n        dataset=dataset,\n        column=F.col(self.getInputCol()),\n        column_datatype=column_datatype,\n        string_order_type=self.getStringOrderType(),\n        mask_token=self.getMaskToken(),\n        max_num_labels=self.getMaxNumLabels(),\n    )\n    self.setLabelsArray(labels)\n\n    return StringIndexTransformer(\n        inputCol=self.getInputCol(),\n        outputCol=self.getOutputCol(),\n        layerName=self.getLayerName(),\n        inputDtype=self.getInputDtype(),\n        outputDtype=self.getOutputDtype(),\n        labelsArray=self.getLabelsArray(),\n        stringOrderType=self.getStringOrderType(),\n        numOOVIndices=self.getNumOOVIndices(),\n        maskToken=self.getMaskToken(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/params/","title":"params","text":""},{"location":"reference/src/kamae/spark/params/base/","title":"base","text":""},{"location":"reference/src/kamae/spark/params/base/#src.kamae.spark.params.base.HasInputDtype","title":"HasInputDtype","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class for a transformer input datatype.</p>"},{"location":"reference/src/kamae/spark/params/base/#src.kamae.spark.params.base.HasInputDtype.getInputDtype","title":"getInputDtype","text":"<pre><code>getInputDtype()\n</code></pre> <p>Gets the value of the inputDtype parameter. :returns: Input datatype.</p> Source code in <code>src/kamae/spark/params/base.py</code> <pre><code>def getInputDtype(self) -&gt; str:\n    \"\"\"\n    Gets the value of the inputDtype parameter.\n    :returns: Input datatype.\n    \"\"\"\n    return self.getOrDefault(self.inputDtype)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/base/#src.kamae.spark.params.base.HasInputDtype.getInputTFDtype","title":"getInputTFDtype","text":"<pre><code>getInputTFDtype()\n</code></pre> <p>Gets the tensorflow datatype string from the inputDtype parameter. Uses the DType enum within Kamae to map the inputDtype to the tensorflow datatype string. :returns: String of the tensorflow datatype.</p> Source code in <code>src/kamae/spark/params/base.py</code> <pre><code>def getInputTFDtype(self) -&gt; Optional[str]:\n    \"\"\"\n    Gets the tensorflow datatype string from the inputDtype parameter.\n    Uses the DType enum within Kamae to map the inputDtype to the tensorflow\n    datatype string.\n    :returns: String of the tensorflow datatype.\n    \"\"\"\n    input_dtype = self.getInputDtype()\n    if input_dtype is None:\n        return None\n    dtypes_map = {dtype.dtype_name: dtype.tf_dtype.name for dtype in DType}\n    return dtypes_map[input_dtype]\n</code></pre>"},{"location":"reference/src/kamae/spark/params/base/#src.kamae.spark.params.base.HasInputDtype.setInputDtype","title":"setInputDtype","text":"<pre><code>setInputDtype(value)\n</code></pre> <p>Sets the parameter inputDtype to the given string value.</p> <p>:param value: String to set the inputDtype parameter to. :raises ValueError: If the input dtype is not supported. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/base.py</code> <pre><code>def setInputDtype(self, value: str) -&gt; \"HasInputDtype\":\n    \"\"\"\n    Sets the parameter inputDtype to the given string value.\n\n    :param value: String to set the inputDtype parameter to.\n    :raises ValueError: If the input dtype is not supported.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    dtype_enums = [dtype.dtype_name for dtype in DType]\n    if value not in dtype_enums:\n        raise ValueError(\n            f\"\"\"Input dtype {value} not supported.\n            Supported dtypes are: {dtype_enums}\"\"\"\n        )\n    return self._set(inputDtype=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/base/#src.kamae.spark.params.base.HasOutputDtype","title":"HasOutputDtype","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class for a transformer output datatype.</p>"},{"location":"reference/src/kamae/spark/params/base/#src.kamae.spark.params.base.HasOutputDtype.getOutputDtype","title":"getOutputDtype","text":"<pre><code>getOutputDtype()\n</code></pre> <p>Gets the value of the outputDtype parameter. :returns: Output datatype.</p> Source code in <code>src/kamae/spark/params/base.py</code> <pre><code>def getOutputDtype(self) -&gt; str:\n    \"\"\"\n    Gets the value of the outputDtype parameter.\n    :returns: Output datatype.\n    \"\"\"\n    return self.getOrDefault(self.outputDtype)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/base/#src.kamae.spark.params.base.HasOutputDtype.getOutputTFDtype","title":"getOutputTFDtype","text":"<pre><code>getOutputTFDtype()\n</code></pre> <p>Gets the tensorflow datatype string from the outputDtype parameter. Uses the DType enum within Kamae to map the outputDtype to the tensorflow datatype string. :returns: String of the tensorflow datatype.</p> Source code in <code>src/kamae/spark/params/base.py</code> <pre><code>def getOutputTFDtype(self) -&gt; Optional[str]:\n    \"\"\"\n    Gets the tensorflow datatype string from the outputDtype parameter.\n    Uses the DType enum within Kamae to map the outputDtype to the tensorflow\n    datatype string.\n    :returns: String of the tensorflow datatype.\n    \"\"\"\n\n    output_dtype = self.getOutputDtype()\n    if output_dtype is None:\n        return None\n    dtypes_map = {dtype.dtype_name: dtype.tf_dtype.name for dtype in DType}\n    return dtypes_map[output_dtype]\n</code></pre>"},{"location":"reference/src/kamae/spark/params/base/#src.kamae.spark.params.base.HasOutputDtype.setOutputDtype","title":"setOutputDtype","text":"<pre><code>setOutputDtype(value)\n</code></pre> <p>Sets the parameter outputDtype to the given string value.</p> <p>:param value: String to set the outputDtype parameter to. :raises ValueError: If the output dtype is not supported. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/base.py</code> <pre><code>def setOutputDtype(self, value: str) -&gt; \"HasOutputDtype\":\n    \"\"\"\n    Sets the parameter outputDtype to the given string value.\n\n    :param value: String to set the outputDtype parameter to.\n    :raises ValueError: If the output dtype is not supported.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    dtype_enums = [dtype.dtype_name for dtype in DType]\n    if value not in dtype_enums:\n        raise ValueError(\n            f\"\"\"Output dtype {value} not supported.\n            Supported dtypes are: {dtype_enums}\"\"\"\n        )\n    return self._set(outputDtype=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/base/#src.kamae.spark.params.base.MultiInputMultiOutputParams","title":"MultiInputMultiOutputParams","text":"<p>               Bases: <code>MultiInputParams</code>, <code>MultiOutputParams</code>, <code>InputOutputExtractor</code>, <code>KamaeDefaultParamsReadable</code>, <code>KamaeDefaultParamsWritable</code></p> <p>Mixin class containing set methods for the multiple input and multiple output columns scenario.</p>"},{"location":"reference/src/kamae/spark/params/base/#src.kamae.spark.params.base.MultiInputParams","title":"MultiInputParams","text":"<p>               Bases: <code>HasInputCols</code></p> <p>Mixin class containing set methods for the multiple input columns scenario.</p>"},{"location":"reference/src/kamae/spark/params/base/#src.kamae.spark.params.base.MultiInputParams.setInputCols","title":"setInputCols","text":"<pre><code>setInputCols(value)\n</code></pre> <p>Sets the parameter inputCols to the given list of strings.</p> <p>:param value: List of strings to set the inputCols parameter to. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/base.py</code> <pre><code>def setInputCols(self, value: List[str]) -&gt; \"MultiInputParams\":\n    \"\"\"\n    Sets the parameter inputCols to the given list of strings.\n\n    :param value: List of strings to set the inputCols parameter to.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(inputCols=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/base/#src.kamae.spark.params.base.MultiInputSingleOutputParams","title":"MultiInputSingleOutputParams","text":"<p>               Bases: <code>MultiInputParams</code>, <code>SingleOutputParams</code>, <code>InputOutputExtractor</code>, <code>KamaeDefaultParamsReadable</code>, <code>KamaeDefaultParamsWritable</code></p> <p>Mixin class containing set methods for the multiple input and single output column scenario.</p>"},{"location":"reference/src/kamae/spark/params/base/#src.kamae.spark.params.base.MultiOutputParams","title":"MultiOutputParams","text":"<p>               Bases: <code>HasLayerName</code>, <code>HasOutputCols</code>, <code>HasOutputDtype</code></p> <p>Mixin class containing set methods for the multiple output columns scenario.</p>"},{"location":"reference/src/kamae/spark/params/base/#src.kamae.spark.params.base.MultiOutputParams.setLayerName","title":"setLayerName","text":"<pre><code>setLayerName(value)\n</code></pre> <p>Sets the parameter layerName to the given string value. Throws an error if the value is the same as one of the output column names, as this causes issues when constructing the pipeline graph.</p> <p>:param value: String to set the layerName parameter to. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/base.py</code> <pre><code>def setLayerName(self, value: str) -&gt; \"MultiOutputParams\":\n    \"\"\"\n    Sets the parameter layerName to the given string value.\n    Throws an error if the value is the same as one of the output column names,\n    as this causes issues when constructing the pipeline graph.\n\n    :param value: String to set the layerName parameter to.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if self.hasParam(\"outputCol\") and self.isDefined(\"outputCols\"):\n        if value in self.getOutputCols():\n            raise ValueError(\n                \"Layer name and output column names must be different.\"\n            )\n    return self._set(layerName=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/base/#src.kamae.spark.params.base.MultiOutputParams.setOutputCols","title":"setOutputCols","text":"<pre><code>setOutputCols(value)\n</code></pre> <p>Sets the parameter outputCols to the given list of strings. Throws an error if one of the output column names is the same as the layer name, as this causes issues when constructing the pipeline graph.</p> <p>:param value: List of strings to set the outputCols parameter to. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/base.py</code> <pre><code>def setOutputCols(self, value: List[str]) -&gt; \"MultiOutputParams\":\n    \"\"\"\n    Sets the parameter outputCols to the given list of strings.\n    Throws an error if one of the output column names is the same as the layer name,\n    as this causes issues when constructing the pipeline graph.\n\n    :param value: List of strings to set the outputCols parameter to.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if self.hasParam(\"layerName\") and self.isDefined(\"layerName\"):\n        if self.getLayerName() in value:\n            raise ValueError(\n                \"Layer name and output column names must be different.\"\n            )\n    return self._set(outputCols=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/base/#src.kamae.spark.params.base.SingleInputMultiOutputParams","title":"SingleInputMultiOutputParams","text":"<p>               Bases: <code>SingleInputParams</code>, <code>MultiOutputParams</code>, <code>InputOutputExtractor</code>, <code>KamaeDefaultParamsReadable</code>, <code>KamaeDefaultParamsWritable</code></p> <p>Mixin class containing set methods for the single input and multiple output columns scenario.</p>"},{"location":"reference/src/kamae/spark/params/base/#src.kamae.spark.params.base.SingleInputParams","title":"SingleInputParams","text":"<p>               Bases: <code>HasInputCol</code></p> <p>Mixin class containing set methods for the single input column scenario.</p>"},{"location":"reference/src/kamae/spark/params/base/#src.kamae.spark.params.base.SingleInputParams.setInputCol","title":"setInputCol","text":"<pre><code>setInputCol(value)\n</code></pre> <p>Sets the parameter inputCol to the given string value.</p> <p>:param value: String to set the inputCol parameter to. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/base.py</code> <pre><code>def setInputCol(self, value: str) -&gt; \"SingleInputParams\":\n    \"\"\"\n    Sets the parameter inputCol to the given string value.\n\n    :param value: String to set the inputCol parameter to.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(inputCol=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/base/#src.kamae.spark.params.base.SingleInputSingleOutputParams","title":"SingleInputSingleOutputParams","text":"<p>               Bases: <code>SingleInputParams</code>, <code>SingleOutputParams</code>, <code>InputOutputExtractor</code>, <code>KamaeDefaultParamsReadable</code>, <code>KamaeDefaultParamsWritable</code></p> <p>Mixin class containing set methods for the single input and single output column scenario.</p>"},{"location":"reference/src/kamae/spark/params/base/#src.kamae.spark.params.base.SingleOutputParams","title":"SingleOutputParams","text":"<p>               Bases: <code>HasLayerName</code>, <code>HasOutputCol</code>, <code>HasOutputDtype</code></p> <p>Mixin class containing set methods for the single output column scenario.</p>"},{"location":"reference/src/kamae/spark/params/base/#src.kamae.spark.params.base.SingleOutputParams.setLayerName","title":"setLayerName","text":"<pre><code>setLayerName(value)\n</code></pre> <p>Sets the parameter layerName to the given string value. Throws an error if the value is the same as the output column name, as this causes issues when constructing the pipeline graph.</p> <p>:param value: String to set the layerName parameter to. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/base.py</code> <pre><code>def setLayerName(self, value: str) -&gt; \"SingleOutputParams\":\n    \"\"\"\n    Sets the parameter layerName to the given string value.\n    Throws an error if the value is the same as the output column name,\n    as this causes issues when constructing the pipeline graph.\n\n    :param value: String to set the layerName parameter to.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if self.hasParam(\"outputCol\") and self.isDefined(\"outputCol\"):\n        if value == self.getOutputCol():\n            raise ValueError(\"Layer name and output column name must be different.\")\n    return self._set(layerName=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/base/#src.kamae.spark.params.base.SingleOutputParams.setOutputCol","title":"setOutputCol","text":"<pre><code>setOutputCol(value)\n</code></pre> <p>Sets the parameter outputCol to the given string value. Throws an error if the value is the same as the layer name, as this causes issues when constructing the pipeline graph.</p> <p>:param value: String to set the outputCol parameter to. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/base.py</code> <pre><code>def setOutputCol(self, value: str) -&gt; \"SingleOutputParams\":\n    \"\"\"\n    Sets the parameter outputCol to the given string value.\n    Throws an error if the value is the same as the layer name,\n    as this causes issues when constructing the pipeline graph.\n\n    :param value: String to set the outputCol parameter to.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if self.hasParam(\"layerName\") and self.isDefined(\"layerName\"):\n        if value == self.getLayerName():\n            raise ValueError(\"Layer name and output column name must be different.\")\n    return self._set(outputCol=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/default_read_write/","title":"default_read_write","text":"<p>Literally a copy of pyspark.ml.util DefaultParamsWritable/Readable/Writer/Reader. This is needed because Databricks runtimes &gt; 13.3LTS have slow metadata writes because they have introduced this v4.0.0 change: https://github.com/apache/spark/pull/47453 They have moved to using atomic writes for metadata, however this is intolerably slow for large pipelines. We temporarily fix this back to the open source code. This may need revisiting in the future.</p>"},{"location":"reference/src/kamae/spark/params/default_read_write/#src.kamae.spark.params.default_read_write.KamaeDefaultParamsReadable","title":"KamaeDefaultParamsReadable","text":"<p>               Bases: <code>DefaultParamsReadable</code></p> <p>DefaultParamsReadable with a workaround for slow metadata writes in Databricks. Replicates the functionality of DefaultParamsReadable in PySpark 3.5.0 since Databricks uses different functionality</p>"},{"location":"reference/src/kamae/spark/params/default_read_write/#src.kamae.spark.params.default_read_write.KamaeDefaultParamsReadable.read","title":"read  <code>classmethod</code>","text":"<pre><code>read()\n</code></pre> <p>Returns a KamaeDefaultParamsReader instance for this class.</p> Source code in <code>src/kamae/spark/params/default_read_write.py</code> <pre><code>@classmethod\ndef read(cls) -&gt; \"KamaeDefaultParamsReader\":\n    \"\"\"Returns a KamaeDefaultParamsReader instance for this class.\"\"\"\n    return KamaeDefaultParamsReader(cls)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/default_read_write/#src.kamae.spark.params.default_read_write.KamaeDefaultParamsReader","title":"KamaeDefaultParamsReader","text":"<p>               Bases: <code>DefaultParamsReader</code></p> <p>DefaultParamsReadable with a workaround for slow metadata writes in Databricks. Replicates the functionality of DefaultParamsReadable in PySpark 3.5.0 since Databricks uses different functionality</p>"},{"location":"reference/src/kamae/spark/params/default_read_write/#src.kamae.spark.params.default_read_write.KamaeDefaultParamsReader.loadMetadata","title":"loadMetadata  <code>staticmethod</code>","text":"<pre><code>loadMetadata(path, sc, expectedClassName='')\n</code></pre> <p>Load metadata saved using meth:<code>DefaultParamsWriter.saveMetadata</code> Parameters</p> <p>path : str sc : class:<code>pyspark.SparkContext</code> expectedClassName : str, optional     If non empty, this is checked against the loaded metadata.</p> Source code in <code>src/kamae/spark/params/default_read_write.py</code> <pre><code>@staticmethod\ndef loadMetadata(\n    path: str, sc: SparkContext, expectedClassName: str = \"\"\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Load metadata saved using :py:meth:`DefaultParamsWriter.saveMetadata`\n    Parameters\n    ----------\n    path : str\n    sc : :py:class:`pyspark.SparkContext`\n    expectedClassName : str, optional\n        If non empty, this is checked against the loaded metadata.\n    \"\"\"\n    metadataPath = os.path.join(path, \"metadata\")\n    # This is the line we need to maintain.\n    metadataStr = sc.textFile(metadataPath, 1).first()\n    loadedVals = DefaultParamsReader._parseMetaData(metadataStr, expectedClassName)\n    return loadedVals\n</code></pre>"},{"location":"reference/src/kamae/spark/params/default_read_write/#src.kamae.spark.params.default_read_write.KamaeDefaultParamsWritable","title":"KamaeDefaultParamsWritable","text":"<p>               Bases: <code>DefaultParamsWritable</code></p> <p>DefaultParamsWritable with a workaround for slow metadata writes in Databricks. Replicates the functionality of DefaultParamsWritable in PySpark 3.5.0 since Databricks uses different functionality</p>"},{"location":"reference/src/kamae/spark/params/default_read_write/#src.kamae.spark.params.default_read_write.KamaeDefaultParamsWritable.write","title":"write","text":"<pre><code>write()\n</code></pre> <p>Returns a DefaultParamsWriter instance for this class.</p> Source code in <code>src/kamae/spark/params/default_read_write.py</code> <pre><code>def write(self) -&gt; MLWriter:\n    \"\"\"Returns a DefaultParamsWriter instance for this class.\"\"\"\n    from pyspark.ml.param import Params\n\n    if isinstance(self, Params):\n        return KamaeDefaultParamsWriter(self)\n    else:\n        raise TypeError(\n            \"\"\"Cannot use KamaeDefaultParamsWritable with type %s because it does\n            not extend Params.\"\"\",\n            type(self),\n        )\n</code></pre>"},{"location":"reference/src/kamae/spark/params/default_read_write/#src.kamae.spark.params.default_read_write.KamaeDefaultParamsWriter","title":"KamaeDefaultParamsWriter","text":"<p>               Bases: <code>DefaultParamsWriter</code></p> <p>DefaultParamsWriter with a workaround for slow metadata writes in Databricks. Replicates the functionality of DefaultParamsWriter in PySpark 3.5.0 since Databricks uses different functionality</p>"},{"location":"reference/src/kamae/spark/params/default_read_write/#src.kamae.spark.params.default_read_write.KamaeDefaultParamsWriter.saveMetadata","title":"saveMetadata  <code>staticmethod</code>","text":"<pre><code>saveMetadata(\n    instance, path, sc, extraMetadata=None, paramMap=None\n)\n</code></pre> <p>Saves metadata + Params to: path + \"/metadata\" - class - timestamp - sparkVersion - uid - paramMap - defaultParamMap (since 2.4.0) - (optionally, extra metadata) Parameters</p> <p>extraMetadata : dict, optional     Extra metadata to be saved at same level as uid, paramMap, etc. paramMap : dict, optional     If given, this is saved in the \"paramMap\" field.</p> Source code in <code>src/kamae/spark/params/default_read_write.py</code> <pre><code>@staticmethod\ndef saveMetadata(\n    instance: \"Params\",\n    path: str,\n    sc: SparkContext,\n    extraMetadata: Optional[Dict[str, Any]] = None,\n    paramMap: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"\n    Saves metadata + Params to: path + \"/metadata\"\n    - class\n    - timestamp\n    - sparkVersion\n    - uid\n    - paramMap\n    - defaultParamMap (since 2.4.0)\n    - (optionally, extra metadata)\n    Parameters\n    ----------\n    extraMetadata : dict, optional\n        Extra metadata to be saved at same level as uid, paramMap, etc.\n    paramMap : dict, optional\n        If given, this is saved in the \"paramMap\" field.\n    \"\"\"\n    metadataPath = os.path.join(path, \"metadata\")\n    metadataJson = DefaultParamsWriter._get_metadata_to_save(\n        instance, sc, extraMetadata, paramMap\n    )\n    # This is the line we need to maintain.\n    sc.parallelize([metadataJson], 1).saveAsTextFile(metadataPath)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/name/","title":"name","text":""},{"location":"reference/src/kamae/spark/params/name/#src.kamae.spark.params.name.HasLayerName","title":"HasLayerName","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class for a layer name.</p>"},{"location":"reference/src/kamae/spark/params/name/#src.kamae.spark.params.name.HasLayerName.getLayerName","title":"getLayerName","text":"<pre><code>getLayerName()\n</code></pre> <p>Gets the value of the layerName parameter.</p> <p>:returns: Layer name.</p> Source code in <code>src/kamae/spark/params/name.py</code> <pre><code>def getLayerName(self) -&gt; str:\n    \"\"\"\n    Gets the value of the layerName parameter.\n\n    :returns: Layer name.\n    \"\"\"\n    return self.getOrDefault(self.layerName)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/","title":"shared","text":""},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.AutoBroadcastParams","title":"AutoBroadcastParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class for the auto broadcast parameter.</p>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.AutoBroadcastParams.getAutoBroadcast","title":"getAutoBroadcast","text":"<pre><code>getAutoBroadcast()\n</code></pre> <p>Gets the autoBroadcast parameter. :returns: autoBroadcast.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getAutoBroadcast(self) -&gt; bool:\n    \"\"\"\n    Gets the autoBroadcast parameter.\n    :returns: autoBroadcast.\n    \"\"\"\n    return self.getOrDefault(self.autoBroadcast)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.AutoBroadcastParams.setAutoBroadcast","title":"setAutoBroadcast","text":"<pre><code>setAutoBroadcast(value)\n</code></pre> <p>Sets the autoBroadcast parameter. :param value: autoBroadcast. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setAutoBroadcast(self, value: bool) -&gt; \"AutoBroadcastParams\":\n    \"\"\"\n    Sets the autoBroadcast parameter.\n    :param value: autoBroadcast.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(autoBroadcast=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.ConstantStringArrayParams","title":"ConstantStringArrayParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing separator parameter needed for constant string array transforms.</p>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.ConstantStringArrayParams.getConstantStringArray","title":"getConstantStringArray","text":"<pre><code>getConstantStringArray()\n</code></pre> <p>Gets the constantStringArray parameter.</p> <p>:returns: List of strings to use as a constant string array.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getConstantStringArray(self) -&gt; List[str]:\n    \"\"\"\n    Gets the constantStringArray parameter.\n\n    :returns: List of strings to use as a constant string array.\n    \"\"\"\n    return self.getOrDefault(self.constantStringArray)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.ConstantStringArrayParams.setConstantStringArray","title":"setConstantStringArray","text":"<pre><code>setConstantStringArray(value)\n</code></pre> <p>Sets the constantStringArray parameter.</p> <p>:param value: List of strings to use as a constant string array. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setConstantStringArray(self, value: List[str]) -&gt; \"ConstantStringArrayParams\":\n    \"\"\"\n    Sets the constantStringArray parameter.\n\n    :param value: List of strings to use as a constant string array.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(constantStringArray=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.DateTimeParams","title":"DateTimeParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class for a datetime transformation</p>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.DateTimeParams.getIncludeTime","title":"getIncludeTime","text":"<pre><code>getIncludeTime()\n</code></pre> <p>Gets the includeTime parameter. :returns: includeTime.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getIncludeTime(self) -&gt; bool:\n    \"\"\"\n    Gets the includeTime parameter.\n    :returns: includeTime.\n    \"\"\"\n    return self.getOrDefault(self.includeTime)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.DateTimeParams.setIncludeTime","title":"setIncludeTime","text":"<pre><code>setIncludeTime(value)\n</code></pre> <p>Sets the includeTime parameter. :param value: includeTime. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setIncludeTime(self, value: bool) -&gt; \"DateTimeParams\":\n    \"\"\"\n    Sets the includeTime parameter.\n    :param value: includeTime.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(includeTime=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.DefaultIntValueParams","title":"DefaultIntValueParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing default integer parameter.</p>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.DefaultIntValueParams.getDefaultValue","title":"getDefaultValue","text":"<pre><code>getDefaultValue()\n</code></pre> <p>Gets the defaultValue parameter. :returns: defaultValue param value.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getDefaultValue(self) -&gt; int:\n    \"\"\"\n    Gets the defaultValue parameter.\n    :returns: defaultValue param value.\n    \"\"\"\n    return self.getOrDefault(self.defaultValue)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.DefaultIntValueParams.setDefaultValue","title":"setDefaultValue","text":"<pre><code>setDefaultValue(value)\n</code></pre> <p>Sets the defaultValue parameter. :param value: Value to set the defaultValue parameter to. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setDefaultValue(self, value: int) -&gt; \"DefaultIntValueParams\":\n    \"\"\"\n    Sets the defaultValue parameter.\n    :param value: Value to set the defaultValue parameter to.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(defaultValue=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.DropUnseenParams","title":"DropUnseenParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing parameters needed to drop unseen index in the one hot encoder layer.</p>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.DropUnseenParams.getDropUnseen","title":"getDropUnseen","text":"<pre><code>getDropUnseen()\n</code></pre> <p>Gets the dropUnseen parameter.</p> <p>:returns: Bool value of whether to drop unseen label index in the one hot encoder layer.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getDropUnseen(self) -&gt; bool:\n    \"\"\"\n    Gets the dropUnseen parameter.\n\n    :returns: Bool value of whether to drop unseen label index\n    in the one hot encoder layer.\n    \"\"\"\n    return self.getOrDefault(self.dropUnseen)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.DropUnseenParams.setDropUnseen","title":"setDropUnseen","text":"<pre><code>setDropUnseen(value)\n</code></pre> <p>Sets the dropUnseen parameter.</p> <p>:param value: Bool value of whether to drop unseen label index in the one hot encoder layer. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setDropUnseen(self, value: bool) -&gt; \"DropUnseenParams\":\n    \"\"\"\n    Sets the dropUnseen parameter.\n\n    :param value: Bool value of whether to drop unseen label index\n    in the one hot encoder layer.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(dropUnseen=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.HashIndexParams","title":"HashIndexParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing bin parameter needed for hash indexing layers.</p>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.HashIndexParams.getMaskValue","title":"getMaskValue","text":"<pre><code>getMaskValue()\n</code></pre> <p>Gets the maskValue parameter.</p> <p>:returns: String value for the mask value to use for hash indexing.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getMaskValue(self) -&gt; str:\n    \"\"\"\n    Gets the maskValue parameter.\n\n    :returns: String value for the mask value to use for hash indexing.\n    \"\"\"\n    return self.getOrDefault(self.maskValue)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.HashIndexParams.getNumBins","title":"getNumBins","text":"<pre><code>getNumBins()\n</code></pre> <p>Gets the numBins parameter.</p> <p>:returns: Integer value for the number of bins to use for hash indexing.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getNumBins(self) -&gt; int:\n    \"\"\"\n    Gets the numBins parameter.\n\n    :returns: Integer value for the number of bins to use for hash indexing.\n    \"\"\"\n    return self.getOrDefault(self.numBins)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.HashIndexParams.setMaskValue","title":"setMaskValue","text":"<pre><code>setMaskValue(value)\n</code></pre> <p>Sets the maskValue parameter.</p> <p>:param value: String value for the mask value to use for hash indexing. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setMaskValue(self, value: str) -&gt; \"HashIndexParams\":\n    \"\"\"\n    Sets the maskValue parameter.\n\n    :param value: String value for the mask value to use for hash indexing.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(maskValue=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.HashIndexParams.setNumBins","title":"setNumBins","text":"<pre><code>setNumBins(value)\n</code></pre> <p>Sets the numBins parameter.</p> <p>:param value: Integer value for the number of bins to use for hash indexing. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setNumBins(self, value: int) -&gt; \"HashIndexParams\":\n    \"\"\"\n    Sets the numBins parameter.\n\n    :param value: Integer value for the number of bins to use for hash indexing.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if value &lt;= 0:\n        raise ValueError(\"Number of bins must be greater than 0.\")\n    return self._set(numBins=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.ImputeMethodParams","title":"ImputeMethodParams","text":"<pre><code>ImputeMethodParams()\n</code></pre> <p>               Bases: <code>Params</code></p> <p>Mixin class containing imputeParam parameter for imputation layer. This parameter is used to select the method to estimate the value that should be imputed over the mask.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__()\n    self.valid_impute_methods = [\"mean\", \"median\"]\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.ImputeMethodParams.getImputeMethod","title":"getImputeMethod","text":"<pre><code>getImputeMethod()\n</code></pre> <p>Gets the imputeParam parameter value. :returns: Str value of the method to use to compute the value to impute.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getImputeMethod(self) -&gt; str:\n    \"\"\"\n    Gets the imputeParam parameter value.\n    :returns: Str value of the method to use to compute the value to impute.\n    \"\"\"\n    return self.getOrDefault(self.imputeMethod)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.ImputeMethodParams.setImputeMethod","title":"setImputeMethod","text":"<pre><code>setImputeMethod(value)\n</code></pre> <p>Sets the imputeParam parameter for the imputation layer.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setImputeMethod(self, value: str) -&gt; \"ImputeMethodParams\":\n    \"\"\"\n    Sets the imputeParam parameter for the imputation layer.\n    \"\"\"\n    if value not in self.valid_impute_methods:\n        raise ValueError(\n            \"Imputation method should be one of: [{}], but received {}\".format(\n                \",\".join(self.valid_impute_methods), value\n            )\n        )\n    return self._set(imputeMethod=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.LabelsArrayParams","title":"LabelsArrayParams","text":"<p>               Bases: <code>Params</code></p>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.LabelsArrayParams.getLabelsArray","title":"getLabelsArray","text":"<pre><code>getLabelsArray()\n</code></pre> <p>Gets the labelArray parameter.</p> <p>:returns: List of strings to use in indexing transformers.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getLabelsArray(self) -&gt; List[str]:\n    \"\"\"\n    Gets the labelArray parameter.\n\n    :returns: List of strings to use in indexing transformers.\n    \"\"\"\n    return self.getOrDefault(self.labelsArray)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.LabelsArrayParams.setLabelsArray","title":"setLabelsArray","text":"<pre><code>setLabelsArray(value)\n</code></pre> <p>Sets the labelArray parameter.</p> <p>:param value: List of strings to use in indexing transformers. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setLabelsArray(self, value: List[str]) -&gt; \"LabelsArrayParams\":\n    \"\"\"\n    Sets the labelArray parameter.\n\n    :param value: List of strings to use in indexing transformers.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(labelsArray=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.LatLonConstantParams","title":"LatLonConstantParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing lat and lon constant parameters.</p>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.LatLonConstantParams.getLatLonConstant","title":"getLatLonConstant","text":"<pre><code>getLatLonConstant()\n</code></pre> <p>Gets the latLonConstant parameter. :returns: List of float value of lat and lon used in haversine distance and bearing angle calculation.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getLatLonConstant(self) -&gt; List[float]:\n    \"\"\"\n    Gets the latLonConstant parameter.\n    :returns: List of float value of lat and lon used in haversine distance\n    and bearing angle calculation.\n    \"\"\"\n    return self.getOrDefault(self.latLonConstant)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.LatLonConstantParams.setLatLonConstant","title":"setLatLonConstant","text":"<pre><code>setLatLonConstant(value)\n</code></pre> <p>Sets the latLonConstant parameter. :param value: List of float lat and lon values. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setLatLonConstant(self, value: List[float]) -&gt; \"LatLonConstantParams\":\n    \"\"\"\n    Sets the latLonConstant parameter.\n    :param value: List of float lat and lon values.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if len(value) != 2:\n        raise ValueError(\"latLonConstant must be a list of two floats: [lat, lon]\")\n    elif self.isDefined(\"inputCols\") and len(self.getInputCols()) != 2:\n        raise ValueError(\n            f\"\"\"In order to set latLonConstant, there must be exactly two\n            input columns. Found {len(self.getInputCols())} columns.\"\"\"\n        )\n    elif value[0] &lt; -90.0 or value[0] &gt; 90.0:\n        raise ValueError(\"Latitude must be between -90 and 90\")\n    elif value[1] &lt; -180.0 or value[1] &gt; 180.0:\n        raise ValueError(\"Longitude must be between -180 and 180\")\n    return self._set(latLonConstant=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.ListwiseParams","title":"ListwiseParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing the parameters needed for Listwise transformers.</p>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.ListwiseParams.getQueryIdCol","title":"getQueryIdCol","text":"<pre><code>getQueryIdCol()\n</code></pre> <p>Gets the query id parameter.</p> <p>:returns:  String for column name to aggregate upon.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getQueryIdCol(self) -&gt; str:\n    \"\"\"\n    Gets the query id parameter.\n\n    :returns:  String for column name to aggregate upon.\n    \"\"\"\n    return self.getOrDefault(self.queryIdCol)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.ListwiseParams.getSortOrder","title":"getSortOrder","text":"<pre><code>getSortOrder()\n</code></pre> <p>Gets the sort order parameter.</p> <p>:returns: String to set the stringOrderType parameter to.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getSortOrder(self) -&gt; str:\n    \"\"\"\n    Gets the sort order parameter.\n\n    :returns: String to set the stringOrderType parameter to.\n    \"\"\"\n    return self.getOrDefault(self.sortOrder)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.ListwiseParams.setQueryIdCol","title":"setQueryIdCol","text":"<pre><code>setQueryIdCol(value)\n</code></pre> <p>Sets the query id parameter.</p> <p>:param value: String for column name to aggregate upon. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setQueryIdCol(self, value: str) -&gt; \"ListwiseParams\":\n    \"\"\"\n    Sets the query id parameter.\n\n    :param value: String for column name to aggregate upon.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(queryIdCol=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.ListwiseParams.setSortOrder","title":"setSortOrder","text":"<pre><code>setSortOrder(value)\n</code></pre> <p>Sets the sortOrder parameter to the given value. Must be one of 'asc' or 'desc'.</p> <p>:param value: String to set the stringOrderType parameter to. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setSortOrder(self, value: str) -&gt; \"ListwiseParams\":\n    \"\"\"\n    Sets the sortOrder parameter to the given value.\n    Must be one of 'asc' or 'desc'.\n\n    :param value: String to set the stringOrderType parameter to.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    possible_order_options = [\n        \"asc\",\n        \"desc\",\n    ]\n    if value not in possible_order_options:\n        raise ValueError(\n            f\"sortOrderType must be one of {', '.join(possible_order_options)}\"\n        )\n    return self._set(sortOrder=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.ListwiseStatisticsParams","title":"ListwiseStatisticsParams","text":"<p>               Bases: <code>ListwiseParams</code></p> <p>Mixin class containing the parameters needed for Listwise Statistics transformers.</p>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.ListwiseStatisticsParams.getMinFilterValue","title":"getMinFilterValue","text":"<pre><code>getMinFilterValue()\n</code></pre> <p>Gets the min filter value parameter.</p> <p>:returns: Minimum value to remove padded values - defaults to &gt;= 0.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getMinFilterValue(self) -&gt; int:\n    \"\"\"\n    Gets the min filter value parameter.\n\n    :returns: Minimum value to remove padded values - defaults to &gt;= 0.\n    \"\"\"\n    return self.getOrDefault(self.minFilterValue)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.ListwiseStatisticsParams.getTopN","title":"getTopN","text":"<pre><code>getTopN()\n</code></pre> <p>Gets the top N parameter.</p> <p>:returns:  Filter to limit length of input feature. Should be ordered first.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getTopN(self) -&gt; int:\n    \"\"\"\n    Gets the top N parameter.\n\n    :returns:  Filter to limit length of input feature. Should be ordered first.\n    \"\"\"\n    return self.getOrDefault(self.topN)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.ListwiseStatisticsParams.setInputCols","title":"setInputCols","text":"<pre><code>setInputCols(value)\n</code></pre> <p>Overrides setting the input columns for the transformer. Throws an error if we do not have exactly two input columns. :returns: Class instance.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setInputCols(self, value: List[str]) -&gt; \"ListwiseStatisticsParams\":\n    \"\"\"\n    Overrides setting the input columns for the transformer.\n    Throws an error if we do not have exactly two input columns.\n    :returns: Class instance.\n    \"\"\"\n    if len(value) != 2:\n        raise ValueError(\n            \"\"\"\n            Arg inputCols must contain exactly two columns.\n            The first is the value column, on which to calculate the statistic.\n            The second is the sort column, based on which to sort the items.\n            If you don't need sorting, use setInputCol instead.\n            \"\"\"\n        )\n    if self.getTopN() is None:\n        raise ValueError(\"topN must be specified when using a sort column.\")\n    return self._set(inputCols=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.ListwiseStatisticsParams.setMinFilterValue","title":"setMinFilterValue","text":"<pre><code>setMinFilterValue(value)\n</code></pre> <p>Sets the min filter value parameter.</p> <p>:param value: Minimum value to remove padded values. Defaults to 0. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setMinFilterValue(self, value: int) -&gt; \"ListwiseStatisticsParams\":\n    \"\"\"\n    Sets the min filter value parameter.\n\n    :param value: Minimum value to remove padded values. Defaults to 0.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(minFilterValue=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.ListwiseStatisticsParams.setTopN","title":"setTopN","text":"<pre><code>setTopN(value)\n</code></pre> <p>Sets the top N parameter.</p> <p>:param value: Filter to limit length of input feature. Should be ordered first. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setTopN(self, value: int) -&gt; \"ListwiseStatisticsParams\":\n    \"\"\"\n    Sets the top N parameter.\n\n    :param value: Filter to limit length of input feature. Should be ordered first.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(topN=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.MaskStringValueParams","title":"MaskStringValueParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing maskValue parameter needed for MinHashIndexTransformer and other transformers that require a string mask value.</p>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.MaskStringValueParams.getMaskValue","title":"getMaskValue","text":"<pre><code>getMaskValue()\n</code></pre> <p>Gets the maskValue parameter. :returns: Str value of the mask value.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getMaskValue(self) -&gt; str:\n    \"\"\"\n    Gets the maskValue parameter.\n    :returns: Str value of the mask value.\n    \"\"\"\n    return self.getOrDefault(self.maskValue)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.MaskStringValueParams.setMaskValue","title":"setMaskValue","text":"<pre><code>setMaskValue(value)\n</code></pre> <p>Sets the maskValue parameter. :param value: Str value to use as the mask value. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setMaskValue(self, value: str) -&gt; \"MaskStringValueParams\":\n    \"\"\"\n    Sets the maskValue parameter.\n    :param value: Str value to use as the mask value.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(maskValue=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.MaskValueParams","title":"MaskValueParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing maskValue parameter needed for standard scale and imputation layers. This parameter is used to ignore certain values in the scaling process. For imputation, the value is ignored by the estimator and imputed over at training and inference.</p>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.MaskValueParams.getMaskValue","title":"getMaskValue","text":"<pre><code>getMaskValue()\n</code></pre> <p>Gets the maskValue parameter. :returns: Float value of the mask value.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getMaskValue(self) -&gt; float:\n    \"\"\"\n    Gets the maskValue parameter.\n    :returns: Float value of the mask value.\n    \"\"\"\n    return self.getOrDefault(self.maskValue)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.MaskValueParams.setMaskValue","title":"setMaskValue","text":"<pre><code>setMaskValue(value)\n</code></pre> <p>Sets the maskValue parameter. :param value: Float value to use as the mask value. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setMaskValue(self, value: float) -&gt; \"MaskValueParams\":\n    \"\"\"\n    Sets the maskValue parameter.\n    :param value: Float value to use as the mask value.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(maskValue=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.MathFloatConstantParams","title":"MathFloatConstantParams","text":"<pre><code>MathFloatConstantParams()\n</code></pre> <p>               Bases: <code>Params</code></p> <p>Mixin class for a math float constant.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__()\n    self._setDefault(layerName=self.uid)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.MathFloatConstantParams.getMathFloatConstant","title":"getMathFloatConstant","text":"<pre><code>getMathFloatConstant()\n</code></pre> <p>Gets the value of the mathFloatConstant parameter.</p> <p>:returns: Float constant used for math operations.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getMathFloatConstant(self) -&gt; float:\n    \"\"\"\n    Gets the value of the mathFloatConstant parameter.\n\n    :returns: Float constant used for math operations.\n    \"\"\"\n    return self.getOrDefault(self.mathFloatConstant)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.MathFloatConstantParams.setMathFloatConstant","title":"setMathFloatConstant","text":"<pre><code>setMathFloatConstant(value)\n</code></pre> <p>Sets the value of the mathFloatConstant parameter.</p> <p>:param value: Float constant used for math operations. :returns: Class instance.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setMathFloatConstant(self, value: float) -&gt; \"MathFloatConstantParams\":\n    \"\"\"\n    Sets the value of the mathFloatConstant parameter.\n\n    :param value: Float constant used for math operations.\n    :returns: Class instance.\n    \"\"\"\n    return self._set(mathFloatConstant=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.NanFillValueParams","title":"NanFillValueParams","text":"<p>               Bases: <code>Params</code></p>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.NanFillValueParams.getNanFillValue","title":"getNanFillValue","text":"<pre><code>getNanFillValue()\n</code></pre> <p>Gets the nanFillValue parameter. :returns: Float value of the fill value.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getNanFillValue(self) -&gt; float:\n    \"\"\"\n    Gets the nanFillValue parameter.\n    :returns: Float value of the fill value.\n    \"\"\"\n    return self.getOrDefault(self.nanFillValue)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.NanFillValueParams.setNanFillValue","title":"setNanFillValue","text":"<pre><code>setNanFillValue(value)\n</code></pre> <p>Sets the nanFillValue parameter. :param value: Float value to use as the fill value. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setNanFillValue(self, value: float) -&gt; \"NanFillValueParams\":\n    \"\"\"\n    Sets the nanFillValue parameter.\n    :param value: Float value to use as the fill value.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if value is None:\n        raise ValueError(\"nanFillValue cannot be None\")\n    return self._set(nanFillValue=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.NegationParams","title":"NegationParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing negation parameter needed for transforms that output a boolean.</p>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.NegationParams.getNegation","title":"getNegation","text":"<pre><code>getNegation()\n</code></pre> <p>Gets the negation parameter.</p> <p>:returns: Bool value of whether to negate the operation.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getNegation(self) -&gt; bool:\n    \"\"\"\n    Gets the negation parameter.\n\n    :returns: Bool value of whether to negate the operation.\n    \"\"\"\n    return self.getOrDefault(self.negation)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.NegationParams.setNegation","title":"setNegation","text":"<pre><code>setNegation(value)\n</code></pre> <p>Sets the negation parameter.</p> <p>:param value: Bool value of whether to negate the operation. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setNegation(self, value: bool) -&gt; \"NegationParams\":\n    \"\"\"\n    Sets the negation parameter.\n\n    :param value: Bool value of whether to negate the operation.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(negation=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.PadValueParams","title":"PadValueParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing pad value parameters needed for ordinal array encoder transformers and array crop transformers.</p>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.PadValueParams.getPadValue","title":"getPadValue","text":"<pre><code>getPadValue()\n</code></pre> <p>Gets the pad value parameter. :returns: string pad value.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getPadValue(self) -&gt; Union[str, int, float]:\n    \"\"\"\n    Gets the pad value parameter.\n    :returns: string pad value.\n    \"\"\"\n    return self.getOrDefault(self.padValue)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.PadValueParams.setPadValue","title":"setPadValue","text":"<pre><code>setPadValue(value)\n</code></pre> <p>Sets the parameter pad value to the given value. :param value: pad value. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setPadValue(self, value: Union[str, int, float]) -&gt; \"PadValueParams\":\n    \"\"\"\n    Sets the parameter pad value to the given value.\n    :param value: pad value.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(padValue=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.StandardScaleParams","title":"StandardScaleParams","text":"<p>               Bases: <code>MaskValueParams</code></p> <p>Mixin class containing mean and standard deviation parameters needed for standard scaler layers.</p>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.StandardScaleParams.getMean","title":"getMean","text":"<pre><code>getMean()\n</code></pre> <p>Gets the mean parameter.</p> <p>:returns: List of float mean values.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getMean(self) -&gt; List[float]:\n    \"\"\"\n    Gets the mean parameter.\n\n    :returns: List of float mean values.\n    \"\"\"\n    return self.getOrDefault(self.mean)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.StandardScaleParams.getStddev","title":"getStddev","text":"<pre><code>getStddev()\n</code></pre> <p>Gets the stddev parameter.</p> <p>:returns: List of float standard deviation values.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getStddev(self) -&gt; List[float]:\n    \"\"\"\n    Gets the stddev parameter.\n\n    :returns: List of float standard deviation values.\n    \"\"\"\n    return self.getOrDefault(self.stddev)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.StandardScaleParams.setMean","title":"setMean","text":"<pre><code>setMean(value)\n</code></pre> <p>Sets the parameter mean to the given Vector value. Saves the mean as a list of floats.</p> <p>:param value: List of mean values. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setMean(self, value: List[float]) -&gt; \"StandardScaleParams\":\n    \"\"\"\n    Sets the parameter mean to the given Vector value.\n    Saves the mean as a list of floats.\n\n    :param value: List of mean values.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if None in set(value):\n        ids = [i for i, x in enumerate(value) if x is None]\n        raise ValueError(\"Got null Mean values at positions: \", ids)\n    return self._set(mean=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.StandardScaleParams.setStddev","title":"setStddev","text":"<pre><code>setStddev(value)\n</code></pre> <p>Sets the parameter stddev to the given Vector value. Saves the standard deviation as a list of floats.</p> <p>:param value: Vector of standard deviation values. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setStddev(self, value: List[float]) -&gt; \"StandardScaleParams\":\n    \"\"\"\n    Sets the parameter stddev to the given Vector value.\n    Saves the standard deviation as a list of floats.\n\n    :param value: Vector of standard deviation values.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if None in set(value):\n        ids = [i for i, x in enumerate(value) if x is None]\n        raise ValueError(\"Got null Stddev values at positions: \", ids)\n    return self._set(stddev=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.StandardScaleSkipZerosParams","title":"StandardScaleSkipZerosParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing maskValue parameter needed for conditional standard scale layers. This parameter is used to ignore zeros when scaling.</p>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.StandardScaleSkipZerosParams.getEpsilon","title":"getEpsilon","text":"<pre><code>getEpsilon()\n</code></pre> <p>Gets the epsilon parameter.</p> <p>:returns: Float value of the epsilon value.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getEpsilon(self) -&gt; float:\n    \"\"\"\n    Gets the epsilon parameter.\n\n    :returns: Float value of the epsilon value.\n    \"\"\"\n    return self.getOrDefault(self.epsilon)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.StandardScaleSkipZerosParams.getSkipZeros","title":"getSkipZeros","text":"<pre><code>getSkipZeros()\n</code></pre> <p>Gets the skipZeros parameter.</p> <p>:returns: Boolean value of the mask value.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getSkipZeros(self) -&gt; bool:\n    \"\"\"\n    Gets the skipZeros parameter.\n\n    :returns: Boolean value of the mask value.\n    \"\"\"\n    return self.getOrDefault(self.skipZeros)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.StandardScaleSkipZerosParams.setEpsilon","title":"setEpsilon","text":"<pre><code>setEpsilon(value)\n</code></pre> <p>Sets the epsilon parameter.</p> <p>:param value: Float value to use as the epsilon value. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setEpsilon(self, value: float) -&gt; \"StandardScaleSkipZerosParams\":\n    \"\"\"\n    Sets the epsilon parameter.\n\n    :param value: Float value to use as the epsilon value.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(epsilon=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.StandardScaleSkipZerosParams.setSkipZeros","title":"setSkipZeros","text":"<pre><code>setSkipZeros(value)\n</code></pre> <p>Sets the skipZeros parameter.</p> <p>:param value: Boolean value to use as the mask value. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setSkipZeros(self, value: bool) -&gt; \"StandardScaleSkipZerosParams\":\n    \"\"\"\n    Sets the skipZeros parameter.\n\n    :param value: Boolean value to use as the mask value.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(skipZeros=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.StringConstantParams","title":"StringConstantParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class for a string constant.</p>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.StringConstantParams.getStringConstant","title":"getStringConstant","text":"<pre><code>getStringConstant()\n</code></pre> <p>Gets the stringConstant parameter.</p> <p>:returns: String constant value to use in different string transformers.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getStringConstant(self) -&gt; str:\n    \"\"\"\n    Gets the stringConstant parameter.\n\n    :returns: String constant value to use in different string transformers.\n    \"\"\"\n    return self.getOrDefault(self.stringConstant)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.StringConstantParams.setStringConstant","title":"setStringConstant","text":"<pre><code>setStringConstant(value)\n</code></pre> <p>Sets the stringConstant parameter.</p> <p>:param value: String constant value to use in different string transformers. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setStringConstant(self, value: str) -&gt; \"StringConstantParams\":\n    \"\"\"\n    Sets the stringConstant parameter.\n\n    :param value: String constant value to use in different string transformers.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(stringConstant=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.StringIndexParams","title":"StringIndexParams","text":"<p>               Bases: <code>LabelsArrayParams</code></p> <p>Mixin class containing parameters needed for string indexer and one hot encoder layers.</p>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.StringIndexParams.getMaskToken","title":"getMaskToken","text":"<pre><code>getMaskToken()\n</code></pre> <p>Gets the maskToken parameter.</p> <p>:returns: String value for the mask token to use for string indexing.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getMaskToken(self) -&gt; str:\n    \"\"\"\n    Gets the maskToken parameter.\n\n    :returns: String value for the mask token to use for string indexing.\n    \"\"\"\n    return self.getOrDefault(self.maskToken)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.StringIndexParams.getMaxNumLabels","title":"getMaxNumLabels","text":"<pre><code>getMaxNumLabels()\n</code></pre> <p>Gets the maxNumLabels parameter.</p> <p>:returns: Int value for the max number of labels to use for string indexing.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getMaxNumLabels(self) -&gt; int:\n    \"\"\"\n    Gets the maxNumLabels parameter.\n\n    :returns: Int value for the max number of labels to use for string\n    indexing.\n    \"\"\"\n    return self.getOrDefault(self.maxNumLabels)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.StringIndexParams.getNumOOVIndices","title":"getNumOOVIndices","text":"<pre><code>getNumOOVIndices()\n</code></pre> <p>Gets the numOOVIndices parameter.</p> <p>:returns: Int value for the number of OOV indices to use for string indexing.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getNumOOVIndices(self) -&gt; int:\n    \"\"\"\n    Gets the numOOVIndices parameter.\n\n    :returns: Int value for the number of OOV indices to use for string\n    indexing.\n    \"\"\"\n    return self.getOrDefault(self.numOOVIndices)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.StringIndexParams.getStringOrderType","title":"getStringOrderType","text":"<pre><code>getStringOrderType()\n</code></pre> <p>Gets the stringOrderType parameter.</p> <p>:returns: String value of the stringOrderType parameter.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getStringOrderType(self) -&gt; str:\n    \"\"\"\n    Gets the stringOrderType parameter.\n\n    :returns: String value of the stringOrderType parameter.\n    \"\"\"\n    return self.getOrDefault(self.stringOrderType)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.StringIndexParams.setMaskToken","title":"setMaskToken","text":"<pre><code>setMaskToken(value)\n</code></pre> <p>Sets the maskToken parameter.</p> <p>:param value: String value for the mask token to use for string indexing. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setMaskToken(self, value: str) -&gt; \"StringIndexParams\":\n    \"\"\"\n    Sets the maskToken parameter.\n\n    :param value: String value for the mask token to use for string indexing.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(maskToken=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.StringIndexParams.setMaxNumLabels","title":"setMaxNumLabels","text":"<pre><code>setMaxNumLabels(value)\n</code></pre> <p>Sets the maxNumLabels parameter.</p> <p>:param value: Int value for the max number of labels to use for string indexing. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setMaxNumLabels(self, value: int) -&gt; \"StringIndexParams\":\n    \"\"\"\n    Sets the maxNumLabels parameter.\n\n    :param value: Int value for the max number of labels to use for string\n    indexing.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if value &lt;= 0:\n        raise ValueError(\"maxNumLabels must be a positive integer\")\n    return self._set(maxNumLabels=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.StringIndexParams.setNumOOVIndices","title":"setNumOOVIndices","text":"<pre><code>setNumOOVIndices(value)\n</code></pre> <p>Sets the numOOVIndices parameter.</p> <p>:param value: Int value for the number of OOV indices to use for string indexing. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setNumOOVIndices(self, value: int) -&gt; \"StringIndexParams\":\n    \"\"\"\n    Sets the numOOVIndices parameter.\n\n    :param value: Int value for the number of OOV indices to use for string\n    indexing.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if value &lt;= 0:\n        raise ValueError(\"numOOVIndices must be a positive integer\")\n    return self._set(numOOVIndices=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.StringIndexParams.setStringOrderType","title":"setStringOrderType","text":"<pre><code>setStringOrderType(value)\n</code></pre> <p>Sets the stringOrderType parameter to the given value. Must be one of: - 'frequencyAsc' - 'frequencyDesc' - 'alphabeticalAsc' - 'alphabeticalDesc'.</p> <p>:param value: String to set the stringOrderType parameter to. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setStringOrderType(self, value: str) -&gt; \"StringIndexParams\":\n    \"\"\"\n    Sets the stringOrderType parameter to the given value.\n    Must be one of:\n    - 'frequencyAsc'\n    - 'frequencyDesc'\n    - 'alphabeticalAsc'\n    - 'alphabeticalDesc'.\n\n    :param value: String to set the stringOrderType parameter to.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    possible_order_options = [\n        \"frequencyAsc\",\n        \"frequencyDesc\",\n        \"alphabeticalAsc\",\n        \"alphabeticalDesc\",\n    ]\n    if value not in possible_order_options:\n        raise ValueError(\n            f\"stringOrderType must be one of {', '.join(possible_order_options)}\"\n        )\n    return self._set(stringOrderType=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.StringRegexParams","title":"StringRegexParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class for string transformers that use regex.</p>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.StringRegexParams.getRegex","title":"getRegex","text":"<pre><code>getRegex()\n</code></pre> <p>Gets the regex parameter.</p> <p>:returns: Bool value of whether to use regex in the string contains  operation.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getRegex(self) -&gt; bool:\n    \"\"\"\n    Gets the regex parameter.\n\n    :returns: Bool value of whether to use regex in the string contains\n     operation.\n    \"\"\"\n    return self.getOrDefault(self.regex)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.StringRegexParams.setRegex","title":"setRegex","text":"<pre><code>setRegex(value)\n</code></pre> <p>Sets the regex parameter.</p> <p>:param value: Bool value of whether to use regex in the string contains operation. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setRegex(self, value: bool) -&gt; \"StringRegexParams\":\n    \"\"\"\n    Sets the regex parameter.\n\n    :param value: Bool value of whether to use regex in the string contains\n    operation.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(regex=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.UnixTimestampParams","title":"UnixTimestampParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class for a unix timestamp</p>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.UnixTimestampParams.getUnit","title":"getUnit","text":"<pre><code>getUnit()\n</code></pre> <p>Gets the unit parameter. :returns: unit.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def getUnit(self) -&gt; str:\n    \"\"\"\n    Gets the unit parameter.\n    :returns: unit.\n    \"\"\"\n    return self.getOrDefault(self.unit)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/shared/#src.kamae.spark.params.shared.UnixTimestampParams.setUnit","title":"setUnit","text":"<pre><code>setUnit(value)\n</code></pre> <p>Sets the unit parameter. :param value: unit. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/params/shared.py</code> <pre><code>def setUnit(self, value: str) -&gt; \"UnixTimestampParams\":\n    \"\"\"\n    Sets the unit parameter.\n    :param value: unit.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    allowed_units = [\"milliseconds\", \"seconds\", \"ms\", \"s\"]\n    if value not in allowed_units:\n        raise ValueError(f\"Unit must be one of {allowed_units}\")\n\n    if value == \"milliseconds\":\n        value = \"ms\"\n    if value == \"seconds\":\n        value = \"s\"\n\n    return self._set(unit=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/params/utils/","title":"utils","text":""},{"location":"reference/src/kamae/spark/params/utils/#src.kamae.spark.params.utils.InputOutputExtractor","title":"InputOutputExtractor","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class for extracting input &amp; output information from a transformer/estimator.</p> <p>Used across all transformers/estimators to facilitate the construction of the pipeline graph.</p>"},{"location":"reference/src/kamae/spark/params/utils/#src.kamae.spark.params.utils.InputOutputExtractor._get_single_or_multi_col","title":"_get_single_or_multi_col","text":"<pre><code>_get_single_or_multi_col(ingress)\n</code></pre> <p>Gets the input or output column name(s) of the layer. If the layer has a single input/output column, then the name of the column is returned as a singleton list. If the layer has multiple input/output columns, then the list of column names is returned. :param ingress: Whether to get input (True) or output (False) column names. :return: List of input or output column names.</p> Source code in <code>src/kamae/spark/params/utils.py</code> <pre><code>def _get_single_or_multi_col(self, ingress: bool) -&gt; List[str]:\n    \"\"\"\n    Gets the input or output column name(s) of the layer. If the layer has a\n    single input/output column, then the name of the column is returned as a\n    singleton list. If the layer has multiple input/output columns, then the list of\n    column names is returned.\n    :param ingress: Whether to get input (True) or output (False) column names.\n    :return: List of input or output column names.\n    \"\"\"\n    single_prefix = \"inputCol\" if ingress else \"outputCol\"\n    multi_prefix = f\"{single_prefix}s\"\n    if self.hasParam(multi_prefix) and self.isDefined(multi_prefix):\n        return self.getOrDefault(multi_prefix)\n    elif self.hasParam(single_prefix) and self.isDefined(single_prefix):\n        return [self.getOrDefault(single_prefix)]\n    else:\n        return []\n</code></pre>"},{"location":"reference/src/kamae/spark/params/utils/#src.kamae.spark.params.utils.InputOutputExtractor.get_column_datatype","title":"get_column_datatype  <code>staticmethod</code>","text":"<pre><code>get_column_datatype(dataset, column_name)\n</code></pre> <p>Gets the datatype of a column in a dataset. First selects the column to ensure that if it is a struct type, the datatype of the struct element is returned.</p> <p>:param dataset: Dataset to get the column datatype from. :param column_name: Name of the column to get the datatype of. :return: Datatype of the column.</p> Source code in <code>src/kamae/spark/params/utils.py</code> <pre><code>@staticmethod\ndef get_column_datatype(dataset: DataFrame, column_name: str) -&gt; DataType:\n    \"\"\"\n    Gets the datatype of a column in a dataset. First selects the column to ensure\n    that if it is a struct type, the datatype of the struct element is returned.\n\n    :param dataset: Dataset to get the column datatype from.\n    :param column_name: Name of the column to get the datatype of.\n    :return: Datatype of the column.\n    \"\"\"\n    return (\n        dataset.select(F.col(column_name).alias(\"input\")).schema[\"input\"].dataType\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/params/utils/#src.kamae.spark.params.utils.InputOutputExtractor.get_layer_inputs_outputs","title":"get_layer_inputs_outputs","text":"<pre><code>get_layer_inputs_outputs()\n</code></pre> <p>Gets the input &amp; output information of the layer. Returns a tuple of lists, the first containing the input column names and the second containing the output column names.</p> <p>:returns: Tuple of lists containing the input and output column names.</p> Source code in <code>src/kamae/spark/params/utils.py</code> <pre><code>def get_layer_inputs_outputs(self) -&gt; Tuple[List[str], List[str]]:\n    \"\"\"\n    Gets the input &amp; output information of the layer. Returns a tuple of lists,\n    the first containing the input column names and the second containing the\n    output column names.\n\n    :returns: Tuple of lists containing the input and output column names.\n    \"\"\"\n    inputs = self._get_single_or_multi_col(ingress=True)\n    outputs = self._get_single_or_multi_col(ingress=False)\n    return inputs, outputs\n</code></pre>"},{"location":"reference/src/kamae/spark/params/utils/#src.kamae.spark.params.utils.InputOutputExtractor.get_multiple_input_cols","title":"get_multiple_input_cols","text":"<pre><code>get_multiple_input_cols(\n    constant_param_name, input_cols_limit=None\n)\n</code></pre> <p>Gets the (possibly multiple) input columns for the transformer. If using multiple input columns, we get them, raising an error if the number is greater than the limit. If using a single input column, we get the input column and the constant_param_name.</p> <p>:returns: List of columns</p> Source code in <code>src/kamae/spark/params/utils.py</code> <pre><code>def get_multiple_input_cols(\n    self, constant_param_name: str, input_cols_limit: Optional[int] = None\n) -&gt; List[Column]:\n    \"\"\"\n    Gets the (possibly multiple) input columns for the transformer.\n    If using multiple input columns, we get them, raising an error if the number is\n    greater than the limit. If using a single input column, we get the input column\n    and the constant_param_name.\n\n    :returns: List of columns\n    \"\"\"\n    if self.isDefined(\"inputCols\"):\n        # If multiple input columns are defined, we get them, throwing an error\n        # if there are more than the limit.\n        input_cols = self.getOrDefault(\"inputCols\")\n        if input_cols_limit is not None and len(input_cols) &gt; input_cols_limit:\n            raise ValueError(\n                f\"\"\"\n                Number of input columns ({len(input_cols)})\n                exceeds limit ({input_cols_limit}).\n                \"\"\"\n            )\n\n        return [F.col(c) for c in input_cols]\n    elif (\n        self.isDefined(\"inputCol\")\n        and self.getOrDefault(constant_param_name) is not None\n    ):\n        # If only one input column is defined, we use it\n        # alongside the constant.\n        return [\n            F.col(self.getOrDefault(\"inputCol\")),\n            F.lit(self.getOrDefault(constant_param_name)).alias(\n                self.uid + constant_param_name\n            ),\n        ]\n    else:\n        # If neither inputCols nor inputCol &amp; constant_param_name are defined,\n        # we raise an error.\n        raise ValueError(\n            f\"\"\"\n            Either inputCols or inputCol &amp; {constant_param_name} must be defined.\n            \"\"\"\n        )\n</code></pre>"},{"location":"reference/src/kamae/spark/pipeline/","title":"pipeline","text":""},{"location":"reference/src/kamae/spark/pipeline/pipeline/","title":"pipeline","text":""},{"location":"reference/src/kamae/spark/pipeline/pipeline/#src.kamae.spark.pipeline.pipeline.KamaeSparkPipeline","title":"KamaeSparkPipeline","text":"<pre><code>KamaeSparkPipeline(*, stages=None)\n</code></pre> <p>               Bases: <code>Pipeline</code></p> <p>KamaeSparkPipeline is a subclass of pyspark.ml.Pipeline that is used to chain together BaseTransformers. It maintains the same functionality as pyspark.ml.Pipeline e.g. serialisation.</p> <p>:param stages: List of LayerTransformers to chain together. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/pipeline/pipeline.py</code> <pre><code>@keyword_only\ndef __init__(self, *, stages: Optional[List[\"KamaePipelineStage\"]] = None) -&gt; None:\n    \"\"\"\n    Initialises the KamaeSparkPipeline object.\n\n    :param stages: List of LayerTransformers to chain together.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__(stages=stages)\n</code></pre>"},{"location":"reference/src/kamae/spark/pipeline/pipeline/#src.kamae.spark.pipeline.pipeline.KamaeSparkPipeline._fit","title":"_fit","text":"<pre><code>_fit(dataset)\n</code></pre> <p>Fits the pipeline to the dataset. Returns a KamaeSparkPipelineModel object.</p> <p>Calls the super fit method of the pyspark.ml.Pipeline class and then constructs a KamaeSparkPipelineModel uses the stages from the fit pipeline.</p> <p>:param dataset: PySpark DataFrame to fit the pipeline to. :returns: KamaeSparkPipelineModel object.</p> Source code in <code>src/kamae/spark/pipeline/pipeline.py</code> <pre><code>def _fit(self, dataset: DataFrame) -&gt; \"KamaeSparkPipelineModel\":\n    \"\"\"\n    Fits the pipeline to the dataset. Returns a KamaeSparkPipelineModel object.\n\n    Calls the super fit method of the pyspark.ml.Pipeline class and\n    then constructs a KamaeSparkPipelineModel uses the stages from the fit pipeline.\n\n    :param dataset: PySpark DataFrame to fit the pipeline to.\n    :returns: KamaeSparkPipelineModel object.\n    \"\"\"\n    expanded_pipeline_stages = self.expand_pipeline_stages()\n\n    for stage in expanded_pipeline_stages:\n        if not (\n            isinstance(stage, BaseEstimator) or isinstance(stage, BaseTransformer)\n        ):\n            raise TypeError(\n                \"Cannot recognize a pipeline stage of type %s.\" % type(stage)\n            )\n\n    # Native Spark checks for the last estimator and executes all transformers\n    # before it, regardless whether there is a dependency between them. See here:\n    # https://github.com/apache/spark/blob/master/python/pyspark/ml/pipeline.py#L120\n    # We can be clever, since we have built a proper DAG, by only executing\n    # transformers that are required by the estimator.\n\n    # Collect the parents of the estimators in the pipeline\n    estimator_parent_stages = self.collect_estimator_parents(\n        expanded_pipeline_stages\n    )\n    # Fit each stage, appending the transformer to the list of transformers\n    # If the stage is a parent of an estimator, transform the dataset.\n    transformers: List[BaseTransformer] = []\n    for stage in expanded_pipeline_stages:\n        if isinstance(stage, BaseTransformer):\n            transformers.append(stage)\n            if stage in estimator_parent_stages:\n                dataset = stage.transform(dataset)\n        else:\n            model = stage.fit(dataset)\n            transformers.append(model)\n            if stage in estimator_parent_stages:\n                dataset = model.transform(dataset)\n    return KamaeSparkPipelineModel(transformers)\n</code></pre>"},{"location":"reference/src/kamae/spark/pipeline/pipeline/#src.kamae.spark.pipeline.pipeline.KamaeSparkPipeline.collect_estimator_parents","title":"collect_estimator_parents  <code>staticmethod</code>","text":"<pre><code>collect_estimator_parents(stages)\n</code></pre> <p>Collects the parent stages of the estimators in the pipeline.</p> <p>Used to determine which transformers to execute before the estimators in the pipeline.</p> <p>:param stages: List of pipeline stages. :returns: List of names of the ancestors of the estimators in the pipeline.</p> Source code in <code>src/kamae/spark/pipeline/pipeline.py</code> <pre><code>@staticmethod\ndef collect_estimator_parents(\n    stages: List[\"KamaePipelineStage\"],\n) -&gt; List[\"KamaePipelineStage\"]:\n    \"\"\"\n    Collects the parent stages of the estimators in the pipeline.\n\n    Used to determine which transformers to execute before the estimators in the\n    pipeline.\n\n    :param stages: List of pipeline stages.\n    :returns: List of names of the ancestors of the estimators in the pipeline.\n    \"\"\"\n    stage_dict = {\n        stage.getOrDefault(\"layerName\"): stage.construct_layer_info()\n        for stage in stages\n    }\n    pipeline_graph = PipelineGraph(stage_dict=stage_dict)\n    estimator_stages = [\n        stage for stage in stages if isinstance(stage, BaseEstimator)\n    ]\n    estimator_parents = []\n    for estimator in estimator_stages:\n        layer_name = estimator.getLayerName()\n        specific_estimator_parents = nx.ancestors(pipeline_graph.graph, layer_name)\n        estimator_parents.extend(specific_estimator_parents)\n\n    distinct_estimator_parents = list(set(estimator_parents))\n    estimator_parent_stages = [\n        stage\n        for stage in stages\n        if stage.getLayerName() in distinct_estimator_parents\n    ]\n    return estimator_parent_stages\n</code></pre>"},{"location":"reference/src/kamae/spark/pipeline/pipeline/#src.kamae.spark.pipeline.pipeline.KamaeSparkPipeline.copy","title":"copy","text":"<pre><code>copy(extra=None)\n</code></pre> <p>Creates a copy of the KamaeSparkPipeline object.</p> <p>:param extra: Additional optional params to copy to new pipeline. :returns: KamaeSparkPipeline object.</p> Source code in <code>src/kamae/spark/pipeline/pipeline.py</code> <pre><code>def copy(self, extra: Optional[\"ParamMap\"] = None) -&gt; \"KamaeSparkPipeline\":\n    \"\"\"\n    Creates a copy of the KamaeSparkPipeline object.\n\n    :param extra: Additional optional params to copy to new pipeline.\n    :returns: KamaeSparkPipeline object.\n    \"\"\"\n    if extra is None:\n        extra = dict()\n    that = Params.copy(self, extra)\n    stages = [stage.copy(extra) for stage in that.getStages()]\n    return that.setStages(stages)\n</code></pre>"},{"location":"reference/src/kamae/spark/pipeline/pipeline/#src.kamae.spark.pipeline.pipeline.KamaeSparkPipeline.expand_pipeline_stages","title":"expand_pipeline_stages","text":"<pre><code>expand_pipeline_stages()\n</code></pre> <p>Expands the pipeline stages to include all nested pipeline stages. If the pipeline stage is itself a pipeline model, it will be expanded recursively.</p> <p>:returns: List of all pipeline stages flattened to transformer level.</p> Source code in <code>src/kamae/spark/pipeline/pipeline.py</code> <pre><code>def expand_pipeline_stages(self) -&gt; List[\"KamaePipelineStage\"]:\n    \"\"\"\n    Expands the pipeline stages to include all nested pipeline stages.\n    If the pipeline stage is itself a pipeline model, it will be expanded\n    recursively.\n\n    :returns: List of all pipeline stages flattened to transformer level.\n    \"\"\"\n    expanded_stages = []\n    for stage in self.getStages():\n        if isinstance(stage, (KamaeSparkPipelineModel, KamaeSparkPipeline)):\n            # Recursively expand the pipeline stages.\n            expanded_stages.extend(stage.expand_pipeline_stages())\n        else:\n            expanded_stages.append(stage)\n    return expanded_stages\n</code></pre>"},{"location":"reference/src/kamae/spark/pipeline/pipeline/#src.kamae.spark.pipeline.pipeline.KamaeSparkPipeline.getStages","title":"getStages","text":"<pre><code>getStages()\n</code></pre> <p>Gets the stages of the pipeline.</p> <p>:returns: List of pipeline stages.</p> Source code in <code>src/kamae/spark/pipeline/pipeline.py</code> <pre><code>def getStages(self) -&gt; List[\"KamaePipelineStage\"]:\n    \"\"\"\n    Gets the stages of the pipeline.\n\n    :returns: List of pipeline stages.\n    \"\"\"\n    return self.getOrDefault(\"stages\")\n</code></pre>"},{"location":"reference/src/kamae/spark/pipeline/pipeline/#src.kamae.spark.pipeline.pipeline.KamaeSparkPipeline.read","title":"read  <code>classmethod</code>","text":"<pre><code>read()\n</code></pre> <p>Uses the KamaeSparkPipelineReader class to read a pipeline from a persistent storage path.</p> <p>:returns: KamaeSparkPipelineReader object.</p> Source code in <code>src/kamae/spark/pipeline/pipeline.py</code> <pre><code>@classmethod\ndef read(cls) -&gt; \"KamaeSparkPipelineReader\":\n    \"\"\"\n    Uses the KamaeSparkPipelineReader class to read a pipeline from a\n    persistent storage path.\n\n    :returns: KamaeSparkPipelineReader object.\n    \"\"\"\n    return KamaeSparkPipelineReader(cls)\n</code></pre>"},{"location":"reference/src/kamae/spark/pipeline/pipeline/#src.kamae.spark.pipeline.pipeline.KamaeSparkPipeline.setParams","title":"setParams","text":"<pre><code>setParams(*, stages=None)\n</code></pre> <p>Sets the keyword arguments of the pipeline.</p> <p>:param stages: List of pipeline stages. :returns: KamaeSparkPipeline object with stages set.</p> Source code in <code>src/kamae/spark/pipeline/pipeline.py</code> <pre><code>@keyword_only\ndef setParams(\n    self, *, stages: Optional[\"KamaePipelineStage\"] = None\n) -&gt; \"KamaeSparkPipeline\":\n    \"\"\"\n    Sets the keyword arguments of the pipeline.\n\n    :param stages: List of pipeline stages.\n    :returns: KamaeSparkPipeline object with stages set.\n    \"\"\"\n    kwargs = self._input_kwargs\n    return self._set(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/pipeline/pipeline/#src.kamae.spark.pipeline.pipeline.KamaeSparkPipeline.setStages","title":"setStages","text":"<pre><code>setStages(value)\n</code></pre> <p>Sets the stages of the pipeline.</p> <p>:param value: List of pipeline stages. :returns: KamaeSparkPipeline object with stages set.</p> Source code in <code>src/kamae/spark/pipeline/pipeline.py</code> <pre><code>def setStages(self, value: List[\"KamaePipelineStage\"]) -&gt; \"KamaeSparkPipeline\":\n    \"\"\"\n    Sets the stages of the pipeline.\n\n    :param value: List of pipeline stages.\n    :returns: KamaeSparkPipeline object with stages set.\n    \"\"\"\n    return self._set(stages=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/pipeline/pipeline/#src.kamae.spark.pipeline.pipeline.KamaeSparkPipeline.write","title":"write","text":"<pre><code>write()\n</code></pre> <p>Uses the KamaeSparkPipelineWriter class to write the pipeline to a persistent storage path.</p> <p>:returns: KamaeSparkPipelineWriter object.</p> Source code in <code>src/kamae/spark/pipeline/pipeline.py</code> <pre><code>def write(self) -&gt; MLWriter:\n    \"\"\"\n    Uses the KamaeSparkPipelineWriter class to write the pipeline to a\n    persistent storage path.\n\n    :returns: KamaeSparkPipelineWriter object.\n    \"\"\"\n    return KamaeSparkPipelineWriter(self)\n</code></pre>"},{"location":"reference/src/kamae/spark/pipeline/pipeline/#src.kamae.spark.pipeline.pipeline.KamaeSparkPipelineReader","title":"KamaeSparkPipelineReader","text":"<pre><code>KamaeSparkPipelineReader(cls)\n</code></pre> <p>               Bases: <code>PipelineReader</code></p> <p>Util class for reading a pipeline from a persistent storage path.</p> Source code in <code>src/kamae/spark/pipeline/pipeline.py</code> <pre><code>def __init__(self, cls: Type[KamaeSparkPipeline]) -&gt; None:\n    super().__init__(cls=cls)\n</code></pre>"},{"location":"reference/src/kamae/spark/pipeline/pipeline/#src.kamae.spark.pipeline.pipeline.KamaeSparkPipelineReader.load","title":"load","text":"<pre><code>load(path)\n</code></pre> <p>Loads a pipeline from a given path.</p> <p>:param path: Path to stored pipeline. :returns: KamaeSparkPipeline object.</p> Source code in <code>src/kamae/spark/pipeline/pipeline.py</code> <pre><code>def load(self, path: str) -&gt; KamaeSparkPipeline:\n    \"\"\"\n    Loads a pipeline from a given path.\n\n    :param path: Path to stored pipeline.\n    :returns: KamaeSparkPipeline object.\n    \"\"\"\n    metadata = DefaultParamsReader.loadMetadata(path, self.sc)\n    uid, stages = PipelineSharedReadWrite.load(metadata, self.sc, path)\n    return KamaeSparkPipeline(stages=stages)._resetUid(uid)\n</code></pre>"},{"location":"reference/src/kamae/spark/pipeline/pipeline/#src.kamae.spark.pipeline.pipeline.KamaeSparkPipelineWriter","title":"KamaeSparkPipelineWriter","text":"<pre><code>KamaeSparkPipelineWriter(instance)\n</code></pre> <p>               Bases: <code>PipelineWriter</code></p> <p>Util class for writing a pipeline to a persistent storage path.</p> Source code in <code>src/kamae/spark/pipeline/pipeline.py</code> <pre><code>def __init__(self, instance: KamaeSparkPipeline) -&gt; None:\n    super().__init__(instance=instance)\n</code></pre>"},{"location":"reference/src/kamae/spark/pipeline/pipeline_model/","title":"pipeline_model","text":""},{"location":"reference/src/kamae/spark/pipeline/pipeline_model/#src.kamae.spark.pipeline.pipeline_model.KamaeSparkPipelineModel","title":"KamaeSparkPipelineModel","text":"<pre><code>KamaeSparkPipelineModel(stages)\n</code></pre> <p>               Bases: <code>PipelineModel</code></p> <p>KamaeSparkPipelineModel is a subclass of pyspark.ml.PipelineModel that is used to chain together LayerTransformers. It maintains the same functionality as pyspark.ml.PipelineModel e.g. serialisation.</p> <p>:param stages: List of LayerTransformers to chain together.</p> Source code in <code>src/kamae/spark/pipeline/pipeline_model.py</code> <pre><code>def __init__(self, stages: List[BaseTransformer]) -&gt; None:\n    \"\"\"\n    Initialises the KamaeSparkPipelineModel object.\n\n    :param stages: List of LayerTransformers to chain together.\n    \"\"\"\n    super().__init__(stages=stages)\n    self.stages = stages\n</code></pre>"},{"location":"reference/src/kamae/spark/pipeline/pipeline_model/#src.kamae.spark.pipeline.pipeline_model.KamaeSparkPipelineModel.build_keras_model","title":"build_keras_model","text":"<pre><code>build_keras_model(tf_input_schema, output_names=None)\n</code></pre> <p>Builds a keras model from the pipeline model using the PipelineGraph helper class.</p> <p>:param tf_input_schema: List of dictionaries containing the input schema for the model. Specifically the name, shape and dtype of each input. These will be passed as is to the Keras Input layer. :param output_names: Optional list of output names for the Keras model. If provided, only the outputs specified are used as model outputs. :returns: Keras model.</p> Source code in <code>src/kamae/spark/pipeline/pipeline_model.py</code> <pre><code>def build_keras_model(\n    self,\n    tf_input_schema: Union[List[tf.TypeSpec], List[Dict[str, Any]]],\n    output_names: Optional[List[str]] = None,\n) -&gt; tf.keras.Model:\n    \"\"\"\n    Builds a keras model from the pipeline model using the PipelineGraph\n    helper class.\n\n    :param tf_input_schema: List of dictionaries containing the input schema for\n    the model. Specifically the name, shape and dtype of each input.\n    These will be passed as is to the Keras Input layer.\n    :param output_names: Optional list of output names for the Keras model. If\n    provided, only the outputs specified are used as model outputs.\n    :returns: Keras model.\n    \"\"\"\n    stage_dict = {\n        stage.getOrDefault(\"layerName\"): stage.construct_layer_info()\n        for stage in self.expand_pipeline_stages()\n    }\n    pipeline_graph = PipelineGraph(stage_dict=stage_dict)\n    return pipeline_graph.build_keras_model(\n        tf_input_schema=tf_input_schema, output_names=output_names\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/pipeline/pipeline_model/#src.kamae.spark.pipeline.pipeline_model.KamaeSparkPipelineModel.copy","title":"copy","text":"<pre><code>copy(extra=None)\n</code></pre> <p>Creates a copy of the KamaeSparkPipelineModel object.</p> <p>:param extra: Additional optional params to copy to new pipeline model. :returns: KamaeSparkPipelineModel object.</p> Source code in <code>src/kamae/spark/pipeline/pipeline_model.py</code> <pre><code>def copy(self, extra: Optional[\"ParamMap\"] = None) -&gt; \"KamaeSparkPipelineModel\":\n    \"\"\"\n    Creates a copy of the KamaeSparkPipelineModel object.\n\n    :param extra: Additional optional params to copy to new pipeline model.\n    :returns: KamaeSparkPipelineModel object.\n    \"\"\"\n    if extra is None:\n        extra = dict()\n    stages = [stage.copy(extra) for stage in self.stages]\n    return KamaeSparkPipelineModel(stages)\n</code></pre>"},{"location":"reference/src/kamae/spark/pipeline/pipeline_model/#src.kamae.spark.pipeline.pipeline_model.KamaeSparkPipelineModel.expand_pipeline_stages","title":"expand_pipeline_stages","text":"<pre><code>expand_pipeline_stages()\n</code></pre> <p>Expands the pipeline stages to include all nested pipeline stages. If the pipeline stage is itself a pipeline model, it will be expanded recursively.</p> <p>:returns: List of all pipeline stages flattened to transformer level.</p> Source code in <code>src/kamae/spark/pipeline/pipeline_model.py</code> <pre><code>def expand_pipeline_stages(self) -&gt; List[BaseTransformer]:\n    \"\"\"\n    Expands the pipeline stages to include all nested pipeline stages.\n    If the pipeline stage is itself a pipeline model, it will be expanded\n    recursively.\n\n    :returns: List of all pipeline stages flattened to transformer level.\n    \"\"\"\n    expanded_stages = []\n    for stage in self.stages:\n        if isinstance(stage, KamaeSparkPipelineModel):\n            # Recursively expand the pipeline stages.\n            expanded_stages.extend(stage.expand_pipeline_stages())\n        else:\n            expanded_stages.append(stage)\n    return expanded_stages\n</code></pre>"},{"location":"reference/src/kamae/spark/pipeline/pipeline_model/#src.kamae.spark.pipeline.pipeline_model.KamaeSparkPipelineModel.get_all_tf_layers","title":"get_all_tf_layers","text":"<pre><code>get_all_tf_layers()\n</code></pre> <p>Gets a list of all tensorflow layers in the pipeline model.</p> <p>:returns: List of tensorflow layers within the pipeline model.</p> Source code in <code>src/kamae/spark/pipeline/pipeline_model.py</code> <pre><code>def get_all_tf_layers(self) -&gt; List[tf.keras.layers.Layer]:\n    \"\"\"\n    Gets a list of all tensorflow layers in the pipeline model.\n\n    :returns: List of tensorflow layers within the pipeline model.\n    \"\"\"\n    return [stage.get_tf_layer() for stage in self.stages]\n</code></pre>"},{"location":"reference/src/kamae/spark/pipeline/pipeline_model/#src.kamae.spark.pipeline.pipeline_model.KamaeSparkPipelineModel.get_keras_tuner_model_builder","title":"get_keras_tuner_model_builder","text":"<pre><code>get_keras_tuner_model_builder(\n    tf_input_schema, hp_dict, output_names=None\n)\n</code></pre> <p>Builds a keras tuner model builder (function) from the pipeline model using the PipelineGraph helper class.</p> <p>:param tf_input_schema: List of dictionaries containing the input schema for the model. Specifically the name, shape and dtype of each input. These will be passed as is to the Keras Input layer. :param hp_dict: Dictionary containing the hyperparameters for the model. :param output_names: Optional list of output names for the Keras model. If provided, only the outputs specified are used as model outputs. :returns: Keras tuner model builder (function).</p> Source code in <code>src/kamae/spark/pipeline/pipeline_model.py</code> <pre><code>def get_keras_tuner_model_builder(\n    self,\n    tf_input_schema: Union[List[tf.TypeSpec], List[Dict[str, Any]]],\n    hp_dict: Dict[str, List[Dict[str, Any]]],\n    output_names: Optional[List[str]] = None,\n) -&gt; Callable[[kt.HyperParameters], tf.keras.Model]:\n    \"\"\"\n    Builds a keras tuner model builder (function) from the pipeline model\n    using the PipelineGraph helper class.\n\n    :param tf_input_schema: List of dictionaries containing the input schema for\n    the model. Specifically the name, shape and dtype of each input.\n    These will be passed as is to the Keras Input layer.\n    :param hp_dict: Dictionary containing the hyperparameters for the model.\n    :param output_names: Optional list of output names for the Keras model. If\n    provided, only the outputs specified are used as model outputs.\n    :returns: Keras tuner model builder (function).\n    \"\"\"\n    stage_dict = {\n        stage.getOrDefault(\"layerName\"): stage.construct_layer_info()\n        for stage in self.stages\n    }\n    pipeline_graph = PipelineGraph(stage_dict=stage_dict)\n    return pipeline_graph.get_keras_tuner_model_builder(\n        tf_input_schema=tf_input_schema, hp_dict=hp_dict, output_names=output_names\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/pipeline/pipeline_model/#src.kamae.spark.pipeline.pipeline_model.KamaeSparkPipelineModel.read","title":"read  <code>classmethod</code>","text":"<pre><code>read()\n</code></pre> <p>Uses the KamaeSparkPipelineModelReader class to read a pipeline model from a persistent storage path.</p> <p>:returns: KamaeSparkPipelineModelReader object.</p> Source code in <code>src/kamae/spark/pipeline/pipeline_model.py</code> <pre><code>@classmethod\ndef read(cls) -&gt; \"KamaeSparkPipelineModelReader\":\n    \"\"\"\n    Uses the KamaeSparkPipelineModelReader class to read a pipeline model from a\n    persistent storage path.\n\n    :returns: KamaeSparkPipelineModelReader object.\n    \"\"\"\n    return KamaeSparkPipelineModelReader(cls)\n</code></pre>"},{"location":"reference/src/kamae/spark/pipeline/pipeline_model/#src.kamae.spark.pipeline.pipeline_model.KamaeSparkPipelineModel.write","title":"write","text":"<pre><code>write()\n</code></pre> <p>Uses the KamaeSparkPipelineModelWriter class to write the pipeline model to a persistent storage path.</p> <p>:returns: KamaeSparkPipelineModelWriter object.</p> Source code in <code>src/kamae/spark/pipeline/pipeline_model.py</code> <pre><code>def write(self) -&gt; MLWriter:\n    \"\"\"\n    Uses the KamaeSparkPipelineModelWriter class to write the pipeline model to a\n    persistent storage path.\n\n    :returns: KamaeSparkPipelineModelWriter object.\n    \"\"\"\n    return KamaeSparkPipelineModelWriter(self)\n</code></pre>"},{"location":"reference/src/kamae/spark/pipeline/pipeline_model/#src.kamae.spark.pipeline.pipeline_model.KamaeSparkPipelineModelReader","title":"KamaeSparkPipelineModelReader","text":"<pre><code>KamaeSparkPipelineModelReader(cls)\n</code></pre> <p>               Bases: <code>PipelineModelReader</code></p> <p>Util class for reading a pipeline model from a persistent storage path.</p> Source code in <code>src/kamae/spark/pipeline/pipeline_model.py</code> <pre><code>def __init__(self, cls: Type[\"KamaeSparkPipelineModel\"]) -&gt; None:\n    super().__init__(cls=cls)\n</code></pre>"},{"location":"reference/src/kamae/spark/pipeline/pipeline_model/#src.kamae.spark.pipeline.pipeline_model.KamaeSparkPipelineModelReader.load","title":"load","text":"<pre><code>load(path)\n</code></pre> <p>Loads a pipeline model from a given path.</p> <p>:param path: Path to stored pipeline model. :returns: KamaeSparkPipelineModel object.</p> Source code in <code>src/kamae/spark/pipeline/pipeline_model.py</code> <pre><code>def load(self, path: str) -&gt; \"KamaeSparkPipelineModel\":\n    \"\"\"\n    Loads a pipeline model from a given path.\n\n    :param path: Path to stored pipeline model.\n    :returns: KamaeSparkPipelineModel object.\n    \"\"\"\n    metadata = DefaultParamsReader.loadMetadata(path, self.sc)\n    uid, stages = PipelineSharedReadWrite.load(metadata, self.sc, path)\n    return KamaeSparkPipelineModel(\n        stages=cast(List[BaseTransformer], stages)\n    )._resetUid(uid)\n</code></pre>"},{"location":"reference/src/kamae/spark/pipeline/pipeline_model/#src.kamae.spark.pipeline.pipeline_model.KamaeSparkPipelineModelWriter","title":"KamaeSparkPipelineModelWriter","text":"<pre><code>KamaeSparkPipelineModelWriter(instance)\n</code></pre> <p>               Bases: <code>PipelineModelWriter</code></p> <p>Util class for writing a pipeline model to a persistent storage path.</p> Source code in <code>src/kamae/spark/pipeline/pipeline_model.py</code> <pre><code>def __init__(self, instance: \"KamaeSparkPipelineModel\") -&gt; None:\n    super().__init__(instance=instance)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/","title":"transformers","text":""},{"location":"reference/src/kamae/spark/transformers/absolute_value/","title":"absolute_value","text":""},{"location":"reference/src/kamae/spark/transformers/absolute_value/#src.kamae.spark.transformers.absolute_value.AbsoluteValueTransformer","title":"AbsoluteValueTransformer","text":"<pre><code>AbsoluteValueTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code></p> <p>Absolute value Spark Transformer for use in Spark pipelines. This transformer applies abs(x) operation to the input.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/absolute_value.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes an AbsoluteValueTransformer transformer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/absolute_value/#src.kamae.spark.transformers.absolute_value.AbsoluteValueTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/absolute_value/#src.kamae.spark.transformers.absolute_value.AbsoluteValueTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which applies abs(<code>inputCol</code>).</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/absolute_value.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which applies abs(`inputCol`).\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_data_type = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n    output_col = single_input_single_output_scalar_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_data_type,\n        func=lambda x: F.abs(x),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/absolute_value/#src.kamae.spark.transformers.absolute_value.AbsoluteValueTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the absolute value transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs an absolute value operation.</p> Source code in <code>src/kamae/spark/transformers/absolute_value.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the absolute value transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs an absolute value operation.\n    \"\"\"\n    return AbsoluteValueLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/array_concatenate/","title":"array_concatenate","text":""},{"location":"reference/src/kamae/spark/transformers/array_concatenate/#src.kamae.spark.transformers.array_concatenate.ArrayConcatenateTransformer","title":"ArrayConcatenateTransformer","text":"<pre><code>ArrayConcatenateTransformer(\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    autoBroadcast=False,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>MultiInputSingleOutputParams</code>, <code>AutoBroadcastParams</code></p> <p>ArrayConcatenate Spark Transformer for use in Spark pipelines. This transformer assembles multiple columns into a single array column.</p> <p>:param inputCols: List of input column names. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param autoBroadcast: If True, the Keras transformer will broadcast scalar inputs to the biggest rank. Default is False. WARNING: This modifies only the Keras layer, not the Spark transformer! :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/array_concatenate.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    autoBroadcast: Optional[bool] = False,\n) -&gt; None:\n    \"\"\"\n    Initialize a ArrayConcatenateTransformer transformer.\n\n    :param inputCols: List of input column names.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param autoBroadcast: If True, the Keras transformer will broadcast scalar\n    inputs to the biggest rank. Default is False.\n    WARNING: This modifies only the Keras layer, not the Spark transformer!\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(autoBroadcast=False)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/array_concatenate/#src.kamae.spark.transformers.array_concatenate.ArrayConcatenateTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/array_concatenate/#src.kamae.spark.transformers.array_concatenate.ArrayConcatenateTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transform the input dataset. Creates a new column named outputCol which is a concatenated array of all input columns.</p> <p>:param dataset: Pyspark DataFrame to transform. :returns: Transformed pyspark dataFrame.</p> Source code in <code>src/kamae/spark/transformers/array_concatenate.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transform the input dataset. Creates a new column named outputCol which is a\n    concatenated array of all input columns.\n\n    :param dataset: Pyspark DataFrame to transform.\n    :returns: Transformed pyspark dataFrame.\n    \"\"\"\n    input_col_names = self.getInputCols()\n    input_cols = [F.col(c) for c in input_col_names]\n    input_col_datatypes = [\n        self.get_column_datatype(dataset=dataset, column_name=c)\n        for c in input_col_names\n    ]\n\n    output_col = self.array_concatenate_transform(\n        input_cols=input_cols,\n        input_col_datatypes=input_col_datatypes,\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/array_concatenate/#src.kamae.spark.transformers.array_concatenate.ArrayConcatenateTransformer.array_concatenate_transform","title":"array_concatenate_transform  <code>staticmethod</code>","text":"<pre><code>array_concatenate_transform(\n    input_cols, input_col_datatypes\n)\n</code></pre> <p>Similar to the transform helpers in the utils module,but for the ArrayConcatenateTransformer. This function is specific to this transformer because whilst it still broadcasts scalars to arrays, we do not want to repeat the innermost array elements. Thus, each array can have different innermost size for concatenation. Other implementations perform elementwise computations across nested arrays and so require that all arrays are exactly the same shape.</p> <p>Caters for the case, where the input columns are:</p> <ol> <li>All scalars.</li> <li>All (possibly nested) arrays.</li> <li>A mix of scalars and (possibly nested) arrays.</li> </ol> <p>If all inputs are scalars, applies concat(array()) directly to the input columns. If inputs are a mixture of scalars and arrays, broadcasts any scalars to the size of the arrays in all N-1 dimensions, and creates an array of size 1 in the final Nth dimension. Then zips the arrays into a single nested array column, stopping before the innermost (Nth) array dimension. Finally, concats along this Nth dimension.</p> <p>Example:</p> <p>Input 1: 1.0 (scalar) Input 2: [             [[2.0, 3.0], [4.0, 5.0]],             [[6.0, 7.0], [8.0, 9.0]]         ] (nested 3D array) Input 3: 10.0 (scalar) Input 4: [             [[11.0, 12.0, 13.0], [14.0, 15.0, 16.0]],             [[17.0, 18.0, 19.0], [20.0, 21.0, 22.0]]         ] (nested 3D array)</p> <p>Input 1 &amp; 3 are broadcast to the size of the arrays up to N-1th dim, giving: Input 1: [             [[1.0], [1.0]],             [[1.0], [1.0]]         ] Input 3: [             [[10.0], [10.0]],             [[10.0], [10.0]]         ] Then zipped together to give: [     [         {             \"input1\": [1.0],             \"input2\": [2.0, 3.0],             \"input3\": [10.0],             \"input4\": [11.0, 12.0, 13.0]         },         {             \"input1\": [1.0],             \"input2\": [4.0, 5.0],             \"input3\": [10.0],             \"input4\": [14.0, 15.0, 16.0]         }     ],     [         {             \"input1\": [1.0],             \"input2\": [6.0, 7.0],             \"input3\": [10.0],             \"input4\": [17.0, 18.0, 19.0]         },         {             \"input1\": [1.0],             \"input2\": [8.0, 9.0],             \"input3\": [10.0],             \"input4\": [20.0, 21.0, 22.0]         }     ] ] Then concats along the Nth dimension to give final output: [     [         [1.0, 2.0, 3.0, 10.0, 11.0, 12.0, 13.0],         [1.0, 4.0, 5.0, 10.0, 14.0, 15.0, 16.0]     ],     [         [1.0, 6.0, 7.0, 10.0, 17.0, 18.0, 19.0],         [1.0, 8.0, 9.0, 10.0, 20.0, 21.0, 22.0]     ] ]</p> <p>:param input_cols: List of input columns. :param input_col_datatypes: List of input column datatypes. :returns: Output column.</p> Source code in <code>src/kamae/spark/transformers/array_concatenate.py</code> <pre><code>@staticmethod\ndef array_concatenate_transform(\n    input_cols: List[Column],\n    input_col_datatypes: List[DataType],\n) -&gt; Column:\n    \"\"\"\n    Similar to the transform helpers in the utils module,but for the\n    ArrayConcatenateTransformer. This function is specific to this transformer\n    because whilst it still broadcasts scalars to arrays, we do not want to repeat\n    the innermost array elements. Thus, each array can have different innermost size\n    for concatenation. Other implementations perform elementwise computations across\n    nested arrays and so require that all arrays are exactly the same shape.\n\n    Caters for the case, where the input columns are:\n\n    1. All scalars.\n    2. All (possibly nested) arrays.\n    3. A mix of scalars and (possibly nested) arrays.\n\n    If all inputs are scalars, applies concat(array()) directly to the input\n    columns. If inputs are a mixture of scalars and arrays, broadcasts any scalars\n    to the size of the arrays in all N-1 dimensions, and creates an array of size 1\n    in the final Nth dimension. Then zips the arrays into a single nested array\n    column, stopping before the innermost (Nth) array dimension. Finally, concats\n    along this Nth dimension.\n\n    Example:\n\n    Input 1: 1.0 (scalar)\n    Input 2: [\n                [[2.0, 3.0], [4.0, 5.0]],\n                [[6.0, 7.0], [8.0, 9.0]]\n            ] (nested 3D array)\n    Input 3: 10.0 (scalar)\n    Input 4: [\n                [[11.0, 12.0, 13.0], [14.0, 15.0, 16.0]],\n                [[17.0, 18.0, 19.0], [20.0, 21.0, 22.0]]\n            ] (nested 3D array)\n\n    Input 1 &amp; 3 are broadcast to the size of the arrays up to N-1th dim, giving:\n    Input 1: [\n                [[1.0], [1.0]],\n                [[1.0], [1.0]]\n            ]\n    Input 3: [\n                [[10.0], [10.0]],\n                [[10.0], [10.0]]\n            ]\n    Then zipped together to give:\n    [\n        [\n            {\n                \"input1\": [1.0],\n                \"input2\": [2.0, 3.0],\n                \"input3\": [10.0],\n                \"input4\": [11.0, 12.0, 13.0]\n            },\n            {\n                \"input1\": [1.0],\n                \"input2\": [4.0, 5.0],\n                \"input3\": [10.0],\n                \"input4\": [14.0, 15.0, 16.0]\n            }\n        ],\n        [\n            {\n                \"input1\": [1.0],\n                \"input2\": [6.0, 7.0],\n                \"input3\": [10.0],\n                \"input4\": [17.0, 18.0, 19.0]\n            },\n            {\n                \"input1\": [1.0],\n                \"input2\": [8.0, 9.0],\n                \"input3\": [10.0],\n                \"input4\": [20.0, 21.0, 22.0]\n            }\n        ]\n    ]\n    Then concats along the Nth dimension to give final output:\n    [\n        [\n            [1.0, 2.0, 3.0, 10.0, 11.0, 12.0, 13.0],\n            [1.0, 4.0, 5.0, 10.0, 14.0, 15.0, 16.0]\n        ],\n        [\n            [1.0, 6.0, 7.0, 10.0, 17.0, 18.0, 19.0],\n            [1.0, 8.0, 9.0, 10.0, 20.0, 21.0, 22.0]\n        ]\n    ]\n\n    :param input_cols: List of input columns.\n    :param input_col_datatypes: List of input column datatypes.\n    :returns: Output column.\n    \"\"\"\n    input_col_names = [f\"input{i}\" for i in range(len(input_cols))]\n    if all(\n        [not isinstance(datatype, ArrayType) for datatype in input_col_datatypes]\n    ):\n        # All inputs are scalars.\n        # Apply the concat, wrapping each scalar in an array.\n        return F.concat(*[F.array(x) for x in input_cols])\n\n    if all([isinstance(datatype, ArrayType) for datatype in input_col_datatypes]):\n        # All inputs are arrays. Zip the arrays together into a single column.\n        nesting_level = get_array_nesting_level(column_dtype=input_col_datatypes[0])\n        zipped_array_column = nested_arrays_zip(\n            columns=input_cols,\n            nest_level=nesting_level - 1,\n            column_names=input_col_names,\n        )\n    else:\n        # Inputs are a mix of scalars and arrays.\n        # Broadcast the scalars to the size of the arrays.\n        scalar_columns = [\n            (idx, col_w_datatype[0])\n            for idx, col_w_datatype in enumerate(\n                zip(input_cols, input_col_datatypes)\n            )\n            if not isinstance(col_w_datatype[1], ArrayType)\n        ]\n        array_columns_w_types = [\n            (idx, col_w_datatype[0], col_w_datatype[1])\n            for idx, col_w_datatype in enumerate(\n                zip(input_cols, input_col_datatypes)\n            )\n            if isinstance(col_w_datatype[1], ArrayType)\n        ]\n        # Broadcast the scalar to the size of the arrays. Use the first array column\n        # to determine the size of the broadcasted scalar. Assumes all arrays are\n        # of the same size in the N-1 dimensions\n        broadcasted_scalars = [\n            (\n                idx,\n                broadcast_scalar_column_to_array_with_inner_singleton_array(\n                    scalar_column=scalar_column,\n                    array_column=array_columns_w_types[0][1],\n                    array_column_datatype=array_columns_w_types[0][2],\n                ),\n            )\n            for idx, scalar_column in scalar_columns\n        ]\n        # Resort the array columns and the broadcasted scalars,\n        # so they match the order of the input columns.\n        columns_w_idx = [\n            (idx, column) for idx, column, _ in array_columns_w_types\n        ] + broadcasted_scalars\n        sorted_columns = [\n            column for idx, column in sorted(columns_w_idx, key=lambda x: x[0])\n        ]\n        nesting_level = get_array_nesting_level(\n            column_dtype=array_columns_w_types[0][2]\n        )\n        zipped_array_column = nested_arrays_zip(\n            columns=sorted_columns,\n            nest_level=nesting_level - 1,\n            column_names=input_col_names,\n        )\n\n    # Create the nested transform function that applies the function to the zipped\n    # array column.\n    nested_func = nested_transform(\n        func=lambda x: F.concat(*[x[c] for c in input_col_names]),\n        nest_level=nesting_level - 1,\n    )\n    return nested_func(zipped_array_column)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/array_concatenate/#src.kamae.spark.transformers.array_concatenate.ArrayConcatenateTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer that concatneates the input tensors.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that concatenates the input tensors.</p> Source code in <code>src/kamae/spark/transformers/array_concatenate.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer that concatneates the input tensors.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter\n    that concatenates the input tensors.\n    \"\"\"\n    return ArrayConcatenateLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        axis=-1,\n        auto_broadcast=self.getAutoBroadcast(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/array_crop/","title":"array_crop","text":""},{"location":"reference/src/kamae/spark/transformers/array_crop/#src.kamae.spark.transformers.array_crop.ArrayCropParams","title":"ArrayCropParams","text":"<p>               Bases: <code>PadValueParams</code></p> <p>Mixin class containing pad value parameters needed for array crop transformers.</p>"},{"location":"reference/src/kamae/spark/transformers/array_crop/#src.kamae.spark.transformers.array_crop.ArrayCropParams.getArrayLength","title":"getArrayLength","text":"<pre><code>getArrayLength()\n</code></pre> <p>Gets the array length parameter. :returns: array length.</p> Source code in <code>src/kamae/spark/transformers/array_crop.py</code> <pre><code>def getArrayLength(self) -&gt; int:\n    \"\"\"\n    Gets the array length parameter.\n    :returns: array length.\n    \"\"\"\n    return self.getOrDefault(self.arrayLength)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/array_crop/#src.kamae.spark.transformers.array_crop.ArrayCropParams.setArrayLength","title":"setArrayLength","text":"<pre><code>setArrayLength(value)\n</code></pre> <p>Sets the parameter array length to the given value. :param value: array length. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/array_crop.py</code> <pre><code>def setArrayLength(self, value: int) -&gt; \"ArrayCropParams\":\n    \"\"\"\n    Sets the parameter array length to the given value.\n    :param value: array length.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if value &lt; 1:\n        raise ValueError(\"Array length must be greater than 0.\")\n    return self._set(arrayLength=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/array_crop/#src.kamae.spark.transformers.array_crop.ArrayCropTransformer","title":"ArrayCropTransformer","text":"<pre><code>ArrayCropTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    arrayLength=128,\n    padValue=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>ArrayCropParams</code></p> <p>Transformer that reshapes arrays into consistent shapes by either cropping or padding.</p> <p>If the tensor is shorter than the specified length, it is padded with specified pad value.</p> <p>:param outputCol: Output column name. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer :param arrayLength: The length to crop or pad the arrays to. Defaults to 128. :param padValue: The value pad the arrays with. Defaults to <code>None</code>. :returns: None</p> Source code in <code>src/kamae/spark/transformers/array_crop.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[Union[str, int, float]] = None,\n    outputDtype: Optional[Union[str, int, float]] = None,\n    layerName: Optional[str] = None,\n    arrayLength: Optional[int] = 128,\n    padValue: Optional[Union[str, int, float]] = None,\n) -&gt; None:\n    \"\"\"\n    Initialises the ArrayCropTransformer\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    :param arrayLength: The length to crop or pad the arrays to. Defaults to 128.\n    :param padValue: The value pad the arrays with. Defaults to `None`.\n    :returns: None\n    \"\"\"\n    super().__init__()\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n    self._pad_type_to_valid_element_types = {\n        \"int\": [\"int\", \"bigint\", \"smallint\"],\n        \"float\": [\"float\", \"double\", \"decimal(10,0)\"],\n        \"string\": [\"string\"],\n        \"boolean\": [\"boolean\"],\n    }\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/array_crop/#src.kamae.spark.transformers.array_crop.ArrayCropTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/array_crop/#src.kamae.spark.transformers.array_crop.ArrayCropTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Performs the cropping and/or padding on the input dataset. Example, crop to length 3, with value '-1':</p> <p>dataset = spark.Dataframe(     [         ['a', 'a', 'a', 'b', 'c'],         ['x', 'z', 'y'],         ['a', 'b',],         ['a', 'x', 'a', 'b',],         []     ],     'input_col'  )  Output: spark.Dataframe(     [         ['a', 'a', 'a', 'b', 'c'],         ['x', 'z', 'y'],         ['a', 'b',],         ['a', 'x', 'a', 'b',],         []     ],     [         ['a', 'a', 'a'],         ['x', 'z', 'y'],         ['a', 'b', '-1'],         ['a', 'x', 'a'],         ['-1', '-1', '-1']     ],     'input_col', 'output_col' ) :param dataset: The input dataframe. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/array_crop.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Performs the cropping and/or padding on the input dataset.\n    Example, crop to length 3, with value '-1':\n\n     dataset = spark.Dataframe(\n        [\n            ['a', 'a', 'a', 'b', 'c'],\n            ['x', 'z', 'y'],\n            ['a', 'b',],\n            ['a', 'x', 'a', 'b',],\n            []\n        ],\n        'input_col'\n     )\n     Output: spark.Dataframe(\n        [\n            ['a', 'a', 'a', 'b', 'c'],\n            ['x', 'z', 'y'],\n            ['a', 'b',],\n            ['a', 'x', 'a', 'b',],\n            []\n        ],\n        [\n            ['a', 'a', 'a'],\n            ['x', 'z', 'y'],\n            ['a', 'b', '-1'],\n            ['a', 'x', 'a'],\n            ['-1', '-1', '-1']\n        ],\n        'input_col', 'output_col'\n    )\n    :param dataset: The input dataframe.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    pad_value_spark_type = self._get_pad_value_type(self.getPadValue())\n    input_col_type = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n    input_col_element_type = get_array_nesting_level_and_element_dtype(\n        input_col_type\n    )[1]\n\n    if (\n        input_col_element_type.simpleString()\n        not in self._pad_type_to_valid_element_types[\n            pad_value_spark_type.simpleString()\n        ]\n    ):\n        raise ValueError(\n            f\"\"\"\n        The pad value type '{type(pad_value_spark_type)}' does\n        not match the element type of the input\n        column '{type(input_col_element_type)}'.\n        \"\"\"\n        )\n\n    output_col = single_input_single_output_array_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_col_type,\n        func=lambda x: F.concat(\n            F.slice(x, 1, self.getArrayLength()),\n            F.array_repeat(\n                F.lit(self.getPadValue()),\n                self.getArrayLength() - F.size(x),\n            ),\n        ),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/array_crop/#src.kamae.spark.transformers.array_crop.ArrayCropTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer that performs the array cropping and padding.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that performs the array cropping and padding operation.</p> Source code in <code>src/kamae/spark/transformers/array_crop.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer that performs the array cropping and padding.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter\n    that performs the array cropping and padding operation.\n    \"\"\"\n    return ArrayCropLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        array_length=self.getArrayLength(),\n        pad_value=self.getPadValue(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/array_split/","title":"array_split","text":""},{"location":"reference/src/kamae/spark/transformers/array_split/#src.kamae.spark.transformers.array_split.ArraySplitTransformer","title":"ArraySplitTransformer","text":"<pre><code>ArraySplitTransformer(\n    inputCol=None,\n    outputCols=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputMultiOutputParams</code></p> <p>ArraySplit Spark Transformer for use in Spark pipelines. This transformer splits an array column into multiple columns.</p> <p>:param inputCol: Input column name. :param outputCols: Output column names. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column(s) to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/array_split.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCols: Optional[List[str]] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize a ArraySplitTransformer transformer.\n\n    :param inputCol: Input column name.\n    :param outputCols: Output column names.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column(s) to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/array_split/#src.kamae.spark.transformers.array_split.ArraySplitTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/array_split/#src.kamae.spark.transformers.array_split.ArraySplitTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column for each output column equal to the value of the input column at the given index.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/array_split.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column for each output column equal\n    to the value of the input column at the given index.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n    output_cols = []\n    for i, column in enumerate(self.getOutputCols()):\n        output_col = single_input_single_output_array_transform(\n            input_col=F.col(self.getInputCol()),\n            input_col_datatype=input_datatype,\n            func=lambda x: F.element_at(x, i + 1),\n        )\n        output_cols.append(output_col.alias(self.getOutputCols()[i]))\n    original_columns = [F.col(c) for c in dataset.columns]\n    select_cols = original_columns + output_cols\n    return dataset.select(select_cols)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/array_split/#src.kamae.spark.transformers.array_split.ArraySplitTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for that unstacks the input tensor and reshapes to the original shape.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that slices the input tensors.</p> Source code in <code>src/kamae/spark/transformers/array_split.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for that unstacks the input tensor and reshapes\n    to the original shape.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter\n    that slices the input tensors.\n    \"\"\"\n    return ArraySplitLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        axis=-1,\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/array_subtract_minimum/","title":"array_subtract_minimum","text":""},{"location":"reference/src/kamae/spark/transformers/array_subtract_minimum/#src.kamae.spark.transformers.array_subtract_minimum.ArraySubtractMinimumParams","title":"ArraySubtractMinimumParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing pad value parameters needed for array subtract min transformers.</p>"},{"location":"reference/src/kamae/spark/transformers/array_subtract_minimum/#src.kamae.spark.transformers.array_subtract_minimum.ArraySubtractMinimumParams.getPadValue","title":"getPadValue","text":"<pre><code>getPadValue()\n</code></pre> <p>Gets the pad value parameter.</p> <p>:returns: float pad value.</p> Source code in <code>src/kamae/spark/transformers/array_subtract_minimum.py</code> <pre><code>def getPadValue(self) -&gt; float:\n    \"\"\"\n    Gets the pad value parameter.\n\n    :returns: float pad value.\n    \"\"\"\n    return self.getOrDefault(self.padValue)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/array_subtract_minimum/#src.kamae.spark.transformers.array_subtract_minimum.ArraySubtractMinimumParams.setPadValue","title":"setPadValue","text":"<pre><code>setPadValue(value)\n</code></pre> <p>Sets the parameter pad value to the given value.</p> <p>:param value: pad value. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/array_subtract_minimum.py</code> <pre><code>def setPadValue(self, value: float) -&gt; \"ArraySubtractMinimumParams\":\n    \"\"\"\n    Sets the parameter pad value to the given value.\n\n    :param value: pad value.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(padValue=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/array_subtract_minimum/#src.kamae.spark.transformers.array_subtract_minimum.ArraySubtractMinimumTransformer","title":"ArraySubtractMinimumTransformer","text":"<pre><code>ArraySubtractMinimumTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    padValue=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>ArraySubtractMinimumParams</code></p> <p>ArraySubtractMinimumTransformer that computes the difference within an array from the minimum non-paded element in the input tensor. The calculation preserves the pad value elements.</p> <p>The main use case in mind for this is working with an array of timestamps.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer :param padValue: The value to be considered as padding. Defaults to <code>None</code>. :returns: None</p> Source code in <code>src/kamae/spark/transformers/array_subtract_minimum.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    padValue: Optional[float] = None,\n) -&gt; None:\n    \"\"\"\n    Initialise the ArraySubtractMinimumTransformer\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    :param padValue: The value to be considered as padding. Defaults to `None`.\n    :returns: None\n    \"\"\"\n    super().__init__()\n    self._setDefault(padValue=None)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/array_subtract_minimum/#src.kamae.spark.transformers.array_subtract_minimum.ArraySubtractMinimumTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/array_subtract_minimum/#src.kamae.spark.transformers.array_subtract_minimum.ArraySubtractMinimumTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Performs the calculation of the differences on the input dataset.</p> Example <p>dataset = spark.Dataframe(    [        [19, 18, 13, 11, 10, -1, -1, -1],        [12, 2, 1, -1, -1, -1, -1, -1],    ],    'input_col' ) Output: spark.Dataframe(    [        [19, 18, 13, 11, 10, -1, -1, -1],        [12, 2, 1, -1, -1, -1, -1, -1],    ],    [        [9, 8, 3, 1, 0, -1, -1, -1],        [11, 1, 0, -1, -1, -1, -1, -1],    ],    'input_col', 'output_col'</p> <p>)</p> <p>:param dataset: The input dataframe. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/array_subtract_minimum.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Performs the calculation of the differences on the input dataset.\n\n    Example:\n     dataset = spark.Dataframe(\n        [\n            [19, 18, 13, 11, 10, -1, -1, -1],\n            [12, 2, 1, -1, -1, -1, -1, -1],\n        ],\n        'input_col'\n     )\n     Output: spark.Dataframe(\n        [\n            [19, 18, 13, 11, 10, -1, -1, -1],\n            [12, 2, 1, -1, -1, -1, -1, -1],\n        ],\n        [\n            [9, 8, 3, 1, 0, -1, -1, -1],\n            [11, 1, 0, -1, -1, -1, -1, -1],\n        ],\n        'input_col', 'output_col'\n    )\n\n    :param dataset: The input dataframe.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_column_type = self.get_column_datatype(dataset, self.getInputCol())\n    if not isinstance(input_column_type, ArrayType):\n        raise ValueError(\n            f\"\"\"Input column {self.getInputCol()} must be of ArrayType.\n            Got {input_column_type} instead.\"\"\"\n        )\n    padded_value = self.getPadValue()\n\n    def array_subtract_min(x: Column, pad_value: Optional[float]) -&gt; Column:\n        if pad_value is None:\n            return F.transform(x, lambda y: y - F.array_min(x))\n        else:\n            return F.transform(\n                x,\n                lambda y: F.when(\n                    y != F.lit(pad_value),\n                    y - F.array_min(F.filter(x, lambda z: z != F.lit(pad_value))),\n                ).otherwise(y),\n            )\n\n    array_subtract = single_input_single_output_array_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_column_type,\n        func=lambda x: array_subtract_min(x, padded_value),\n    )\n    return dataset.withColumn(self.getOutputCol(), array_subtract)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/array_subtract_minimum/#src.kamae.spark.transformers.array_subtract_minimum.ArraySubtractMinimumTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the sequential difference transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that performs the sequential difference operation.</p> Source code in <code>src/kamae/spark/transformers/array_subtract_minimum.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the sequential difference transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n    performs the sequential difference operation.\n    \"\"\"\n    return ArraySubtractMinimumLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        pad_value=self.getPadValue(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/base/","title":"base","text":""},{"location":"reference/src/kamae/spark/transformers/base/#src.kamae.spark.transformers.base.BaseTransformer","title":"BaseTransformer","text":"<pre><code>BaseTransformer()\n</code></pre> <p>               Bases: <code>Transformer</code>, <code>SparkOperation</code></p> <p>Abstract class for all transformers.</p> Source code in <code>src/kamae/spark/transformers/base.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"\n    Initializes the transformer.\n    \"\"\"\n    super().__init__()\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/base/#src.kamae.spark.transformers.base.BaseTransformer.construct_layer_info","title":"construct_layer_info","text":"<pre><code>construct_layer_info()\n</code></pre> <p>Constructs the layer info dictionary. Contains the layer name, the tensorflow layer, and the inputs and outputs. This is used when constructing the pipeline graph.</p> <p>:returns: Dictionary containing layer information such as name, tensorflow layer, inputs, and outputs.</p> Source code in <code>src/kamae/spark/transformers/base.py</code> <pre><code>def construct_layer_info(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Constructs the layer info dictionary.\n    Contains the layer name, the tensorflow layer, and the inputs and outputs.\n    This is used when constructing the pipeline graph.\n\n    :returns: Dictionary containing layer information such as\n    name, tensorflow layer, inputs, and outputs.\n    \"\"\"\n    inputs, outputs = self.get_layer_inputs_outputs()\n    return {\n        \"name\": self.getOrDefault(\"layerName\"),\n        \"layer\": self.get_tf_layer(),\n        \"inputs\": inputs,\n        \"outputs\": outputs,\n    }\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/base/#src.kamae.spark.transformers.base.BaseTransformer.get_tf_layer","title":"get_tf_layer  <code>abstractmethod</code>","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer to be used in the model. This is the only abstract method that must be implemented. :returns: Tensorflow Layer</p> Source code in <code>src/kamae/spark/transformers/base.py</code> <pre><code>@abstractmethod\ndef get_tf_layer(self) -&gt; Union[tf.keras.layers.Layer, List[tf.keras.layers.Layer]]:\n    \"\"\"\n    Gets the tensorflow layer to be used in the model.\n    This is the only abstract method that must be implemented.\n    :returns: Tensorflow Layer\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/base/#src.kamae.spark.transformers.base.BaseTransformer.transform","title":"transform","text":"<pre><code>transform(dataset, params=None)\n</code></pre> <p>Overrides the transform method of the parent class to add casting of input and output columns to the preferred data type.</p> <p>:param dataset: Input dataset. :param params: Optional additional parameters. :returns: Transformed dataset.</p> Source code in <code>src/kamae/spark/transformers/base.py</code> <pre><code>def transform(\n    self, dataset: DataFrame, params: Optional[\"ParamMap\"] = None\n) -&gt; DataFrame:\n    \"\"\"\n    Overrides the transform method of the parent class to add casting of input and\n    output columns to the preferred data type.\n\n    :param dataset: Input dataset.\n    :param params: Optional additional parameters.\n    :returns: Transformed dataset.\n    \"\"\"\n    try:\n        dataset = self._create_casted_input_output_columns(\n            dataset=dataset, ingress=True\n        )\n        self._check_input_dtypes_compatible(\n            dataset, self._get_single_or_multi_col(ingress=True)\n        )\n\n        # Set transformer input columns to casted columns\n        self.set_input_columns_to_from_casted(\n            dataset=dataset,\n            suffix=self.tmp_column_suffix,\n        )\n\n        # Call the super transform method\n        transformed_dataset = super().transform(dataset=dataset, params=params)\n\n        # Reset the transformer input columns from casted columns\n        self.set_input_columns_to_from_casted(\n            dataset=dataset,\n            suffix=self.tmp_column_suffix,\n            reverse=True,\n        )\n\n        # Drop the temporary casted columns\n        transformed_dataset = self.drop_tmp_casted_input_columns(\n            transformed_dataset\n        )\n\n        transformed_dataset = self._create_casted_input_output_columns(\n            dataset=transformed_dataset, ingress=False\n        )\n        return transformed_dataset\n    except Exception as e:\n        param_dict = {\n            param[0].name: param[1] for param in self.extractParamMap().items()\n        }\n        raise e.__class__(\n            f\"Error in transformer: {self.uid} with params: {param_dict}\"\n        ).with_traceback(e.__traceback__)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bearing_angle/","title":"bearing_angle","text":""},{"location":"reference/src/kamae/spark/transformers/bearing_angle/#src.kamae.spark.transformers.bearing_angle.BearingAngleParams","title":"BearingAngleParams","text":"<p>               Bases: <code>LatLonConstantParams</code>, <code>MultiInputSingleOutputParams</code></p> <p>Mixin class setting input cols.</p>"},{"location":"reference/src/kamae/spark/transformers/bearing_angle/#src.kamae.spark.transformers.bearing_angle.BearingAngleParams.setInputCols","title":"setInputCols","text":"<pre><code>setInputCols(value)\n</code></pre> <p>Overrides setting the input columns for the transformer. Throws an error if we do not have either two or four input columns depending on whether latLonConstant is provided. :param value: List of input columns. :returns: Class instance.</p> Source code in <code>src/kamae/spark/transformers/bearing_angle.py</code> <pre><code>def setInputCols(self, value: List[str]) -&gt; \"BearingAngleParams\":\n    \"\"\"\n    Overrides setting the input columns for the transformer.\n    Throws an error if we do not have either two or four input columns depending\n    on whether latLonConstant is provided.\n    :param value: List of input columns.\n    :returns: Class instance.\n    \"\"\"\n    if self.getLatLonConstant() is not None and len(value) != 2:\n        raise ValueError(\n            \"\"\"When setting inputCols for HaversineDistanceTransformer,\n            if the latLonConstant is not None,\n            there must be exactly two input columns.\"\"\"\n        )\n    elif len(value) not in [2, 4]:\n        raise ValueError(\n            \"\"\"When setting inputCols for HaversineDistanceTransformer,\n            there must be either two or four input columns.\"\"\"\n        )\n    return self._set(inputCols=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bearing_angle/#src.kamae.spark.transformers.bearing_angle.BearingAngleTransformer","title":"BearingAngleTransformer","text":"<pre><code>BearingAngleTransformer(\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    latLonConstant=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>BearingAngleParams</code></p> <p>Bearing Angle Calculator Spark Transformer for use in Spark pipelines. This transformer computes the bearing angle between two lat/lon pairs. This can be between four columns (one for each lat/lon) or between two columns and a constant. The transformer will return null angle if any of the lat/lon values are out of bounds. For lat, this is [-90, 90] and for lon, this is [-180, 180].</p> <p>input columns are required. These must be in the order [lat, lon]. If latLonConstant is not provided, then four input columns are required. These must be in the order [lat1, lon1, lat2, lon2]. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param latLonConstant: Optional list of lat/lon constant to use. Must be in the order [lat, lon]. If not provided, then four input columns are required. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/bearing_angle.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    latLonConstant: Optional[List[float]] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes a BearingAngle transformer.\n    :param inputCols: Input column names. If latLonConstant is provided, then two\n    input columns are required. These must be in the order [lat, lon].\n    If latLonConstant is not provided, then four input columns are required.\n    These must be in the order [lat1, lon1, lat2, lon2].\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param latLonConstant: Optional list of lat/lon constant to use.\n    Must be in the order [lat, lon].\n    If not provided, then four input columns are required.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(latLonConstant=None)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bearing_angle/#src.kamae.spark.transformers.bearing_angle.BearingAngleTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None. :returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/bearing_angle/#src.kamae.spark.transformers.bearing_angle.BearingAngleTransformer._get_input_cols","title":"_get_input_cols","text":"<pre><code>_get_input_cols()\n</code></pre> <p>Gets the input columns as a list of pyspark.sql.Column. Also checks if the lat and long are out of bounds and if so casts them to null. This will make the output null. If the latLonConstant is provided, then returns the input columns and the constant split into lat and lon. Otherwise, returns the input columns. :returns: List of input columns.</p> Source code in <code>src/kamae/spark/transformers/bearing_angle.py</code> <pre><code>def _get_input_cols(self) -&gt; List[Column]:\n    \"\"\"\n    Gets the input columns as a list of pyspark.sql.Column. Also checks if the\n    lat and long are out of bounds and if so casts them to null.\n    This will make the output null.\n    If the latLonConstant is provided, then returns the input columns and the\n    constant split into lat and lon. Otherwise, returns the input columns.\n    :returns: List of input columns.\n    \"\"\"\n    input_cols = self.getInputCols()\n\n    lat_lon_constant = self.getLatLonConstant()\n\n    if lat_lon_constant is not None:\n        return [\n            F.col(input_cols[0]).alias(self.uid + \"_lat1\"),\n            F.col(input_cols[1]).alias(self.uid + \"_lon1\"),\n            F.lit(lat_lon_constant[0]).alias(self.uid + \"_lat2\"),\n            F.lit(lat_lon_constant[1]).alias(self.uid + \"_lon2\"),\n        ]\n    else:\n        return [\n            F.col(input_cols[0]).alias(self.uid + \"_lat1\"),\n            F.col(input_cols[1]).alias(self.uid + \"_lon1\"),\n            F.col(input_cols[2]).alias(self.uid + \"_lat2\"),\n            F.col(input_cols[3]).alias(self.uid + \"_lon2\"),\n        ]\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bearing_angle/#src.kamae.spark.transformers.bearing_angle.BearingAngleTransformer._to_radians_col","title":"_to_radians_col  <code>staticmethod</code>","text":"<pre><code>_to_radians_col(x)\n</code></pre> <p>Converts a column of degrees to radians. :param x: Column of degrees. :returns: Column of radians in double precision.</p> Source code in <code>src/kamae/spark/transformers/bearing_angle.py</code> <pre><code>@staticmethod\ndef _to_radians_col(x: Column) -&gt; Column:\n    \"\"\"\n    Converts a column of degrees to radians.\n    :param x: Column of degrees.\n    :returns: Column of radians in double precision.\n    \"\"\"\n    return x.cast(DoubleType()) * F.lit(math.pi / 180)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bearing_angle/#src.kamae.spark.transformers.bearing_angle.BearingAngleTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which is the bearing angle between the input lat/lon columns. Returns null if any of the lat/lon values are out of bounds. :param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/bearing_angle.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which is the bearing angle between the input lat/lon columns.\n    Returns null if any of the lat/lon values are out of bounds.\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_cols = self._get_input_cols()\n    # input_cols can contain either actual columns or lit(constants). In order to\n    # determine the datatype of the input columns, we select them from the dataset\n    # first.\n    input_col_names = dataset.select(input_cols).columns\n    input_col_datatypes = [\n        self.get_column_datatype(dataset=dataset.select(input_cols), column_name=c)\n        for c in input_col_names\n    ]\n\n    def bearing_calculate_transform(\n        x: Column, input_col_names: List[str]\n    ) -&gt; Column:\n        lat1_radians = self._to_radians_col(\n            self.validate_lat_lon_column(x[input_col_names[0]])\n        )\n        lon1_radians = self._to_radians_col(\n            self.validate_lat_lon_column(x[input_col_names[1]], lat=False)\n        )\n        lat2_radians = self._to_radians_col(\n            self.validate_lat_lon_column(x[input_col_names[2]])\n        )\n        lon2_radians = self._to_radians_col(\n            self.validate_lat_lon_column(x[input_col_names[3]], lat=False)\n        )\n\n        lon_difference = lon2_radians - lon1_radians\n        # Bearing formula calculation\n        y = F.sin(lon_difference) * F.cos(lat2_radians)\n\n        x = F.cos(lat1_radians) * F.sin(lat2_radians)\n        x -= F.sin(lat1_radians) * F.cos(lat2_radians) * F.cos(lon_difference)\n\n        # Calculate bearing in degrees\n        bearing = F.atan2(y, x)\n        bearing_deg = F.pmod(F.degrees(bearing) + 360, 360)\n        return bearing_deg\n\n    output_col = multi_input_single_output_scalar_transform(\n        input_cols=input_cols,\n        input_col_names=input_col_names,\n        input_col_datatypes=input_col_datatypes,\n        func=lambda x: bearing_calculate_transform(x, input_col_names),\n    )\n\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bearing_angle/#src.kamae.spark.transformers.bearing_angle.BearingAngleTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the bearing angle transformer. :returns: Tensorflow keras layer with name equal to the layerName parameter that  computes the bearing angle between two lat/lon pairs.</p> Source code in <code>src/kamae/spark/transformers/bearing_angle.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the bearing angle transformer.\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     computes the bearing angle between two lat/lon pairs.\n    \"\"\"\n    return BearingAngleLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        lat_lon_constant=self.getLatLonConstant(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bearing_angle/#src.kamae.spark.transformers.bearing_angle.BearingAngleTransformer.validate_lat_lon_column","title":"validate_lat_lon_column  <code>staticmethod</code>","text":"<pre><code>validate_lat_lon_column(x, lat=True)\n</code></pre> <p>Validates that the input column is a valid lat or lon column. If not, then casts the column to null. :param x: Input column. :param lat: Whether the column is a lat column or not. :returns: Column.</p> Source code in <code>src/kamae/spark/transformers/bearing_angle.py</code> <pre><code>@staticmethod\ndef validate_lat_lon_column(x: Column, lat: bool = True) -&gt; Column:\n    \"\"\"\n    Validates that the input column is a valid lat or lon column.\n    If not, then casts the column to null.\n    :param x: Input column.\n    :param lat: Whether the column is a lat column or not.\n    :returns: Column.\n    \"\"\"\n    cond = x.between(-90.0, 90.0) if lat else x.between(-180.0, 180.0)\n    return F.when(cond, x)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bin/","title":"bin","text":""},{"location":"reference/src/kamae/spark/transformers/bin/#src.kamae.spark.transformers.bin.BinParams","title":"BinParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing parameters needed for Bin transform layers.</p>"},{"location":"reference/src/kamae/spark/transformers/bin/#src.kamae.spark.transformers.bin.BinParams._check_params_size","title":"_check_params_size","text":"<pre><code>_check_params_size(param_name, param_value)\n</code></pre> <p>Checks that the length of the given parameter is the same as the length of the other parameters.</p> <p>Used to ensure that the parameters are consistent with each other.</p> <p>:param param_name: Name of the parameter to check. :param param_value: Value of the parameter to check. :returns: None :raises ValueError: If the length of the given parameter is not the same as the length of the other parameters.</p> Source code in <code>src/kamae/spark/transformers/bin.py</code> <pre><code>def _check_params_size(self, param_name: str, param_value: List[Any]) -&gt; None:\n    \"\"\"\n    Checks that the length of the given parameter is the same as the length of\n    the other parameters.\n\n    Used to ensure that the parameters are consistent with each other.\n\n    :param param_name: Name of the parameter to check.\n    :param param_value: Value of the parameter to check.\n    :returns: None\n    :raises ValueError: If the length of the given parameter is not the same as\n    the length of the other parameters.\n    \"\"\"\n    names_to_check = [\"conditionOperators\", \"binValues\", \"binLabels\"]\n    names_to_check.remove(param_name)\n    for name in names_to_check:\n        if self.isDefined(name):\n            if len(param_value) != len(self.getOrDefault(name)):\n                raise ValueError(\n                    f\"\"\"{param_name} must have the same length as {name} but got\n                    {len(param_value)} and {len(self.getOrDefault(name))}\"\"\"\n                )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bin/#src.kamae.spark.transformers.bin.BinParams.getBinLabels","title":"getBinLabels","text":"<pre><code>getBinLabels()\n</code></pre> <p>Gets the binLabels parameter.</p> <p>:returns: List of string values use when binning.</p> Source code in <code>src/kamae/spark/transformers/bin.py</code> <pre><code>def getBinLabels(self) -&gt; List[Union[float, int, str]]:\n    \"\"\"\n    Gets the binLabels parameter.\n\n    :returns: List of string values use when binning.\n    \"\"\"\n    return self.getOrDefault(self.binLabels)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bin/#src.kamae.spark.transformers.bin.BinParams.getBinValues","title":"getBinValues","text":"<pre><code>getBinValues()\n</code></pre> <p>Gets the binValues parameter.</p> <p>:returns: List of float values to compare to input column</p> Source code in <code>src/kamae/spark/transformers/bin.py</code> <pre><code>def getBinValues(self) -&gt; List[float]:\n    \"\"\"\n    Gets the binValues parameter.\n\n    :returns: List of float values to compare to input column\n    \"\"\"\n    return self.getOrDefault(self.binValues)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bin/#src.kamae.spark.transformers.bin.BinParams.getConditionOperators","title":"getConditionOperators","text":"<pre><code>getConditionOperators()\n</code></pre> <p>Gets the conditionOperators parameter.</p> <p>:returns: List of string values describing the operator to use in condition: - eq - neq - lt - gt - leq - geq</p> Source code in <code>src/kamae/spark/transformers/bin.py</code> <pre><code>def getConditionOperators(self) -&gt; List[str]:\n    \"\"\"\n    Gets the conditionOperators parameter.\n\n    :returns: List of string values describing the operator to use in condition:\n    - eq\n    - neq\n    - lt\n    - gt\n    - leq\n    - geq\n    \"\"\"\n    return self.getOrDefault(self.conditionOperators)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bin/#src.kamae.spark.transformers.bin.BinParams.getDefaultLabel","title":"getDefaultLabel","text":"<pre><code>getDefaultLabel()\n</code></pre> <p>Gets the defaultLabel parameter.</p> <p>:returns: Default label to use when binning.</p> Source code in <code>src/kamae/spark/transformers/bin.py</code> <pre><code>def getDefaultLabel(self) -&gt; Union[float, int, str]:\n    \"\"\"\n    Gets the defaultLabel parameter.\n\n    :returns: Default label to use when binning.\n    \"\"\"\n    return self.getOrDefault(self.defaultLabel)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bin/#src.kamae.spark.transformers.bin.BinParams.setBinLabels","title":"setBinLabels","text":"<pre><code>setBinLabels(value)\n</code></pre> <p>Sets the binLabels parameter.</p> <p>:param value: List of string values use when binning. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/bin.py</code> <pre><code>def setBinLabels(self, value: List[Union[float, int, str]]) -&gt; \"BinParams\":\n    \"\"\"\n    Sets the binLabels parameter.\n\n    :param value: List of string values use when binning.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    self._check_params_size(\"binLabels\", value)\n    return self._set(binLabels=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bin/#src.kamae.spark.transformers.bin.BinParams.setBinValues","title":"setBinValues","text":"<pre><code>setBinValues(value)\n</code></pre> <p>Sets the binValues parameter.</p> <p>:param value: List of float values to compare to input column :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/bin.py</code> <pre><code>def setBinValues(self, value: List[float]) -&gt; \"BinParams\":\n    \"\"\"\n    Sets the binValues parameter.\n\n    :param value: List of float values to compare to input column\n    :returns: Instance of class mixed in.\n    \"\"\"\n    self._check_params_size(\"binValues\", value)\n    return self._set(binValues=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bin/#src.kamae.spark.transformers.bin.BinParams.setConditionOperators","title":"setConditionOperators","text":"<pre><code>setConditionOperators(value)\n</code></pre> <p>Sets the conditionOperators parameter.</p> <p>:param value: List of string values describing the operator to use in condition: - eq - neq - lt - gt - leq - geq :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/bin.py</code> <pre><code>def setConditionOperators(self, value: List[str]) -&gt; \"BinParams\":\n    \"\"\"\n    Sets the conditionOperators parameter.\n\n    :param value: List of string values describing the operator to use in condition:\n    - eq\n    - neq\n    - lt\n    - gt\n    - leq\n    - geq\n    :returns: Instance of class mixed in.\n    \"\"\"\n    allowed_operators = [\"eq\", \"neq\", \"lt\", \"gt\", \"leq\", \"geq\"]\n    if any([v not in allowed_operators for v in value]):\n        raise ValueError(\n            f\"\"\"All conditionOperators must be one of {allowed_operators},\n            but got {value}\"\"\"\n        )\n    self._check_params_size(\"conditionOperators\", value)\n    return self._set(conditionOperators=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bin/#src.kamae.spark.transformers.bin.BinParams.setDefaultLabel","title":"setDefaultLabel","text":"<pre><code>setDefaultLabel(value)\n</code></pre> <p>Sets the defaultLabel parameter.</p> <p>:param value: Default label to use when binning. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/bin.py</code> <pre><code>def setDefaultLabel(self, value: Union[float, int, str]) -&gt; \"BinParams\":\n    \"\"\"\n    Sets the defaultLabel parameter.\n\n    :param value: Default label to use when binning.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(defaultLabel=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bin/#src.kamae.spark.transformers.bin.BinTransformer","title":"BinTransformer","text":"<pre><code>BinTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    conditionOperators=None,\n    binValues=None,\n    binLabels=None,\n    defaultLabel=None,\n    layerName=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>BinParams</code></p> <p>Bin Spark Transformer for use in Spark pipelines. This transformer performs a binning operation on a column in a Spark dataframe.</p> <p>The binning operation is performed by comparing the input column to a list of values using a list of operators. The bin label corresponding to the first condition that evaluates to True is returned.</p> <p>If no conditions evaluate to True, the default label is returned.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param conditionOperators: List of string values describing the operator to use in condition: - eq - neq - lt - gt - leq - geq :param binValues: Float values to compare to input column. :param binLabels: Bin labels to use when binning. :param defaultLabel: Default label to use when binning. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/bin.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    conditionOperators: Optional[List[str]] = None,\n    binValues: Optional[List[float]] = None,\n    binLabels: Optional[List[Union[float, int, str]]] = None,\n    defaultLabel: Optional[Union[float, int, str]] = None,\n    layerName: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes a BinTransformer transformer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param conditionOperators: List of string values describing the operator to\n    use in condition:\n    - eq\n    - neq\n    - lt\n    - gt\n    - leq\n    - geq\n    :param binValues: Float values to compare to input column.\n    :param binLabels: Bin labels to use when binning.\n    :param defaultLabel: Default label to use when binning.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bin/#src.kamae.spark.transformers.bin.BinTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/bin/#src.kamae.spark.transformers.bin.BinTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which uses the binValues and binLabels parameters to bin the input column according to the conditionOperators parameter.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/bin.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which uses the binValues and binLabels parameters to bin the input column\n    according to the conditionOperators parameter.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    condition_operators = [\n        get_condition_operator(c) for c in self.getConditionOperators()\n    ]\n    bin_values = self.getBinValues()\n    bin_labels = self.getBinLabels()\n\n    def bin_func(x: Column) -&gt; Column:\n        \"\"\"\n        Perfoms the binning of a given column x.\n        :param x: Column to bin.\n        :returns: Binned column.\n        \"\"\"\n        bin_output = F.lit(self.getDefaultLabel())\n        # Loop through the conditions.\n        # Reverse the list of conditions so that we start from the last condition\n        # and work backwards. This ensures that the first condition that is met\n        # is the one that is used.\n        conds = zip(condition_operators[::-1], bin_values[::-1], bin_labels[::-1])\n\n        for cond_op, value, label in conds:\n            bin_output = F.when(cond_op(x, value), F.lit(label)).otherwise(\n                bin_output,\n            )\n        return bin_output\n\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n    output_col = single_input_single_output_scalar_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: bin_func(x),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bin/#src.kamae.spark.transformers.bin.BinTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the bin transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs the binning operation.</p> Source code in <code>src/kamae/spark/transformers/bin.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the bin transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs the binning operation.\n    \"\"\"\n    return BinLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        condition_operators=self.getConditionOperators(),\n        bin_values=self.getBinValues(),\n        bin_labels=self.getBinLabels(),\n        default_label=self.getDefaultLabel(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bloom_encode/","title":"bloom_encode","text":""},{"location":"reference/src/kamae/spark/transformers/bloom_encode/#src.kamae.spark.transformers.bloom_encode.BloomEncodeParams","title":"BloomEncodeParams","text":"<p>               Bases: <code>HashIndexParams</code></p> <p>Mixin class containing parameters needed for bloom encoding.</p>"},{"location":"reference/src/kamae/spark/transformers/bloom_encode/#src.kamae.spark.transformers.bloom_encode.BloomEncodeParams.getFeatureCardinality","title":"getFeatureCardinality","text":"<pre><code>getFeatureCardinality()\n</code></pre> <p>Gets the value of <code>featureCardinality</code> parameter.</p> Source code in <code>src/kamae/spark/transformers/bloom_encode.py</code> <pre><code>def getFeatureCardinality(self) -&gt; int:\n    \"\"\"\n    Gets the value of `featureCardinality` parameter.\n    \"\"\"\n    return self.getOrDefault(self.featureCardinality)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bloom_encode/#src.kamae.spark.transformers.bloom_encode.BloomEncodeParams.getNumBins","title":"getNumBins","text":"<pre><code>getNumBins()\n</code></pre> <p>Gets the number of bins to use for hash indexing.</p> Source code in <code>src/kamae/spark/transformers/bloom_encode.py</code> <pre><code>def getNumBins(self) -&gt; int:\n    \"\"\"\n    Gets the number of bins to use for hash indexing.\n    \"\"\"\n    if self.getUseHeuristicNumBins() and self.getFeatureCardinality() is not None:\n        return max(round(self.getFeatureCardinality() * 0.2), 2)\n    elif self.getUseHeuristicNumBins():\n        raise ValueError(\n            \"\"\"If useHeuristicNumBins is set to True, then the featureCardinality\n            parameter must be set.\"\"\"\n        )\n    return self.getOrDefault(self.numBins)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bloom_encode/#src.kamae.spark.transformers.bloom_encode.BloomEncodeParams.getNumHashFns","title":"getNumHashFns","text":"<pre><code>getNumHashFns()\n</code></pre> <p>Gets the value of <code>numHashFns</code> parameter.</p> Source code in <code>src/kamae/spark/transformers/bloom_encode.py</code> <pre><code>def getNumHashFns(self) -&gt; int:\n    \"\"\"\n    Gets the value of `numHashFns` parameter.\n    \"\"\"\n    return self.getOrDefault(self.numHashFns)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bloom_encode/#src.kamae.spark.transformers.bloom_encode.BloomEncodeParams.getUseHeuristicNumBins","title":"getUseHeuristicNumBins","text":"<pre><code>getUseHeuristicNumBins()\n</code></pre> <p>Gets the value of <code>useHeuristicNumBins</code> parameter.</p> Source code in <code>src/kamae/spark/transformers/bloom_encode.py</code> <pre><code>def getUseHeuristicNumBins(self) -&gt; bool:\n    \"\"\"\n    Gets the value of `useHeuristicNumBins` parameter.\n    \"\"\"\n    return self.getOrDefault(self.useHeuristicNumBins)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bloom_encode/#src.kamae.spark.transformers.bloom_encode.BloomEncodeParams.setFeatureCardinality","title":"setFeatureCardinality","text":"<pre><code>setFeatureCardinality(value)\n</code></pre> <p>Sets the <code>featureCardinality</code> parameter.</p> Source code in <code>src/kamae/spark/transformers/bloom_encode.py</code> <pre><code>def setFeatureCardinality(self, value: int) -&gt; \"BloomEncodeParams\":\n    \"\"\"\n    Sets the `featureCardinality` parameter.\n    \"\"\"\n    if value &lt; 1:\n        raise ValueError(\"featureCardinality must be greater than 0\")\n    return self._set(featureCardinality=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bloom_encode/#src.kamae.spark.transformers.bloom_encode.BloomEncodeParams.setNumHashFns","title":"setNumHashFns","text":"<pre><code>setNumHashFns(value)\n</code></pre> <p>Sets the <code>numHashFns</code> parameter.</p> Source code in <code>src/kamae/spark/transformers/bloom_encode.py</code> <pre><code>def setNumHashFns(self, value: int) -&gt; \"BloomEncodeParams\":\n    \"\"\"\n    Sets the `numHashFns` parameter.\n    \"\"\"\n    if value &lt; 2:\n        raise ValueError(\"numHashFns must be at least 2.\")\n    return self._set(numHashFns=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bloom_encode/#src.kamae.spark.transformers.bloom_encode.BloomEncodeParams.setUseHeuristicNumBins","title":"setUseHeuristicNumBins","text":"<pre><code>setUseHeuristicNumBins(value)\n</code></pre> <p>Sets the <code>useHeuristicNumBins</code> parameter.</p> Source code in <code>src/kamae/spark/transformers/bloom_encode.py</code> <pre><code>def setUseHeuristicNumBins(self, value: bool) -&gt; \"BloomEncodeParams\":\n    \"\"\"\n    Sets the `useHeuristicNumBins` parameter.\n    \"\"\"\n    return self._set(useHeuristicNumBins=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bloom_encode/#src.kamae.spark.transformers.bloom_encode.BloomEncodeTransformer","title":"BloomEncodeTransformer","text":"<pre><code>BloomEncodeTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    numHashFns=3,\n    numBins=None,\n    maskValue=None,\n    featureCardinality=None,\n    useHeuristicNumBins=False,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>BloomEncodeParams</code>, <code>SingleInputSingleOutputParams</code></p> <p>Bloom encoder Spark Transformer for use in Spark pipelines. This transformer performs bloom encoding on the input column resulting in an array of integers of size equal to numHashFns. See paper for more details: https://arxiv.org/pdf/1706.03993.pdf</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param numHashFns: Number of hash functions to use. Defaults to 3. The paper suggests a range of 2-4 hash functions for optimal performance. :param numBins: Number of hash bins. Note that this includes the <code>maskValue</code> bin, so the effective number of bins is <code>(num_bins - 1)</code> if <code>maskValue</code> is set. If <code>useHeuristicNumBins</code> is set to True, then this parameter is ignored and the number of bins is automatically set. See the description of this parameter below for how the heuristic is built. :param maskValue: A value that represents masked inputs, which are mapped to index 0. Defaults to None, meaning no mask term will be added and the hashing will start at index 0. :param featureCardinality: The cardinality of the input tensor. Needed to use the num of bins heuristic. Defaults to None, meaning the number of bins will not use the heuristic and will need to be set manually. :param useHeuristicNumBins: If set to True, the number of bins is automatically set by fixing the ratio of the feature cardinality to the number of bins to be b/f = 0.2. This ratio was found to be optimal in the paper for a wide variety of usecases. Therefore, numBins = featureCardinality * 0.2. This reduces the cardinality of the input tensor by 5x. Requires the <code>featureCardinality</code> parameter to be set. Defaults to False. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/bloom_encode.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    numHashFns: int = 3,\n    numBins: Optional[int] = None,\n    maskValue: Optional[str] = None,\n    featureCardinality: Optional[int] = None,\n    useHeuristicNumBins: bool = False,\n) -&gt; None:\n    \"\"\"\n    Instantiates a BloomEncode transformer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param numHashFns: Number of hash functions to use. Defaults to 3.\n    The paper suggests a range of 2-4 hash functions for optimal performance.\n    :param numBins: Number of hash bins. Note that this includes the `maskValue`\n    bin, so the effective number of bins is `(num_bins - 1)` if `maskValue`\n    is set. If `useHeuristicNumBins` is set to True, then this parameter is\n    ignored and the number of bins is automatically set. See the description of this\n    parameter below for how the heuristic is built.\n    :param maskValue: A value that represents masked inputs, which are mapped to\n    index 0. Defaults to None, meaning no mask term will be added and the\n    hashing will start at index 0.\n    :param featureCardinality: The cardinality of the input tensor. Needed to\n    use the num of bins heuristic. Defaults to None, meaning the number of bins will\n    not use the heuristic and will need to be set manually.\n    :param useHeuristicNumBins: If set to True, the number of bins is automatically\n    set by fixing the ratio of the feature cardinality to the number of bins\n    to be b/f = 0.2. This ratio was found to be optimal in the paper for a wide\n    variety of usecases. Therefore, numBins = featureCardinality * 0.2. This reduces\n    the cardinality of the input tensor by 5x.\n    Requires the `featureCardinality` parameter to be set. Defaults to False.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    kwargs = self._input_kwargs\n    self._setDefault(\n        numHashFns=3,\n        numBins=None,\n        maskValue=None,\n        featureCardinality=None,\n        useHeuristicNumBins=False,\n    )\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bloom_encode/#src.kamae.spark.transformers.bloom_encode.BloomEncodeTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/bloom_encode/#src.kamae.spark.transformers.bloom_encode.BloomEncodeTransformer._create_salted_input","title":"_create_salted_input","text":"<pre><code>_create_salted_input(column_data_type)\n</code></pre> <p>Builds the salted inputs according to how many hash functions are used. Specifically concatenates the input column with the string \"0\" using a separator of the hash function index.</p> <p>:returns: Salted input spark column</p> Source code in <code>src/kamae/spark/transformers/bloom_encode.py</code> <pre><code>def _create_salted_input(self, column_data_type: DataType) -&gt; Column:\n    \"\"\"\n    Builds the salted inputs according to how many hash functions are used.\n    Specifically concatenates the input column with the string \"0\" using a\n    separator of the hash function index.\n\n    :returns: Salted input spark column\n    \"\"\"\n    return single_input_single_output_scalar_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=column_data_type,\n        func=lambda x: F.array(\n            [F.concat(F.lit(x), F.lit(i)) for i in range(self.getNumHashFns())]\n        ),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bloom_encode/#src.kamae.spark.transformers.bloom_encode.BloomEncodeTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column named outputCol with the bloom encoded input column.</p> <p>:param dataset: Pyspark DataFrame to transform. :returns: Transformed pyspark dataFrame.</p> Source code in <code>src/kamae/spark/transformers/bloom_encode.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column named outputCol with the\n    bloom encoded input column.\n\n    :param dataset: Pyspark DataFrame to transform.\n    :returns: Transformed pyspark dataFrame.\n    \"\"\"\n    num_bins = self.getNumBins()\n    if num_bins is None:\n        # num_bins can be None only if useHeuristicNumBins is False\n        raise ValueError(\"numBins must be set if useHeuristicNumBins is False.\")\n    mask_value = self.getMaskValue()\n\n    input_data_type = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n    salted_input = self._create_salted_input(input_data_type)\n\n    # The salting process nests the input column into another array. Thus, the array\n    # nesting level is increased by 1.\n    def bloom_encode(x: List[str]) -&gt; List[int]:\n        return [\n            hash_udf(\n                label=y,\n                num_bins=num_bins,\n                # If the user set a mask value, then this won't match the inputs\n                # as they have been salted. So we need to salt the mask value as\n                # well.\n                mask_value=f\"{mask_value}{i}\" if mask_value is not None else None,\n            )\n            for i, y in enumerate(x)\n        ]\n\n    output_col = single_input_single_output_array_udf_transform(\n        input_col=salted_input,\n        # Input datatype is from salted input, so it has an additional; nesting.\n        input_col_datatype=ArrayType(input_data_type),\n        func=bloom_encode,\n        udf_return_element_datatype=IntegerType(),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bloom_encode/#src.kamae.spark.transformers.bloom_encode.BloomEncodeTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer that performs the bloom encoding.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that performs the bloom encoding operation.</p> Source code in <code>src/kamae/spark/transformers/bloom_encode.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer that performs the bloom encoding.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter\n    that performs the bloom encoding operation.\n    \"\"\"\n    return BloomEncodeLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        num_hash_fns=self.getNumHashFns(),\n        num_bins=self.getNumBins(),\n        mask_value=self.getMaskValue(),\n        feature_cardinality=self.getFeatureCardinality(),\n        use_heuristic_num_bins=self.getUseHeuristicNumBins(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bucketize/","title":"bucketize","text":""},{"location":"reference/src/kamae/spark/transformers/bucketize/#src.kamae.spark.transformers.bucketize.BucketizeParams","title":"BucketizeParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing splits parameter needed for bucketing.</p>"},{"location":"reference/src/kamae/spark/transformers/bucketize/#src.kamae.spark.transformers.bucketize.BucketizeParams.check_splits_sorted","title":"check_splits_sorted  <code>staticmethod</code>","text":"<pre><code>check_splits_sorted(splits)\n</code></pre> <p>Checks that the splits parameter is sorted.</p> <p>:param splits: List of float values to use for bucketing.</p> Source code in <code>src/kamae/spark/transformers/bucketize.py</code> <pre><code>@staticmethod\ndef check_splits_sorted(splits: List[float]) -&gt; None:\n    \"\"\"\n    Checks that the splits parameter is sorted.\n\n    :param splits: List of float values to use for bucketing.\n    \"\"\"\n    if splits is not None and splits != sorted(splits):\n        raise ValueError(\"`splits` argument must be a sorted list!\")\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bucketize/#src.kamae.spark.transformers.bucketize.BucketizeParams.getSplits","title":"getSplits","text":"<pre><code>getSplits()\n</code></pre> <p>Gets the splits parameter.</p> <p>:returns: List of float values to use for bucketing.</p> Source code in <code>src/kamae/spark/transformers/bucketize.py</code> <pre><code>def getSplits(self) -&gt; List[float]:\n    \"\"\"\n    Gets the splits parameter.\n\n    :returns: List of float values to use for bucketing.\n    \"\"\"\n    return self.getOrDefault(self.splits)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bucketize/#src.kamae.spark.transformers.bucketize.BucketizeParams.setSplits","title":"setSplits","text":"<pre><code>setSplits(value)\n</code></pre> <p>Sets the splits parameter.</p> <p>:param value: List of float values to use for bucketing. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/bucketize.py</code> <pre><code>def setSplits(self, value: List[float]) -&gt; \"BucketizeParams\":\n    \"\"\"\n    Sets the splits parameter.\n\n    :param value: List of float values to use for bucketing.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    self.check_splits_sorted(value)\n    return self._set(splits=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bucketize/#src.kamae.spark.transformers.bucketize.BucketizeTransformer","title":"BucketizeTransformer","text":"<pre><code>BucketizeTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    splits=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>BucketizeParams</code>, <code>SingleInputSingleOutputParams</code></p> <p>BucketizeLayer Spark Transformer for use in Spark pipelines. This transformer buckets a numerical column into bins. Buckets will be created based on the splits parameter. The bins are integer values starting at 1 and ending at the number of splits + 1. The 0 index is reserved for masking/padding.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param splits: List of float values to use for bucketing. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/bucketize.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    splits: Optional[List[float]] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes an BucketizeTransformer transformer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param splits: List of float values to use for bucketing.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bucketize/#src.kamae.spark.transformers.bucketize.BucketizeTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/bucketize/#src.kamae.spark.transformers.bucketize.BucketizeTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which is the <code>inputCol</code> bucketed into bins accoring to the <code>splits</code> parameter.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/bucketize.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which is the `inputCol` bucketed into bins accoring to the `splits` parameter.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    splits = self.getSplits()\n    # We need to create a UDF to perform binary search on the splits.\n\n    def bucketize(value: Optional[Union[float, int]]) -&gt; Optional[int]:\n        # If null, keep null. There is no best bucket to place these into.\n        if value is None:\n            return None\n        # We add 1 because we want to reserve the 0 index for mask/padding.\n        return bisect_right(splits, value) + 1\n\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n    output_col = single_input_single_output_scalar_udf_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: bucketize(x),\n        udf_return_element_datatype=IntegerType(),\n    )\n\n    return dataset.withColumn(\n        self.getOutputCol(),\n        output_col,\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/bucketize/#src.kamae.spark.transformers.bucketize.BucketizeTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the BucketizeLayer transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs a bucketing operation.</p> Source code in <code>src/kamae/spark/transformers/bucketize.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the BucketizeLayer transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs a bucketing operation.\n    \"\"\"\n    return BucketizeLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        splits=self.getSplits(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/conditional_standard_scale/","title":"conditional_standard_scale","text":""},{"location":"reference/src/kamae/spark/transformers/conditional_standard_scale/#src.kamae.spark.transformers.conditional_standard_scale.ConditionalStandardScaleTransformer","title":"ConditionalStandardScaleTransformer","text":"<pre><code>ConditionalStandardScaleTransformer(\n    inputCol=None,\n    outputCol=None,\n    layerName=None,\n    inputDtype=None,\n    outputDtype=None,\n    mean=None,\n    stddev=None,\n    skipZeros=False,\n    epsilon=0,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>StandardScaleParams</code>, <code>StandardScaleSkipZerosParams</code>, <code>SingleInputSingleOutputParams</code></p> <p>Conditional standard scaler transformer for use in Spark pipelines. This is used to standardize/transform the input column using the mean and the standard deviation. The skip_zeros parameter allows to apply the standard scaling process only when input is not equal to zero. If equal to zero, it will remain zero in the output value as it was in the input value.</p> <p>WARNING: If the input is an array, we assume that the array has a constant shape across all rows.</p> <p>more control over the standard scaling process by allowing the user to specify a mask to be used during the fit and transform process, and to specify whether to skip zeros during the transform process.</p> <p>:param inputCol: Input column name to standardize. :param outputCol: Output column name. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param mean: List of mean values corresponding to the input column. :param stddev: List of standard deviation values corresponding to the input column. :param skipZeros: If True, during spark transform and keras inference, do not apply the scaling when the values to scale are equal to zero. :param epsilon: Small value to add to conditional check of zeros. Valid only when skipZeros is True. Defaults to 0. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/conditional_standard_scale.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    layerName: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    mean: Optional[List[float]] = None,\n    stddev: Optional[List[float]] = None,\n    skipZeros: bool = False,\n    epsilon: float = 0,\n) -&gt; None:\n    \"\"\"\n    Initializes a ConditionalStandardScaleParams transformer.\n    It differs from the default StandardScaleParams in that it gives\n    more control over the standard scaling process by allowing the user\n    to specify a mask to be used during the fit and transform process, and\n    to specify whether to skip zeros during the transform process.\n\n    :param inputCol: Input column name to standardize.\n    :param outputCol: Output column name.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param mean: List of mean values corresponding to the input column.\n    :param stddev: List of standard deviation values corresponding to the\n    input column.\n    :param skipZeros: If True, during spark transform and keras inference,\n    do not apply the scaling when the values to scale are equal to zero.\n    :param epsilon: Small value to add to conditional check of zeros. Valid only\n    when skipZeros is True. Defaults to 0.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(skipZeros=False, epsilon=0)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/conditional_standard_scale/#src.kamae.spark.transformers.conditional_standard_scale.ConditionalStandardScaleTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/conditional_standard_scale/#src.kamae.spark.transformers.conditional_standard_scale.ConditionalStandardScaleTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset using the mean and standard deviation to standardize the input column. If a mask value is set, it is used to ignore elements in the dataset with that value, and they will remain unchanged in the standardization process. If skipZeros is set to True, it also ignores elements with value equal to zero in the standardization process.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Pyspark dataframe with the input column standardized,  named as the output column.</p> Source code in <code>src/kamae/spark/transformers/conditional_standard_scale.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset using the mean and standard deviation\n    to standardize the input column. If a mask value is set, it is used\n    to ignore elements in the dataset with that value, and they will remain\n    unchanged in the standardization process. If skipZeros is set to True,\n    it also ignores elements with value equal to zero in the standardization\n    process.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Pyspark dataframe with the input column standardized,\n     named as the output column.\n    \"\"\"\n    original_input_datatype = self.get_column_datatype(dataset, self.getInputCol())\n    if not isinstance(original_input_datatype, ArrayType):\n        input_col = F.array(F.col(self.getInputCol()))\n        input_datatype = ArrayType(original_input_datatype)\n    else:\n        input_col = F.col(self.getInputCol())\n        input_datatype = original_input_datatype\n\n    shift = F.array([F.lit(m) for m in self.getMean()])\n    scale = F.array([F.lit(1.0 / s if s != 0 else 0.0) for s in self.getStddev()])\n    if self.getSkipZeros():\n        eps = self.getEpsilon()\n        func = lambda x: F.transform(  # noqa: E731\n            x,\n            lambda y, i: F.when(\n                # x != (0 +- eps)\n                F.abs(y) &gt; F.lit(eps),\n                (y - F.lit(shift)[i]) * F.lit(scale)[i],\n            ).otherwise(0),\n        )\n    else:\n        func = lambda x: F.transform(  # noqa: E731\n            x,\n            lambda y, i: (y - F.lit(shift)[i]) * F.lit(scale)[i],\n        )\n    output_col = single_input_single_output_array_transform(\n        input_col=input_col,\n        input_col_datatype=input_datatype,\n        func=func,\n    )\n    if not isinstance(original_input_datatype, ArrayType):\n        output_col = output_col.getItem(0)\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/conditional_standard_scale/#src.kamae.spark.transformers.conditional_standard_scale.ConditionalStandardScaleTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the standard scaler transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter  that performs the standardization.</p> Source code in <code>src/kamae/spark/transformers/conditional_standard_scale.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the standard scaler transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter\n     that performs the standardization.\n    \"\"\"\n    np_mean = np.array(self.getMean())\n    np_variance = np.array(self.getStddev()) ** 2\n    return ConditionalStandardScaleLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        mean=np_mean,\n        variance=np_variance,\n        skip_zeros=self.getSkipZeros(),\n        epsilon=self.getEpsilon(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/cosine_similarity/","title":"cosine_similarity","text":""},{"location":"reference/src/kamae/spark/transformers/cosine_similarity/#src.kamae.spark.transformers.cosine_similarity.CosineSimilarityTransformer","title":"CosineSimilarityTransformer","text":"<pre><code>CosineSimilarityTransformer(\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>MultiInputSingleOutputParams</code></p> <p>Cosine Similarity Spark Transformer for use in Spark pipelines. This transformer computes the cosine similarity between two array columns.</p> <p>:param inputCols: Input column names. Must be two columns. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/cosine_similarity.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes a CosineSimilarityTransformer transformer.\n\n    :param inputCols: Input column names. Must be two columns.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/cosine_similarity/#src.kamae.spark.transformers.cosine_similarity.CosineSimilarityTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/cosine_similarity/#src.kamae.spark.transformers.cosine_similarity.CosineSimilarityTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which is the cosine similarity between the two input columns.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/cosine_similarity.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which is the cosine similarity between the two input columns.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_col_names = self.getInputCols()\n    input_cols = [F.col(c) for c in input_col_names]\n\n    # Check both columns are arrays.\n    datatypes = [\n        self.get_column_datatype(dataset=dataset, column_name=input_col_name)\n        for input_col_name in input_col_names\n    ]\n    if not all([isinstance(datatype, ArrayType) for datatype in datatypes]):\n        raise TypeError(\n            f\"\"\"Expected input columns to be of type ArrayType,\n            received {datatypes} instead.\"\"\"\n        )\n\n    # Compute dot product and the norms of the two arrays. The arrays are\n    # represented by the \"input_0\" and \"input_1\" elements in the zipped array.\n    def dot_product(x: Column) -&gt; Column:\n        return F.aggregate(\n            x,\n            F.lit(0.0),\n            lambda acc, y: acc + y[input_col_names[0]] * y[input_col_names[1]],\n        )\n\n    def norm(x: Column, col_name: str) -&gt; Column:\n        return F.sqrt(\n            F.aggregate(\n                x, F.lit(0.0), lambda acc, y: acc + y[col_name] * y[col_name]\n            )\n        )\n\n    # If the norms are zero, then we match the tensorflow behavior and return 0.0.\n    output_col = multi_input_single_output_array_transform(\n        input_cols=input_cols,\n        input_col_datatypes=datatypes,\n        input_col_names=input_col_names,\n        func=lambda x: F.coalesce(\n            dot_product(x)\n            / (norm(x, input_col_names[0]) * norm(x, input_col_names[1])),\n            F.lit(0.0),\n        ),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/cosine_similarity/#src.kamae.spark.transformers.cosine_similarity.CosineSimilarityTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the cosine similarity transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  computes the cosine similarity between two arrays.</p> Source code in <code>src/kamae/spark/transformers/cosine_similarity.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the cosine similarity transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     computes the cosine similarity between two arrays.\n    \"\"\"\n    return CosineSimilarityLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        axis=-1,\n        keepdims=True,\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/cosine_similarity/#src.kamae.spark.transformers.cosine_similarity.CosineSimilarityTransformer.setInputCols","title":"setInputCols","text":"<pre><code>setInputCols(value)\n</code></pre> <p>Sets the inputCols parameter. Ensures that there are only two input columns.</p> <p>:param value: List of input column names. :returns: Class instance.</p> Source code in <code>src/kamae/spark/transformers/cosine_similarity.py</code> <pre><code>def setInputCols(self, value: List[str]) -&gt; \"CosineSimilarityTransformer\":\n    \"\"\"\n    Sets the inputCols parameter. Ensures that there are only two input columns.\n\n    :param value: List of input column names.\n    :returns: Class instance.\n    \"\"\"\n    if len(value) != 2:\n        raise ValueError(\n            f\"\"\"Expected 2 input columns, received {len(value)}\n            input columns instead.\"\"\"\n        )\n    return self._set(inputCols=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/current_date/","title":"current_date","text":""},{"location":"reference/src/kamae/spark/transformers/current_date/#src.kamae.spark.transformers.current_date.CurrentDateTransformer","title":"CurrentDateTransformer","text":"<pre><code>CurrentDateTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code></p> <p>Returns the current UTC date in yyyy-MM-dd format.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/current_date.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialises the CurrentDateTransformer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n    # TODO: Remove this when we only support PySpark 3.5+. It is only used to get\n    #  the timezone set by the user for datetime operations. In 3.5+ we can use the\n    #  current_timezone() function. Also is there a better way to access this than\n    #  inside a class attribute? Setting it at the top of the file causes issues\n    #  in tests as we import all transformers when the package is loaded.\n    self.spark = SparkSession.builder.getOrCreate()\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/current_date/#src.kamae.spark.transformers.current_date.CurrentDateTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/current_date/#src.kamae.spark.transformers.current_date.CurrentDateTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Returns a column of the current date. If an array column is provided, we return an array column of identical structure with elements populated by the current date.</p> <p>:param dataset: Input dataframe. :returns: Transformed dataframe.</p> Source code in <code>src/kamae/spark/transformers/current_date.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Returns a column of the current date. If an array column is provided,\n    we return an array column of identical structure with elements populated by\n    the current date.\n\n    :param dataset: Input dataframe.\n    :returns: Transformed dataframe.\n    \"\"\"\n\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n\n    def current_utc_date() -&gt; Column:\n        \"\"\"\n        Returns the current UTC date. Spark respects the timezone set in the Spark\n        session so we need to convert the local timestamp to UTC before extracting\n        the date.\n\n        :returns: Column of the current UTC date.\n        \"\"\"\n        local_timestamp = F.localtimestamp()\n        # TODO: Replace this with current_timezone() once we only support PySpark\n        #  3.5+\n        local_timezone = self.spark.conf.get(\"spark.sql.session.timeZone\")\n        return F.to_date(F.to_utc_timestamp(local_timestamp, local_timezone))\n\n    output_col = single_input_single_output_scalar_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: current_utc_date().cast(\"string\"),\n    )\n\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/current_date/#src.kamae.spark.transformers.current_date.CurrentDateTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer.</p> <p>:returns: CurrentDateLayer Tensorflow layer.</p> Source code in <code>src/kamae/spark/transformers/current_date.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer.\n\n    :returns: CurrentDateLayer Tensorflow layer.\n    \"\"\"\n    return CurrentDateLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/current_date_time/","title":"current_date_time","text":""},{"location":"reference/src/kamae/spark/transformers/current_date_time/#src.kamae.spark.transformers.current_date_time.CurrentDateTimeTransformer","title":"CurrentDateTimeTransformer","text":"<pre><code>CurrentDateTimeTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code></p> <p>Returns the current UTC datetime in yyyy-MM-dd HHss.SSS format.</p> <p>NOTE: Parity between this and its TensorFlow counterpart is very difficult at the millisecond level. We have to round the TensorFlow timestamp to the 3rd decimal place for milliseconds, because  Spark already truncates to 3 decimal places. Therefore, parity is not guaranteed at this precision.</p> <p>It is recommended not to rely on parity at the millisecond level.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/current_date_time.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialises the CurrentDateTime layer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n    # TODO: Remove this when we only support PySpark 3.5+. It is only used to get\n    #  the timezone set by the user for datetime operations. In 3.5+ we can use the\n    #  current_timezone() function. Also is there a better way to access this than\n    #  inside a class attribute? Setting it at the top of the file causes issues\n    #  in tests as we import all transformers when the package is loaded.\n    self.spark = SparkSession.builder.getOrCreate()\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/current_date_time/#src.kamae.spark.transformers.current_date_time.CurrentDateTimeTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/current_date_time/#src.kamae.spark.transformers.current_date_time.CurrentDateTimeTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Returns a column of the current datetime. If an array column is provided, we return an array column of identical structure with elements populated by the current datetime</p> <p>:param dataset: Input dataframe. :returns: Transformed dataframe.</p> Source code in <code>src/kamae/spark/transformers/current_date_time.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Returns a column of the current datetime. If an array column is provided,\n    we return an array column of identical structure with elements populated by\n    the current datetime\n\n    :param dataset: Input dataframe.\n    :returns: Transformed dataframe.\n    \"\"\"\n\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n\n    def current_utc_timestamp() -&gt; Column:\n        \"\"\"\n        Returns the current UTC timestamp. Spark respects the timezone\n        set in the Spark session for datetime operations, so we need to be agnostic\n        to the timezone set by the user.\n\n        :returns: Column of the current UTC timestamp.\n        \"\"\"\n        local_timestamp = F.localtimestamp()\n        # TODO: Replace this with current_timezone() once we only support PySpark\n        #  3.5+\n        local_timezone = self.spark.conf.get(\"spark.sql.session.timeZone\")\n        return F.date_format(\n            F.to_utc_timestamp(local_timestamp, local_timezone),\n            \"yyyy-MM-dd HH:mm:ss.SSS\",\n        )\n\n    output_col = single_input_single_output_scalar_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: current_utc_timestamp(),\n    )\n\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/current_date_time/#src.kamae.spark.transformers.current_date_time.CurrentDateTimeTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer.</p> <p>:returns: CurrentDateTimeLayer Tensorflow layer.</p> Source code in <code>src/kamae/spark/transformers/current_date_time.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer.\n\n    :returns: CurrentDateTimeLayer Tensorflow layer.\n    \"\"\"\n    return CurrentDateTimeLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/current_unix_timestamp/","title":"current_unix_timestamp","text":""},{"location":"reference/src/kamae/spark/transformers/current_unix_timestamp/#src.kamae.spark.transformers.current_unix_timestamp.CurrentUnixTimestampTransformer","title":"CurrentUnixTimestampTransformer","text":"<pre><code>CurrentUnixTimestampTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    unit=\"s\",\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>UnixTimestampParams</code></p> <p>Returns the current unix timestamp in either seconds or milliseconds.</p> <p>NOTE: Parity between this and its TensorFlow counterpart is very difficult at the millisecond level. TensorFlow provides much more precision of the timestamp, and has floating 64-bit precision of the unix timestamp in seconds. Whereas Spark 3.4.0 only supports millisecond precision (3 decimal places of unix timestamp in seconds). Therefore, parity is not guaranteed at this precision.</p> <p>It is recommended not to rely on parity at the millisecond level.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param unit: Unit of the output timestamp. Can be either \"s\" (or \"seconds\") for seconds or \"ms\" (or \"milliseconds\") for milliseconds. Defaults to \"s\". :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/current_unix_timestamp.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    unit: str = \"s\",\n) -&gt; None:\n    \"\"\"\n    Initialises the CurrentUnixTimestamp layer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param unit: Unit of the output timestamp. Can be either \"s\" (or \"seconds\")\n    for seconds or \"ms\" (or \"milliseconds\") for milliseconds. Defaults to \"s\".\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(unit=\"s\")\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/current_unix_timestamp/#src.kamae.spark.transformers.current_unix_timestamp.CurrentUnixTimestampTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/current_unix_timestamp/#src.kamae.spark.transformers.current_unix_timestamp.CurrentUnixTimestampTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Returns a column of the current unix timestamp. If an array column is provided, we return an array column of identical structure with elements populated by the current unix timestamp.</p> <p>:param dataset: Input dataframe. :returns: Transformed dataframe.</p> Source code in <code>src/kamae/spark/transformers/current_unix_timestamp.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Returns a column of the current unix timestamp. If an array column is provided,\n    we return an array column of identical structure with elements populated by\n    the current unix timestamp.\n\n    :param dataset: Input dataframe.\n    :returns: Transformed dataframe.\n    \"\"\"\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n\n    def current_unix_timestamp() -&gt; Column:\n        \"\"\"\n        Returns the current unix timestamp in either seconds or milliseconds.\n\n        :returns: Column of the current unix timestamp.\n        \"\"\"\n        # TODO: For PySpark 3.5+ we can use unix_millis. For now, we use\n        #  unix_timestamp that returns seconds (truncated so no milliseconds).\n        #  In order to get milliseconds, we get the milliseconds from the current\n        #  timestamp and add it to the truncated seconds.\n        current_ts = F.current_timestamp()\n        unix_timestamp_in_trucated_seconds = F.unix_timestamp(current_ts)\n        milliseconds_str = F.date_format(current_ts, \"SSS\")\n        milliseconds_float = milliseconds_str.cast(\"float\") / 1000.0\n        unix_timestamp_in_seconds = (\n            unix_timestamp_in_trucated_seconds + milliseconds_float\n        )\n        return (\n            unix_timestamp_in_seconds\n            if self.getUnit() == \"s\"\n            else unix_timestamp_in_seconds * 1000.0\n        )\n\n    output_col = single_input_single_output_scalar_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: current_unix_timestamp(),\n    )\n\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/current_unix_timestamp/#src.kamae.spark.transformers.current_unix_timestamp.CurrentUnixTimestampTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer.</p> <p>:returns: CurrentUnixTimestampLayer Tensorflow layer.</p> Source code in <code>src/kamae/spark/transformers/current_unix_timestamp.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer.\n\n    :returns: CurrentUnixTimestampLayer Tensorflow layer.\n    \"\"\"\n    return CurrentUnixTimestampLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        unit=self.getUnit(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/date_add/","title":"date_add","text":""},{"location":"reference/src/kamae/spark/transformers/date_add/#src.kamae.spark.transformers.date_add.DateAddTransformer","title":"DateAddTransformer","text":"<pre><code>DateAddTransformer(\n    inputCol=None,\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    numDays=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>MultiInputSingleOutputParams</code>, <code>DateAdditionParams</code></p> <p>Transformer to add or subtract a static or dynamic (column) number of days from a date column.</p> <p>WARNING: This transform destroys the time component of the date column.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Layer name. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param numDays: Number of days to add/subtract. Negative values subtract. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/date_add.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    numDays: Optional[int] = None,\n) -&gt; None:\n    \"\"\"\n    Initialises the date add transform layer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Layer name. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param numDays: Number of days to add/subtract. Negative values subtract.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(numDays=None)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/date_add/#src.kamae.spark.transformers.date_add.DateAddTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/date_add/#src.kamae.spark.transformers.date_add.DateAddTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Adds or subtracts a number of days from a date column.</p> <p>:param dataset: Input dataframe. :returns: Transformed dataframe.</p> Source code in <code>src/kamae/spark/transformers/date_add.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Adds or subtracts a number of days from a date column.\n\n    :param dataset: Input dataframe.\n    :returns: Transformed dataframe.\n    \"\"\"\n    input_cols = self.get_multiple_input_cols(\n        constant_param_name=\"numDays\", input_cols_limit=2\n    )\n    # input_cols can contain either actual columns or lit(constants). In order to\n    # determine the datatype of the input columns, we select them from the dataset\n    # first.\n    input_col_names = dataset.select(input_cols).columns\n    input_col_datatypes = [\n        self.get_column_datatype(dataset=dataset.select(input_cols), column_name=c)\n        for c in input_col_names\n    ]\n    if not isinstance(get_element_type(input_col_datatypes[0]), StringType):\n        raise ValueError(\n            f\"\"\"Expected input column {input_col_names[0]} to have element type\n            StringType, but got {input_col_datatypes[0]}.\"\"\"\n        )\n    if not isinstance(\n        get_element_type(input_col_datatypes[1]),\n        (ByteType, ShortType, IntegerType, LongType),\n    ):\n        raise ValueError(\n            f\"\"\"Expected input column {input_col_names[1]} to have element type\n             ByteType, ShortType or IntegerType, but got {input_col_datatypes[1]}.\n             \"\"\"\n        )\n    date_col_name = input_col_names[0]\n    num_days_col_name = input_col_names[1]\n    output_col = multi_input_single_output_scalar_transform(\n        input_cols=input_cols,\n        input_col_names=input_col_names,\n        input_col_datatypes=input_col_datatypes,\n        # Cast to int since LongType not supported in date_add, but we want to allow\n        # it as an input type.\n        func=lambda x: F.date_add(\n            x[date_col_name], x[num_days_col_name].cast(\"int\")\n        ).cast(\"string\"),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/date_add/#src.kamae.spark.transformers.date_add.DateAddTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer.</p> <p>:returns: DateAddLayer Tensorflow layer.</p> Source code in <code>src/kamae/spark/transformers/date_add.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer.\n\n    :returns: DateAddLayer Tensorflow layer.\n    \"\"\"\n    return DateAddLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        num_days=self.getNumDays(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/date_add/#src.kamae.spark.transformers.date_add.DateAddTransformer.setInputCols","title":"setInputCols","text":"<pre><code>setInputCols(value)\n</code></pre> <p>Sets the value of the inputCols parameter.</p> <p>:param value: Input column names. :returns: Class instance.</p> Source code in <code>src/kamae/spark/transformers/date_add.py</code> <pre><code>def setInputCols(self, value: List[str]) -&gt; \"DateAddTransformer\":\n    \"\"\"\n    Sets the value of the inputCols parameter.\n\n    :param value: Input column names.\n    :returns: Class instance.\n    \"\"\"\n    if len(value) != 2:\n        raise ValueError(\"If using multiple inputs, exactly two are required.\")\n    if self.getNumDays() is not None:\n        raise ValueError(\"Cannot use multiple inputs if numDays is set.\")\n    if self.getInputDtype() is not None:\n        raise ValueError(\n            \"\"\"Input auto-casting is set via inputDtype, however multiple inputs are\n            being used. Auto-casting inputs is not supported for multiple inputs in\n            the DateAddTransformer because the two inputs must be\n            different types.\"\"\"\n        )\n    return self._set(inputCols=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/date_add/#src.kamae.spark.transformers.date_add.DateAddTransformer.setInputDtype","title":"setInputDtype","text":"<pre><code>setInputDtype(value)\n</code></pre> <p>Overrides setting the parameter inputDtype to the given string value.</p> <p>If multiple input columns are being used, the inputDtype parameter cannot be set.</p> <p>:param value: String to set the inputDtype parameter to. :raises ValueError: If inputCols is set. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/date_add.py</code> <pre><code>def setInputDtype(self, value: str) -&gt; \"DateAddTransformer\":\n    \"\"\"\n    Overrides setting the parameter inputDtype to the given string value.\n\n    If multiple input columns are being used, the inputDtype parameter cannot be\n    set.\n\n    :param value: String to set the inputDtype parameter to.\n    :raises ValueError: If inputCols is set.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if self.isDefined(\"inputCols\"):\n        raise ValueError(\n            \"\"\"Input auto-casting is not supported for multiple inputs in the\n            DateAddTransformer because the two inputs must be different types.\"\"\"\n        )\n    return self._set(inputDtype=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/date_add/#src.kamae.spark.transformers.date_add.DateAdditionParams","title":"DateAdditionParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class for a date addition transformer.</p>"},{"location":"reference/src/kamae/spark/transformers/date_add/#src.kamae.spark.transformers.date_add.DateAdditionParams.getNumDays","title":"getNumDays","text":"<pre><code>getNumDays()\n</code></pre> <p>Gets the value of the numDays parameter.</p> <p>:returns: Number of days to add/subtract.</p> Source code in <code>src/kamae/spark/transformers/date_add.py</code> <pre><code>def getNumDays(self) -&gt; int:\n    \"\"\"\n    Gets the value of the numDays parameter.\n\n    :returns: Number of days to add/subtract.\n    \"\"\"\n    return self.getOrDefault(self.numDays)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/date_add/#src.kamae.spark.transformers.date_add.DateAdditionParams.setNumDays","title":"setNumDays","text":"<pre><code>setNumDays(value)\n</code></pre> <p>Sets the value of the numDays parameter.</p> <p>:param value: Number of days to add/subtract. :returns: Class instance.</p> Source code in <code>src/kamae/spark/transformers/date_add.py</code> <pre><code>def setNumDays(self, value: int) -&gt; \"DateAdditionParams\":\n    \"\"\"\n    Sets the value of the numDays parameter.\n\n    :param value: Number of days to add/subtract.\n    :returns: Class instance.\n    \"\"\"\n    if self.isDefined(\"inputCols\"):\n        raise ValueError(\"Cannot set numDays if using multiple inputCols.\")\n    return self._set(numDays=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/date_diff/","title":"date_diff","text":""},{"location":"reference/src/kamae/spark/transformers/date_diff/#src.kamae.spark.transformers.date_diff.DateDiffTransformer","title":"DateDiffTransformer","text":"<pre><code>DateDiffTransformer(\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    defaultValue=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>MultiInputSingleOutputParams</code>, <code>DefaultIntValueParams</code></p> <p>DateDiffLayer Spark Transformer for use in Spark pipelines. This transformer calculates the difference between two dates.</p> <p>:param inputCols: Input column names. The inputs must be in yyyy-MM-dd (HHss.SSS) format and must be passed to the layer in the order [start date , end date]. The transformer will return a negative value if the order is reversed. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param defaultValue: Default value to use when one of the dates is the empty string. Empty strings can be used when the date is not available. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/date_diff.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    defaultValue: Optional[int] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes an DateDiffTransformer transformer.\n\n    :param inputCols: Input column names.\n    The inputs must be in yyyy-MM-dd (HH:mm:ss.SSS) format and\n    must be passed to the layer in the order [start date , end date].\n    The transformer will return a negative value if the order is reversed.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param defaultValue: Default value to use when one of the dates is the empty\n    string. Empty strings can be used when the date is not available.\n    :returns: None - class instantiated.\n    \"\"\"\n\n    super().__init__()\n    self._setDefault(defaultValue=None)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/date_diff/#src.kamae.spark.transformers.date_diff.DateDiffTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/date_diff/#src.kamae.spark.transformers.date_diff.DateDiffTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which contains the result of the date difference operation of the inputCols</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/date_diff.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which contains the result of the date difference operation of the inputCols\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_col_names = self.getInputCols()\n    input_cols = [F.col(c) for c in input_col_names]\n    input_col_datatypes = [\n        self.get_column_datatype(dataset=dataset, column_name=c)\n        for c in input_col_names\n    ]\n\n    def date_diff(x: Column) -&gt; Column:\n        if self.getDefaultValue() is not None:\n            return F.when(\n                (x[input_col_names[0]] == F.lit(\"\"))\n                | (x[input_col_names[1]] == F.lit(\"\")),\n                F.lit(self.getDefaultValue()),\n            ).otherwise(F.datediff(x[input_col_names[1]], x[input_col_names[0]]))\n        return F.datediff(x[input_col_names[1]], x[input_col_names[0]])\n\n    output_col = multi_input_single_output_scalar_transform(\n        input_cols=input_cols,\n        input_col_datatypes=input_col_datatypes,\n        input_col_names=input_col_names,\n        func=lambda x: date_diff(x),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/date_diff/#src.kamae.spark.transformers.date_diff.DateDiffTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the absolute value transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs an absolute value operation.</p> Source code in <code>src/kamae/spark/transformers/date_diff.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the absolute value transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs an absolute value operation.\n    \"\"\"\n    return DateDiffLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        default_value=self.getDefaultValue(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/date_diff/#src.kamae.spark.transformers.date_diff.DateDiffTransformer.setInputCols","title":"setInputCols","text":"<pre><code>setInputCols(value)\n</code></pre> <p>Overrides setting the input columns for the transformer. Throws an error if we do not have exactly two input columns.</p> <p>:param value: List of input columns. :returns: Class instance.</p> Source code in <code>src/kamae/spark/transformers/date_diff.py</code> <pre><code>def setInputCols(self, value: List[str]) -&gt; \"DateDiffTransformer\":\n    \"\"\"\n    Overrides setting the input columns for the transformer.\n    Throws an error if we do not have exactly two input columns.\n\n    :param value: List of input columns.\n    :returns: Class instance.\n    \"\"\"\n    if len(value) != 2:\n        raise ValueError(\n            \"\"\"When setting inputCols for DateDiffTransformer,\n            there must be exactly two input columns.\"\"\"\n        )\n    return self._set(inputCols=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/date_parse/","title":"date_parse","text":""},{"location":"reference/src/kamae/spark/transformers/date_parse/#src.kamae.spark.transformers.date_parse.DateParseParams","title":"DateParseParams","text":"<p>               Bases: <code>DefaultIntValueParams</code></p> <p>Mixin class for a date part.</p>"},{"location":"reference/src/kamae/spark/transformers/date_parse/#src.kamae.spark.transformers.date_parse.DateParseParams.getDatePart","title":"getDatePart","text":"<pre><code>getDatePart()\n</code></pre> <p>Gets the value of the datePart parameter.</p> <p>:returns: Date part to extract from date.</p> Source code in <code>src/kamae/spark/transformers/date_parse.py</code> <pre><code>def getDatePart(self) -&gt; str:\n    \"\"\"\n    Gets the value of the datePart parameter.\n\n    :returns: Date part to extract from date.\n    \"\"\"\n    return self.getOrDefault(self.datePart)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/date_parse/#src.kamae.spark.transformers.date_parse.DateParseParams.setDatePart","title":"setDatePart","text":"<pre><code>setDatePart(value)\n</code></pre> <p>Sets the value of the datePart parameter.</p> <p>:param value: Date part to extract from date. :returns: Class instance.</p> Source code in <code>src/kamae/spark/transformers/date_parse.py</code> <pre><code>def setDatePart(self, value: str) -&gt; \"DateParseParams\":\n    \"\"\"\n    Sets the value of the datePart parameter.\n\n    :param value: Date part to extract from date.\n    :returns: Class instance.\n    \"\"\"\n\n    allowed_date_parts = {\n        \"Year\",\n        \"DayOfYear\",\n        \"MonthOfYear\",\n        \"DayOfMonth\",\n        \"DayOfWeek\",\n        \"Hour\",\n        \"Minute\",\n        \"Second\",\n        \"Millisecond\",\n    }\n\n    if value not in allowed_date_parts:\n        raise ValueError(\n            f\"Invalid date part: {value}. Must be one of {allowed_date_parts}\"\n        )\n\n    return self._set(datePart=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/date_parse/#src.kamae.spark.transformers.date_parse.DateParseTransformer","title":"DateParseTransformer","text":"<pre><code>DateParseTransformer(\n    datePart=None,\n    defaultValue=None,\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>DateParseParams</code></p> <p>Date parse transform layer. This layer parses a date(time) column into a specified date part. We require the date format to be yyyy-MM-dd (HHss.SSS).</p> <p>Date parts can be one of the following: - <code>DayOfWeek</code> - day of week (Monday = 1, Sunday = 7) - <code>DayOfMonth</code> - day of month - <code>DayOfYear</code> - day of year e.g. (2021-01-01 = 1, 2021-12-31 = 365) - <code>MonthOfYear</code> - month of year - <code>Year</code> - year - <code>Hour</code> - hour e.g. (2021-01-01 00:00:00 = 0, 2021-01-01 23:59:59 = 23) - <code>Minute</code> - minute e.g. (2021-01-01 00:00:00 = 0, 2021-01-01 00:59:00 = 59) - <code>Second</code> - second e.g. (2021-01-01 00:00:00 = 0, 2021-01-01 00:00:59 = 59) - <code>Millisecond</code> - millisecond (2021-01-01 00:00:00.357 = 357)</p> <p>In the case a timestamp is not provided, all hour, minutes, seconds and milliseconds fields will be returned as 0.</p> <p>:param datePart: Date part to extract from date. :param defaultValue: Default value to use when the date is the empty string. Empty strings can be used when the date is not available. :param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Layer name. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/date_parse.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    datePart: Optional[str] = None,\n    defaultValue: Optional[int] = None,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialises the date parse transform layer.\n\n    :param datePart: Date part to extract from date.\n    :param defaultValue: Default value to use when the date is the empty string.\n    Empty strings can be used when the date is not available.\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Layer name. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(defaultValue=None)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/date_parse/#src.kamae.spark.transformers.date_parse.DateParseTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/date_parse/#src.kamae.spark.transformers.date_parse.DateParseTransformer._parse_date","title":"_parse_date","text":"<pre><code>_parse_date(column)\n</code></pre> <p>Private function to parse the date column. :param column: Input column. :returns: Parsed datepart column.</p> Source code in <code>src/kamae/spark/transformers/date_parse.py</code> <pre><code>def _parse_date(self, column: Column) -&gt; Column:\n    \"\"\"\n    Private function to parse the date column.\n    :param column: Input column.\n    :returns: Parsed datepart column.\n    \"\"\"\n\n    date_part_to_format_pattern = {\n        \"Year\": \"y\",\n        \"DayOfYear\": \"D\",\n        \"MonthOfYear\": \"M\",\n        \"DayOfMonth\": \"d\",\n        \"DayOfWeek\": \"E\",\n        \"Hour\": \"H\",\n        \"Minute\": \"m\",\n        \"Second\": \"s\",\n        \"Millisecond\": \"SSS\",\n    }\n\n    formatted_date = F.date_format(\n        column, date_part_to_format_pattern[self.getDatePart()]\n    )\n\n    if self.getDatePart() == \"DayOfWeek\":\n        day_of_week_mapping = {\n            \"Mon\": 1,\n            \"Tue\": 2,\n            \"Wed\": 3,\n            \"Thu\": 4,\n            \"Fri\": 5,\n            \"Sat\": 6,\n            \"Sun\": 7,\n        }\n\n        day_of_week_spark_mapping = F.create_map(\n            [F.lit(x) for x in chain(*day_of_week_mapping.items())]\n        )\n\n        return day_of_week_spark_mapping[formatted_date]\n\n    return formatted_date.cast(\"int\")\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/date_parse/#src.kamae.spark.transformers.date_parse.DateParseTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input date column into the specified date part.</p> <p>Utilises date format, which itself uses the following: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html</p> <p>:param dataset: Input dataframe. :returns: Transformed dataframe.</p> Source code in <code>src/kamae/spark/transformers/date_parse.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input date column into the specified date part.\n\n    Utilises date format, which itself uses the following:\n    https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n\n    :param dataset: Input dataframe.\n    :returns: Transformed dataframe.\n    \"\"\"\n\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n\n    def date_parse(x: Column) -&gt; Column:\n        if self.getDefaultValue() is not None:\n            return F.when(x == F.lit(\"\"), F.lit(self.getDefaultValue())).otherwise(\n                self._parse_date(x)\n            )\n        return self._parse_date(x)\n\n    output_col = single_input_single_output_scalar_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: date_parse(x),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/date_parse/#src.kamae.spark.transformers.date_parse.DateParseTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer.</p> <p>:returns: DateParseLayer Tensorflow layer.</p> Source code in <code>src/kamae/spark/transformers/date_parse.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer.\n\n    :returns: DateParseLayer Tensorflow layer.\n    \"\"\"\n\n    if not self.isDefined(\"datePart\"):\n        raise ValueError(\"Date part must be set.\")\n    date_part = self.getDatePart()\n\n    return DateParseLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        date_part=date_part,\n        default_value=self.getDefaultValue(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/date_time_to_unix_timestamp/","title":"date_time_to_unix_timestamp","text":""},{"location":"reference/src/kamae/spark/transformers/date_time_to_unix_timestamp/#src.kamae.spark.transformers.date_time_to_unix_timestamp.DateTimeToUnixTimestampTransformer","title":"DateTimeToUnixTimestampTransformer","text":"<pre><code>DateTimeToUnixTimestampTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    unit=\"s\",\n    layerName=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>UnixTimestampParams</code></p> <p>Transformer that converts a datetime string to a unix timestamp.</p> <p>The unix timestamp can be in milliseconds or seconds, set by the <code>unit</code> parameter.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param unit: Unit of the output timestamp. Can be <code>milliseconds</code> (shorthand <code>ms</code>) or <code>seconds</code> (shorthand <code>s</code>). Default is <code>s</code> (seconds). :param layerName: Layer name. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/date_time_to_unix_timestamp.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    unit: str = \"s\",\n    layerName: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialises the DateTimeToUnixTimestampTransformer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param unit: Unit of the output timestamp. Can be `milliseconds`\n    (shorthand `ms`) or `seconds` (shorthand `s`). Default is `s` (seconds).\n    :param layerName: Layer name. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(unit=\"s\")\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/date_time_to_unix_timestamp/#src.kamae.spark.transformers.date_time_to_unix_timestamp.DateTimeToUnixTimestampTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/date_time_to_unix_timestamp/#src.kamae.spark.transformers.date_time_to_unix_timestamp.DateTimeToUnixTimestampTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input integer timestamp to the date string with format yyyy-MM-dd HHss.SSS.</p> <p>:param dataset: Input dataframe. :returns: Transformed dataframe.</p> Source code in <code>src/kamae/spark/transformers/date_time_to_unix_timestamp.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input integer timestamp to the date string with format\n    yyyy-MM-dd HH:mm:ss.SSS.\n\n    :param dataset: Input dataframe.\n    :returns: Transformed dataframe.\n    \"\"\"\n\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n\n    def datetime_to_unix_timestamp(datetime: Column) -&gt; Column:\n        \"\"\"\n        Converts a datetime string to a unix timestamp in seconds.\n\n        :param datetime: Column of datetime strings.\n        :returns: Column of unix timestamps.\n        \"\"\"\n        # Check if we have a date only or datetime. This is quite a crude way to\n        # check if we have a datetime string.\n        split_datetime = F.split(datetime, \" \")\n        is_datetime = F.size(split_datetime) &gt; 1\n        # If we have a date only, add 00:00:00.000 UTC. Otherwise, add UTC suffix\n        # This is to ensure that the datetime string is in the correct format for\n        # PySpark\n        datetime_w_utc_tz = F.when(\n            is_datetime, F.concat(datetime, F.lit(\" UTC\"))\n        ).otherwise(F.concat(datetime, F.lit(\" 00:00:00.000 UTC\")))\n        # Convert datetime string to timestamp\n        datetime_timestamp = F.to_timestamp(datetime_w_utc_tz)\n        # Convert timestamp to unix timestamp\n        unix_timestamp_wo_milliseconds = F.unix_timestamp(datetime_timestamp)\n        # Extract milliseconds from the datetime string\n        milliseconds_str = F.date_format(datetime_timestamp, \"SSS\")\n        # Convert milliseconds to float\n        milliseconds_float = milliseconds_str.cast(\"float\") / 1000.0\n        # Add milliseconds to the unix timestamp if we have them\n        return unix_timestamp_wo_milliseconds + milliseconds_float\n\n    output_col = single_input_single_output_scalar_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: datetime_to_unix_timestamp(x)\n        if self.getUnit() == \"s\"\n        else datetime_to_unix_timestamp(x) * 1000.0,\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/date_time_to_unix_timestamp/#src.kamae.spark.transformers.date_time_to_unix_timestamp.DateTimeToUnixTimestampTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer that performs the datetime to unix timestamp.</p> <p>:returns: Tensorflow layer that performs the unix timestamp to date transform.</p> Source code in <code>src/kamae/spark/transformers/date_time_to_unix_timestamp.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer that performs the datetime to unix timestamp.\n\n    :returns: Tensorflow layer that performs the unix timestamp to date transform.\n    \"\"\"\n    return DateTimeToUnixTimestampLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        unit=self.getUnit(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/divide/","title":"divide","text":""},{"location":"reference/src/kamae/spark/transformers/divide/#src.kamae.spark.transformers.divide.DivideTransformer","title":"DivideTransformer","text":"<pre><code>DivideTransformer(\n    inputCol=None,\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    mathFloatConstant=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>MultiInputSingleOutputParams</code>, <code>MathFloatConstantParams</code></p> <p>DivideLayer Spark Transformer for use in Spark pipelines. This transformer divides a column by a constant or another column.</p> <p>:param inputCol: Input column name. Only used if inputCols is not specified. If specified, we divide this column by the mathFloatConstant. :param inputCols: Input column names. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param mathFloatConstant: Optional constant to divide by. If not provided, then two input columns are required. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/divide.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    mathFloatConstant: Optional[float] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes an DivideTransformer transformer.\n\n    :param inputCol: Input column name. Only used if inputCols is not specified.\n    If specified, we divide this column by the mathFloatConstant.\n    :param inputCols: Input column names.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param mathFloatConstant: Optional constant to divide by. If not provided,\n    then two input columns are required.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(mathFloatConstant=None)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/divide/#src.kamae.spark.transformers.divide.DivideTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/divide/#src.kamae.spark.transformers.divide.DivideTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which is the same as the column with name <code>inputCol</code>.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/divide.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which is the same as the column with name `inputCol`.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_cols = self.get_multiple_input_cols(\n        constant_param_name=\"mathFloatConstant\",\n    )\n    # input_cols can contain either actual columns or lit(constants). In order to\n    # determine the datatype of the input columns, we select them from the dataset\n    # first.\n    input_col_names = dataset.select(input_cols).columns\n    input_col_datatypes = [\n        self.get_column_datatype(dataset=dataset.select(input_cols), column_name=c)\n        for c in input_col_names\n    ]\n\n    def divide_no_nan(column1: Column, column2: Column) -&gt; Column:\n        \"\"\"\n        Divide two columns, and if the result is NaN, return 0.0 instead\n        \"\"\"\n        return F.coalesce(column1 / column2, F.lit(0.0))\n\n    output_col = multi_input_single_output_scalar_transform(\n        input_cols=input_cols,\n        input_col_names=input_col_names,\n        input_col_datatypes=input_col_datatypes,\n        func=lambda x: reduce(divide_no_nan, [x[c] for c in input_col_names]),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/divide/#src.kamae.spark.transformers.divide.DivideTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the divide transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs a divide operation.</p> Source code in <code>src/kamae/spark/transformers/divide.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the divide transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs a divide operation.\n    \"\"\"\n    return DivideLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        divisor=self.getMathFloatConstant(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/exp/","title":"exp","text":""},{"location":"reference/src/kamae/spark/transformers/exp/#src.kamae.spark.transformers.exp.ExpTransformer","title":"ExpTransformer","text":"<pre><code>ExpTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code></p> <p>exp value Spark Transformer for use in Spark pipelines. This transformer applies exp(x) operation to the input.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/exp.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes an ExpTransformer transformer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/exp/#src.kamae.spark.transformers.exp.ExpTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/exp/#src.kamae.spark.transformers.exp.ExpTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which applies exp(<code>inputCol</code>).</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/exp.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which applies exp(`inputCol`).\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n    output_col = single_input_single_output_scalar_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: F.exp(x),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/exp/#src.kamae.spark.transformers.exp.ExpTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the exp value transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs an exp value operation.</p> Source code in <code>src/kamae/spark/transformers/exp.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the exp value transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs an exp value operation.\n    \"\"\"\n    return ExpLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/exponent/","title":"exponent","text":""},{"location":"reference/src/kamae/spark/transformers/exponent/#src.kamae.spark.transformers.exponent.ExponentParams","title":"ExponentParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing alpha parameter needed for exponent transform layers.</p>"},{"location":"reference/src/kamae/spark/transformers/exponent/#src.kamae.spark.transformers.exponent.ExponentParams.getExponent","title":"getExponent","text":"<pre><code>getExponent()\n</code></pre> <p>Gets the exponent parameter.</p> <p>:returns: Float value of exponent used in exponent transform.</p> Source code in <code>src/kamae/spark/transformers/exponent.py</code> <pre><code>def getExponent(self) -&gt; float:\n    \"\"\"\n    Gets the exponent parameter.\n\n    :returns: Float value of exponent used in exponent transform.\n    \"\"\"\n    return self.getOrDefault(self.exponent)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/exponent/#src.kamae.spark.transformers.exponent.ExponentParams.setExponent","title":"setExponent","text":"<pre><code>setExponent(value)\n</code></pre> <p>Sets the exponent parameter.</p> <p>:param value: Float value to use in exponent transform: x^exponent. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/exponent.py</code> <pre><code>def setExponent(self, value: float) -&gt; \"ExponentParams\":\n    \"\"\"\n    Sets the exponent parameter.\n\n    :param value: Float value to use in exponent transform: x^exponent.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(exponent=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/exponent/#src.kamae.spark.transformers.exponent.ExponentTransformer","title":"ExponentTransformer","text":"<pre><code>ExponentTransformer(\n    inputCol=None,\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    exponent=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>MultiInputSingleOutputParams</code>, <code>ExponentParams</code></p> <p>Exponent Spark Transformer for use in Spark pipelines. This transformer applies x^exponent in the case of single input and or x^y in the case of two inputs.</p> <p>:param inputCol: Input column name. Only used if inputCols is not specified. If specified, we raise this column by the exponent. :param inputCols: Input column names. If provided, we raise the first column by the second column. Must have exactly two columns. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param exponent: Optional exponent/power to raise the input to. If not provided, then two input columns are required. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/exponent.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    exponent: Optional[float] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes an ExponentTransformer transformer.\n\n    :param inputCol: Input column name. Only used if inputCols is not specified.\n    If specified, we raise this column by the exponent.\n    :param inputCols: Input column names. If provided, we raise the first column by\n    the second column. Must have exactly two columns.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param exponent: Optional exponent/power to raise the input to. If not provided,\n    then two input columns are required.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(exponent=None)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/exponent/#src.kamae.spark.transformers.exponent.ExponentTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/exponent/#src.kamae.spark.transformers.exponent.ExponentTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which applies the exponent transform to the input column(s).</p> <p>If one column is provided via inputCol, we raise that column to the power of the exponent parameter. If two columns are provided via inputCols, we raise the first column to the power of the second column.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/exponent.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which applies the exponent transform to the input column(s).\n\n    If one column is provided via inputCol, we raise that column to the power of\n    the exponent parameter. If two columns are provided via inputCols,\n    we raise the first column to the power of the second column.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_cols = self.get_multiple_input_cols(\n        constant_param_name=\"exponent\", input_cols_limit=2\n    )\n    # input_cols can contain either actual columns or lit(constants). In order to\n    # determine the datatype of the input columns, we select them from the dataset\n    # first.\n    input_col_names = dataset.select(input_cols).columns\n    input_col_datatypes = [\n        self.get_column_datatype(dataset=dataset.select(input_cols), column_name=c)\n        for c in input_col_names\n    ]\n\n    output_col = multi_input_single_output_scalar_transform(\n        input_cols=input_cols,\n        input_col_names=input_col_names,\n        input_col_datatypes=input_col_datatypes,\n        func=lambda x: F.pow(x[input_col_names[0]], x[input_col_names[1]]),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/exponent/#src.kamae.spark.transformers.exponent.ExponentTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the exp value transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs an exp value operation.</p> Source code in <code>src/kamae/spark/transformers/exponent.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the exp value transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs an exp value operation.\n    \"\"\"\n    return ExponentLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        exponent=self.getExponent(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/exponent/#src.kamae.spark.transformers.exponent.ExponentTransformer.setInputCols","title":"setInputCols","text":"<pre><code>setInputCols(value)\n</code></pre> <p>Overrides setting the input columns for the transformer. Throws an error if we do not have exactly two input columns.</p> <p>:param value: List of input columns. :returns: Class instance.</p> Source code in <code>src/kamae/spark/transformers/exponent.py</code> <pre><code>def setInputCols(self, value: List[str]) -&gt; \"ExponentTransformer\":\n    \"\"\"\n    Overrides setting the input columns for the transformer.\n    Throws an error if we do not have exactly two input columns.\n\n    :param value: List of input columns.\n    :returns: Class instance.\n    \"\"\"\n    if len(value) != 2:\n        raise ValueError(\n            \"\"\"When setting inputCols for ExponentTransformer,\n            there must be exactly two input columns.\"\"\"\n        )\n    return self._set(inputCols=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/hash_index/","title":"hash_index","text":""},{"location":"reference/src/kamae/spark/transformers/hash_index/#src.kamae.spark.transformers.hash_index.HashIndexTransformer","title":"HashIndexTransformer","text":"<pre><code>HashIndexTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    numBins=None,\n    maskValue=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>HashIndexParams</code>, <code>SingleInputSingleOutputParams</code></p> <p>Hash indexer Spark Transformer for use in Spark pipelines. This transformer hashes the input column and then bins it into the specified number of bins using modulo arithmetic.</p> <p>NOTE: If your data contains null characters: https://en.wikipedia.org/wiki/Null_character This transformer could fail since the hashing algorithm uses cannot accept null characters. If you have null characters in your data, you should remove them.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param numBins: Number of bins to use for hash indexing. :param maskValue: Mask value to use for hash indexing. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/hash_index.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    numBins: Optional[int] = None,\n    maskValue: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Instantiates a HashIndexTransformer transformer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param numBins: Number of bins to use for hash indexing.\n    :param maskValue: Mask value to use for hash indexing.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(maskValue=None)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/hash_index/#src.kamae.spark.transformers.hash_index.HashIndexTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/hash_index/#src.kamae.spark.transformers.hash_index.HashIndexTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column named outputCol with the hash indexed input column.</p> <p>:param dataset: Pyspark DataFrame to transform. :returns: Transformed pyspark dataFrame.</p> Source code in <code>src/kamae/spark/transformers/hash_index.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column named outputCol with the\n    hash indexed input column.\n\n    :param dataset: Pyspark DataFrame to transform.\n    :returns: Transformed pyspark dataFrame.\n    \"\"\"\n    num_bins = self.getNumBins()\n    mask_value = self.getMaskValue()\n\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n    output_col = single_input_single_output_scalar_udf_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: hash_udf(label=x, num_bins=num_bins, mask_value=mask_value),\n        udf_return_element_datatype=IntegerType(),\n    )\n\n    return dataset.withColumn(\n        self.getOutputCol(),\n        output_col,\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/hash_index/#src.kamae.spark.transformers.hash_index.HashIndexTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer that performs the hash indexing.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that performs the hash indexing operation.</p> Source code in <code>src/kamae/spark/transformers/hash_index.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer that performs the hash indexing.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter\n    that performs the hash indexing operation.\n    \"\"\"\n    return HashIndexLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        num_bins=self.getNumBins(),\n        mask_value=self.getMaskValue(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/haversine_distance/","title":"haversine_distance","text":""},{"location":"reference/src/kamae/spark/transformers/haversine_distance/#src.kamae.spark.transformers.haversine_distance.HaversineDistanceParams","title":"HaversineDistanceParams","text":"<p>               Bases: <code>LatLonConstantParams</code>, <code>MultiInputSingleOutputParams</code></p> <p>Mixin class containing unit parameters.</p>"},{"location":"reference/src/kamae/spark/transformers/haversine_distance/#src.kamae.spark.transformers.haversine_distance.HaversineDistanceParams.getUnit","title":"getUnit","text":"<pre><code>getUnit()\n</code></pre> <p>Gets the unit parameter. :returns: The unit to use for the distance calculation.</p> Source code in <code>src/kamae/spark/transformers/haversine_distance.py</code> <pre><code>def getUnit(self) -&gt; str:\n    \"\"\"\n    Gets the unit parameter.\n    :returns: The unit to use for the distance calculation.\n    \"\"\"\n    return self.getOrDefault(self.unit)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/haversine_distance/#src.kamae.spark.transformers.haversine_distance.HaversineDistanceParams.setInputCols","title":"setInputCols","text":"<pre><code>setInputCols(value)\n</code></pre> <p>Overrides setting the input columns for the transformer. Throws an error if we do not have either two or four input columns depending on whether latLonConstant is provided. :param value: List of input columns. :returns: Class instance.</p> Source code in <code>src/kamae/spark/transformers/haversine_distance.py</code> <pre><code>def setInputCols(self, value: List[str]) -&gt; \"HaversineDistanceParams\":\n    \"\"\"\n    Overrides setting the input columns for the transformer.\n    Throws an error if we do not have either two or four input columns depending\n    on whether latLonConstant is provided.\n    :param value: List of input columns.\n    :returns: Class instance.\n    \"\"\"\n    if self.getLatLonConstant() is not None and len(value) != 2:\n        raise ValueError(\n            \"\"\"When setting inputCols for HaversineDistanceTransformer,\n            if the latLonConstant is not None,\n            there must be exactly two input columns.\"\"\"\n        )\n    elif len(value) not in [2, 4]:\n        raise ValueError(\n            \"\"\"When setting inputCols for HaversineDistanceTransformer,\n            there must be either two or four input columns.\"\"\"\n        )\n    return self._set(inputCols=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/haversine_distance/#src.kamae.spark.transformers.haversine_distance.HaversineDistanceParams.setUnit","title":"setUnit","text":"<pre><code>setUnit(value)\n</code></pre> <p>Sets the unit parameter. :param value: The unit to use for the distance calculation. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/haversine_distance.py</code> <pre><code>def setUnit(self, value: str) -&gt; \"HaversineDistanceParams\":\n    \"\"\"\n    Sets the unit parameter.\n    :param value: The unit to use for the distance calculation.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if value not in [\"km\", \"miles\"]:\n        raise ValueError(\"unit must be either 'km' or 'miles'\")\n    return self._set(unit=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/haversine_distance/#src.kamae.spark.transformers.haversine_distance.HaversineDistanceTransformer","title":"HaversineDistanceTransformer","text":"<pre><code>HaversineDistanceTransformer(\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    latLonConstant=None,\n    unit=\"km\",\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>HaversineDistanceParams</code></p> <p>Haversine Distance Spark Transformer for use in Spark pipelines. This transformer computes the haversine distance between two lat/lon pairs. This can be between four columns (one for each lat/lon) or between two columns and a constant.</p> <p>The transformer will return null distance if any of the lat/lon values are out of bounds. For lat, this is [-90, 90] and for lon, this is [-180, 180].</p> <p>:param inputCols: Input column names. If latLonConstant is provided, then two input columns are required. These must be in the order [lat, lon]. If latLonConstant is not provided, then four input columns are required. These must be in the order [lat1, lon1, lat2, lon2]. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param latLonConstant: Optional list of lat/lon constant to use. Must be in the order [lat, lon]. If not provided, then four input columns are required. :param unit: The unit to use for the distance calculation. Must be either \"km\" or \"miles\". :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/haversine_distance.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    latLonConstant: Optional[List[float]] = None,\n    unit: str = \"km\",\n) -&gt; None:\n    \"\"\"\n    Initializes an HaversineDistanceTransformer transformer.\n\n    :param inputCols: Input column names. If latLonConstant is provided, then two\n    input columns are required. These must be in the order [lat, lon].\n    If latLonConstant is not provided, then four input columns are required.\n    These must be in the order [lat1, lon1, lat2, lon2].\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param latLonConstant: Optional list of lat/lon constant to use.\n    Must be in the order [lat, lon].\n    If not provided, then four input columns are required.\n    :param unit: The unit to use for the distance calculation.\n    Must be either \"km\" or \"miles\".\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(latLonConstant=None, unit=\"km\")\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n    self.earth_radius = 6371.0 if unit == \"km\" else 3958.8\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/haversine_distance/#src.kamae.spark.transformers.haversine_distance.HaversineDistanceTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/haversine_distance/#src.kamae.spark.transformers.haversine_distance.HaversineDistanceTransformer._get_input_cols","title":"_get_input_cols","text":"<pre><code>_get_input_cols()\n</code></pre> <p>Gets the input columns as a list of pyspark.sql.Column. Also checks if the lat and lons are out of bounds and if so casts them to null. This will make the output null.</p> <p>If the latLonConstant is provided, then returns the input columns and the constant split into lat and lon. Otherwise, returns the input columns.</p> <p>:returns: List of input columns.</p> Source code in <code>src/kamae/spark/transformers/haversine_distance.py</code> <pre><code>def _get_input_cols(self) -&gt; List[Column]:\n    \"\"\"\n    Gets the input columns as a list of pyspark.sql.Column. Also checks if the\n    lat and lons are out of bounds and if so casts them to null.\n    This will make the output null.\n\n    If the latLonConstant is provided, then returns the input columns and the\n    constant split into lat and lon. Otherwise, returns the input columns.\n\n    :returns: List of input columns.\n    \"\"\"\n    input_cols = self.getInputCols()\n\n    lat_lon_constant = self.getLatLonConstant()\n\n    if lat_lon_constant is not None:\n        return [\n            F.col(input_cols[0]).alias(self.uid + \"_lat1\"),\n            F.col(input_cols[1]).alias(self.uid + \"_lon1\"),\n            F.lit(lat_lon_constant[0]).alias(self.uid + \"_lat2\"),\n            F.lit(lat_lon_constant[1]).alias(self.uid + \"_lon2\"),\n        ]\n    else:\n        return [\n            F.col(input_cols[0]).alias(self.uid + \"_lat1\"),\n            F.col(input_cols[1]).alias(self.uid + \"_lon1\"),\n            F.col(input_cols[2]).alias(self.uid + \"_lat2\"),\n            F.col(input_cols[3]).alias(self.uid + \"_lon2\"),\n        ]\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/haversine_distance/#src.kamae.spark.transformers.haversine_distance.HaversineDistanceTransformer._to_radians_col","title":"_to_radians_col  <code>staticmethod</code>","text":"<pre><code>_to_radians_col(x)\n</code></pre> <p>Converts a column of degrees to radians.</p> <p>:param x: Column of degrees. :returns: Column of radians in double precision.</p> Source code in <code>src/kamae/spark/transformers/haversine_distance.py</code> <pre><code>@staticmethod\ndef _to_radians_col(x: Column) -&gt; Column:\n    \"\"\"\n    Converts a column of degrees to radians.\n\n    :param x: Column of degrees.\n    :returns: Column of radians in double precision.\n    \"\"\"\n    return x.cast(DoubleType()) * F.lit(math.pi / 180)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/haversine_distance/#src.kamae.spark.transformers.haversine_distance.HaversineDistanceTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which is the haversine distance between the input lat/lon columns.</p> <p>Returns null if any of the lat/lon values are out of bounds.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/haversine_distance.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which is the haversine distance between the input lat/lon columns.\n\n    Returns null if any of the lat/lon values are out of bounds.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_cols = self._get_input_cols()\n    # input_cols can contain either actual columns or lit(constants). In order to\n    # determine the datatype of the input columns, we select them from the dataset\n    # first.\n    input_col_names = dataset.select(input_cols).columns\n    input_col_datatypes = [\n        self.get_column_datatype(dataset=dataset.select(input_cols), column_name=c)\n        for c in input_col_names\n    ]\n\n    def haversine_distance_transform(\n        x: Column, input_col_names: List[str]\n    ) -&gt; Column:\n        lat1_radians = self._to_radians_col(\n            self.validate_lat_lon_column(x[input_col_names[0]])\n        )\n        lon1_radians = self._to_radians_col(\n            self.validate_lat_lon_column(x[input_col_names[1]], lat=False)\n        )\n        lat2_radians = self._to_radians_col(\n            self.validate_lat_lon_column(x[input_col_names[2]])\n        )\n        lon2_radians = self._to_radians_col(\n            self.validate_lat_lon_column(x[input_col_names[3]], lat=False)\n        )\n\n        lat_diff = lat2_radians - lat1_radians\n        lon_diff = lon2_radians - lon1_radians\n\n        a = F.pow(F.sin(lat_diff / 2.0), 2.0) + F.cos(lat1_radians) * F.cos(\n            lat2_radians\n        ) * F.pow(F.sin(lon_diff / 2.0), 2.0)\n        c = 2.0 * F.asin(F.pow(a, 0.5))\n        r = F.lit(self.earth_radius)\n\n        return c * r\n\n    output_col = multi_input_single_output_scalar_transform(\n        input_cols=input_cols,\n        input_col_names=input_col_names,\n        input_col_datatypes=input_col_datatypes,\n        func=lambda x: haversine_distance_transform(x, input_col_names),\n    )\n\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/haversine_distance/#src.kamae.spark.transformers.haversine_distance.HaversineDistanceTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the haversine distance transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  computes the haversine distance between two lat/lon pairs.</p> Source code in <code>src/kamae/spark/transformers/haversine_distance.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the haversine distance transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     computes the haversine distance between two lat/lon pairs.\n    \"\"\"\n    return HaversineDistanceLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        lat_lon_constant=self.getLatLonConstant(),\n        unit=self.getUnit(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/haversine_distance/#src.kamae.spark.transformers.haversine_distance.HaversineDistanceTransformer.validate_lat_lon_column","title":"validate_lat_lon_column  <code>staticmethod</code>","text":"<pre><code>validate_lat_lon_column(x, lat=True)\n</code></pre> <p>Validates that the input column is a valid lat or lon column. If not, then casts the column to null.</p> <p>:param x: Input column. :param lat: Whether the column is a lat column or not. :returns: Column.</p> Source code in <code>src/kamae/spark/transformers/haversine_distance.py</code> <pre><code>@staticmethod\ndef validate_lat_lon_column(x: Column, lat: bool = True) -&gt; Column:\n    \"\"\"\n    Validates that the input column is a valid lat or lon column.\n    If not, then casts the column to null.\n\n    :param x: Input column.\n    :param lat: Whether the column is a lat column or not.\n    :returns: Column.\n    \"\"\"\n    cond = x.between(-90.0, 90.0) if lat else x.between(-180.0, 180.0)\n    return F.when(cond, x)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/identity/","title":"identity","text":""},{"location":"reference/src/kamae/spark/transformers/identity/#src.kamae.spark.transformers.identity.IdentityTransformer","title":"IdentityTransformer","text":"<pre><code>IdentityTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code></p> <p>IdentityLayer Spark Transformer for use in Spark pipelines. This transformer simply passes the input to the output unchanged. Used for cases where you want to keep the input the same.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/identity.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes an IdentityTransformer transformer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/identity/#src.kamae.spark.transformers.identity.IdentityTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/identity/#src.kamae.spark.transformers.identity.IdentityTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which is the same as the column with name <code>inputCol</code>.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/identity.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which is the same as the column with name `inputCol`.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    return dataset.withColumn(self.getOutputCol(), F.col(self.getInputCol()))\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/identity/#src.kamae.spark.transformers.identity.IdentityTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the identity transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs an IdentityLayer operation.</p> Source code in <code>src/kamae/spark/transformers/identity.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the identity transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs an IdentityLayer operation.\n    \"\"\"\n    # Tensorflow &lt;= 2.11 does not contain tf.keras.layers.IdentityLayer\n    # so we use a lambda layer instead.\n    # When we have a subclassed identity layer, we can use that.\n    return IdentityLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/if_statement/","title":"if_statement","text":""},{"location":"reference/src/kamae/spark/transformers/if_statement/#src.kamae.spark.transformers.if_statement.IfStatementParams","title":"IfStatementParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing parameters needed for IfStatementTransformer transform layers.</p>"},{"location":"reference/src/kamae/spark/transformers/if_statement/#src.kamae.spark.transformers.if_statement.IfStatementParams.getConditionOperator","title":"getConditionOperator","text":"<pre><code>getConditionOperator()\n</code></pre> <p>Gets the conditionOperator parameter.</p> <p>:returns: String value describing the operator to use in condition: - eq - neq - lt - gt - leq - geq</p> Source code in <code>src/kamae/spark/transformers/if_statement.py</code> <pre><code>def getConditionOperator(self) -&gt; str:\n    \"\"\"\n    Gets the conditionOperator parameter.\n\n    :returns: String value describing the operator to use in condition:\n    - eq\n    - neq\n    - lt\n    - gt\n    - leq\n    - geq\n    \"\"\"\n    return self.getOrDefault(self.conditionOperator)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/if_statement/#src.kamae.spark.transformers.if_statement.IfStatementParams.getResultIfFalse","title":"getResultIfFalse","text":"<pre><code>getResultIfFalse()\n</code></pre> <p>Gets the resultIfFalse parameter.</p> <p>:returns: Value to return if condition is false.</p> Source code in <code>src/kamae/spark/transformers/if_statement.py</code> <pre><code>def getResultIfFalse(self) -&gt; Union[float, int, str, bool]:\n    \"\"\"\n    Gets the resultIfFalse parameter.\n\n    :returns: Value to return if condition is false.\n    \"\"\"\n    return self.getOrDefault(self.resultIfFalse)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/if_statement/#src.kamae.spark.transformers.if_statement.IfStatementParams.getResultIfTrue","title":"getResultIfTrue","text":"<pre><code>getResultIfTrue()\n</code></pre> <p>Gets the resultIfTrue parameter.</p> <p>:returns: Value to return if condition is true.</p> Source code in <code>src/kamae/spark/transformers/if_statement.py</code> <pre><code>def getResultIfTrue(self) -&gt; Union[float, int, str, bool]:\n    \"\"\"\n    Gets the resultIfTrue parameter.\n\n    :returns: Value to return if condition is true.\n    \"\"\"\n    return self.getOrDefault(self.resultIfTrue)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/if_statement/#src.kamae.spark.transformers.if_statement.IfStatementParams.getValueToCompare","title":"getValueToCompare","text":"<pre><code>getValueToCompare()\n</code></pre> <p>Gets the valueToCompare parameter.</p> <p>:returns: Value to compare to input column.</p> Source code in <code>src/kamae/spark/transformers/if_statement.py</code> <pre><code>def getValueToCompare(self) -&gt; Union[float, int, str, bool]:\n    \"\"\"\n    Gets the valueToCompare parameter.\n\n    :returns: Value to compare to input column.\n    \"\"\"\n    return self.getOrDefault(self.valueToCompare)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/if_statement/#src.kamae.spark.transformers.if_statement.IfStatementParams.setConditionOperator","title":"setConditionOperator","text":"<pre><code>setConditionOperator(value)\n</code></pre> <p>Sets the conditionOperator parameter.</p> <p>:param value: String value describing the operator to use in condition: - eq - neq - lt - gt - leq - geq :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/if_statement.py</code> <pre><code>def setConditionOperator(self, value: str) -&gt; \"IfStatementParams\":\n    \"\"\"\n    Sets the conditionOperator parameter.\n\n    :param value: String value describing the operator to use in condition:\n    - eq\n    - neq\n    - lt\n    - gt\n    - leq\n    - geq\n    :returns: Instance of class mixed in.\n    \"\"\"\n    allowed_operators = [\"eq\", \"neq\", \"lt\", \"gt\", \"leq\", \"geq\"]\n    if value not in allowed_operators:\n        raise ValueError(\n            f\"conditionOperator must be one of {allowed_operators}, but got {value}\"\n        )\n    if (\n        self.getValueToCompare() is not None\n        and not isinstance(self.getValueToCompare(), Number)\n        and value in [\"lt\", \"leq\", \"gt\", \"geq\"]\n    ):\n        raise ValueError(\n            \"\"\"conditionOperator must be eq or neq when\n            valueToCompare is not a number.\"\"\"\n        )\n    return self._set(conditionOperator=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/if_statement/#src.kamae.spark.transformers.if_statement.IfStatementParams.setResultIfFalse","title":"setResultIfFalse","text":"<pre><code>setResultIfFalse(value)\n</code></pre> <p>Sets the resultIfFalse parameter.</p> <p>:param value: Value to return if condition is false. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/if_statement.py</code> <pre><code>def setResultIfFalse(\n    self, value: Union[float, int, str, bool]\n) -&gt; \"IfStatementParams\":\n    \"\"\"\n    Sets the resultIfFalse parameter.\n\n    :param value: Value to return if condition is false.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(resultIfFalse=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/if_statement/#src.kamae.spark.transformers.if_statement.IfStatementParams.setResultIfTrue","title":"setResultIfTrue","text":"<pre><code>setResultIfTrue(value)\n</code></pre> <p>Sets the resultIfTrue parameter.</p> <p>:param value: Value to return if condition is true. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/if_statement.py</code> <pre><code>def setResultIfTrue(\n    self, value: Union[float, int, str, bool]\n) -&gt; \"IfStatementParams\":\n    \"\"\"\n    Sets the resultIfTrue parameter.\n\n    :param value: Value to return if condition is true.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(resultIfTrue=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/if_statement/#src.kamae.spark.transformers.if_statement.IfStatementParams.setValueToCompare","title":"setValueToCompare","text":"<pre><code>setValueToCompare(value)\n</code></pre> <p>Sets the valueToCompare parameter.</p> <p>:param value: Value to compare to input column. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/if_statement.py</code> <pre><code>def setValueToCompare(\n    self, value: Union[float, int, str, bool]\n) -&gt; \"IfStatementParams\":\n    \"\"\"\n    Sets the valueToCompare parameter.\n\n    :param value: Value to compare to input column.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if (\n        self.isDefined(\"conditionOperator\")\n        and not isinstance(value, Number)\n        and self.getConditionOperator() in [\"lt\", \"leq\", \"gt\", \"geq\"]\n    ):\n        raise TypeError(\n            \"\"\"valueToCompare must be a number if conditionOperator is one of\n            lt, leq, gt, or geq.\"\"\"\n        )\n    return self._set(valueToCompare=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/if_statement/#src.kamae.spark.transformers.if_statement.IfStatementTransformer","title":"IfStatementTransformer","text":"<pre><code>IfStatementTransformer(\n    inputCol=None,\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    conditionOperator=None,\n    valueToCompare=None,\n    resultIfTrue=None,\n    resultIfFalse=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>MultiInputSingleOutputParams</code>, <code>IfStatementParams</code></p> <p>IfStatement Spark Transformer for use in Spark pipelines. This transformer computes an if statement between a set of constants and columns.</p> <p>:param inputCol: Input column name. Only used if inputCols is not specified. If specified, then all other aspects of the if statement are constant. :param inputCols: Input column names. List of input columns to in the case where the if statement is not constant. Must be specified in the order [valueToCompare, resultIfTrue, resultIfFalse]. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param conditionOperator: Operator to use in condition: eq, neq, lt, gt, leq, geq. :param valueToCompare: Optional value to compare to input column. If not specified, then assumed to be the first input column. :param resultIfTrue: Optional value to return if condition is true. If not specified, then assumed to be the second input column. :param resultIfFalse: Optional value to return if condition is false. If not specified, then assumed to be the third input column. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/if_statement.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    conditionOperator: Optional[str] = None,\n    valueToCompare: Optional[Union[float, int, str, bool]] = None,\n    resultIfTrue: Optional[Union[float, int, str, bool]] = None,\n    resultIfFalse: Optional[Union[float, int, str, bool]] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes an IfStatementTransformer transformer.\n\n    :param inputCol: Input column name. Only used if inputCols is not specified.\n    If specified, then all other aspects of the if statement are constant.\n    :param inputCols: Input column names. List of input columns to in the case\n    where the if statement is not constant. Must be specified in the order\n    [valueToCompare, resultIfTrue, resultIfFalse].\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param conditionOperator: Operator to use in condition:\n    eq, neq, lt, gt, leq, geq.\n    :param valueToCompare: Optional value to compare to input column.\n    If not specified, then assumed to be the first input column.\n    :param resultIfTrue: Optional value to return if condition is true.\n    If not specified, then assumed to be the second input column.\n    :param resultIfFalse: Optional value to return if condition is false.\n    If not specified, then assumed to be the third input column.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(\n        valueToCompare=None,\n        resultIfTrue=None,\n        resultIfFalse=None,\n    )\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/if_statement/#src.kamae.spark.transformers.if_statement.IfStatementTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/if_statement/#src.kamae.spark.transformers.if_statement.IfStatementTransformer._construct_input_cols","title":"_construct_input_cols","text":"<pre><code>_construct_input_cols()\n</code></pre> <p>Constructs the input columns for the transformer. This is fairly complex since the user can set any of the following: - valueToCompare - resultIfTrue - resultIfFalse</p> <p>If all of these are set, then the user should have provided a single <code>inputCol</code>. Otherwise, if any of these are not set, then the user should have provided <code>inputCols</code> containing the missing values. The <code>inputCols</code> should be in the order [valueToCompare, resultIfTrue, resultIfFalse]. But if the user has specified, for example, <code>resultIfTrue</code> as a constant then the <code>inputCols</code> should be in the order [valueToCompare, resultIfFalse].</p> <p>:returns: Tuple of 4 pyspark columns.</p> Source code in <code>src/kamae/spark/transformers/if_statement.py</code> <pre><code>def _construct_input_cols(self) -&gt; List[Column]:\n    \"\"\"\n    Constructs the input columns for the transformer. This is fairly complex\n    since the user can set any of the following:\n    - valueToCompare\n    - resultIfTrue\n    - resultIfFalse\n\n    If all of these are set, then the user should have provided a single `inputCol`.\n    Otherwise, if any of these are not set, then the user should have provided\n    `inputCols` containing the missing values. The `inputCols` should be in the\n    order [valueToCompare, resultIfTrue, resultIfFalse]. But if the user has\n    specified, for example, `resultIfTrue` as a constant then the\n    `inputCols` should be in the order [valueToCompare, resultIfFalse].\n\n    :returns: Tuple of 4 pyspark columns.\n    \"\"\"\n    optional_constant_cols = [\"valueToCompare\", \"resultIfTrue\", \"resultIfFalse\"]\n    optional_constants_defined = {\n        const: self.getOrDefault(const) is not None\n        for const in optional_constant_cols\n    }\n\n    if self.isDefined(\"inputCols\"):\n        # If the user has set inputCols, then some or all of the optional constants\n        # are defined as input column variables\n        input_cols = self.getInputCols()\n        if len(input_cols) + sum(optional_constants_defined.values()) != 4:\n            raise ValueError(\n                f\"\"\"Total number of input columns and constants must be 4,\n                but got {len(input_cols)} input columns and\n                {sum(optional_constants_defined.values())} constants.\"\"\"\n            )\n        # The first input column is always the value to compare\n        input_col_list = [\n            F.col(input_cols[0]),\n        ]\n        input_col_counter = 1\n        for const_col in optional_constant_cols:\n            # Loop through the optional constant names.\n            # If the constant is not defined then it must be an input column.\n            # Otherwise, it is a literal value.\n            if not optional_constants_defined[const_col]:\n                input_col_list.append(F.col(input_cols[input_col_counter]))\n                input_col_counter += 1\n            else:\n                input_col_list.append(F.lit(self.getOrDefault(const_col)))\n        return [\n            input_col_list[0].alias(self.uid + \"_inputCol\"),\n            input_col_list[1].alias(self.uid + \"_valueToCompare\"),\n            input_col_list[2].alias(self.uid + \"_resultIfTrue\"),\n            input_col_list[3].alias(self.uid + \"_resultIfFalse\"),\n        ]\n    elif self.isDefined(\"inputCol\"):\n        # If the user has set inputCol, then all the optional constants\n        # must be defined.\n        if not all(\n            [\n                self.getOrDefault(const) is not None\n                for const in optional_constant_cols\n            ]\n        ):\n            raise ValueError(\n                f\"\"\"Must specify all of {optional_constant_cols}\"\n                if inputCol is specified.\"\"\"\n            )\n        return [\n            F.col(self.getInputCol()).alias(self.uid + \"_inputCol\"),\n            F.lit(self.getValueToCompare()).alias(self.uid + \"_valueToCompare\"),\n            F.lit(self.getResultIfTrue()).alias(self.uid + \"_resultIfTrue\"),\n            F.lit(self.getResultIfFalse()).alias(self.uid + \"_resultIfFalse\"),\n        ]\n    else:\n        raise ValueError(\"Must specify either inputCol or inputCols.\")\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/if_statement/#src.kamae.spark.transformers.if_statement.IfStatementTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which is the output of an if statement with condition:</p> <p>IF <code>inputCol</code> <code>conditionOperator</code> <code>valueToCompare</code> THEN <code>resultIfTrue</code> ELSE <code>resultIfFalse</code></p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/if_statement.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which is the output of an if statement with condition:\n\n    IF `inputCol` `conditionOperator` `valueToCompare`\n    THEN `resultIfTrue`\n    ELSE `resultIfFalse`\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_cols = self._construct_input_cols()\n    # input_cols can contain either actual columns or lit(constants). In order to\n    # determine the datatype of the input columns, we select them from the dataset\n    # first.\n    input_col_names = dataset.select(input_cols).columns\n    input_col_datatypes = [\n        self.get_column_datatype(dataset=dataset.select(input_cols), column_name=c)\n        for c in input_col_names\n    ]\n    condition_operator = get_condition_operator(self.getConditionOperator())\n\n    output_col = multi_input_single_output_scalar_transform(\n        input_cols=input_cols,\n        input_col_names=input_col_names,\n        input_col_datatypes=input_col_datatypes,\n        func=lambda x: F.when(\n            condition_operator(x[input_col_names[0]], x[input_col_names[1]]),\n            x[input_col_names[2]],\n        ).otherwise(x[input_col_names[3]]),\n    )\n\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/if_statement/#src.kamae.spark.transformers.if_statement.IfStatementTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the numerical if statement transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs the numerical if statement.</p> Source code in <code>src/kamae/spark/transformers/if_statement.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the numerical if statement transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs the numerical if statement.\n    \"\"\"\n    if not self.isDefined(\"conditionOperator\"):\n        raise ValueError(\"Must specify conditionOperator to use tensorflow layer.\")\n\n    return IfStatementLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        condition_operator=self.getConditionOperator(),\n        value_to_compare=self.getValueToCompare(),\n        result_if_true=self.getResultIfTrue(),\n        result_if_false=self.getResultIfFalse(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/if_statement/#src.kamae.spark.transformers.if_statement.IfStatementTransformer.setInputCols","title":"setInputCols","text":"<pre><code>setInputCols(value)\n</code></pre> <p>Sets the inputCols parameter, throwing an error if the total length of the inputCols and the constants are more than 4.</p> <p>:param value: List of input column names. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/if_statement.py</code> <pre><code>def setInputCols(self, value: List[str]) -&gt; \"IfStatementTransformer\":\n    \"\"\"\n    Sets the inputCols parameter, throwing an error if the total length of the\n    inputCols and the constants are more than 4.\n\n    :param value: List of input column names.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    optional_constant_cols = [\"valueToCompare\", \"resultIfTrue\", \"resultIfFalse\"]\n    num_defined_constants = len(\n        [c for c in optional_constant_cols if self.getOrDefault(c) is not None]\n    )\n    if len(value) + num_defined_constants &gt; 4:\n        raise ValueError(\n            f\"\"\"Total number of input columns and constants cannot be more than 4,\n            but got {len(value)} input columns and\n            {num_defined_constants} constants.\"\"\"\n        )\n\n    return self._set(inputCols=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/impute/","title":"impute","text":""},{"location":"reference/src/kamae/spark/transformers/impute/#src.kamae.spark.transformers.impute.ImputeParams","title":"ImputeParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class used to provide imputation and mask value needed for imputation.</p>"},{"location":"reference/src/kamae/spark/transformers/impute/#src.kamae.spark.transformers.impute.ImputeParams.getImputeValue","title":"getImputeValue","text":"<pre><code>getImputeValue()\n</code></pre> <p>Gets the imputeValue parameter.</p> <p>:returns: String, float or int imputeValue</p> Source code in <code>src/kamae/spark/transformers/impute.py</code> <pre><code>def getImputeValue(self) -&gt; Union[float, int, str]:\n    \"\"\"\n    Gets the imputeValue parameter.\n\n    :returns: String, float or int imputeValue\n    \"\"\"\n    return self.getOrDefault(self.imputeValue)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/impute/#src.kamae.spark.transformers.impute.ImputeParams.getMaskValue","title":"getMaskValue","text":"<pre><code>getMaskValue()\n</code></pre> <p>Gets the maskValue parameter. :returns: String, float or int value of the mask value.</p> Source code in <code>src/kamae/spark/transformers/impute.py</code> <pre><code>def getMaskValue(self) -&gt; Union[float, int, str]:\n    \"\"\"\n    Gets the maskValue parameter.\n    :returns: String, float or int value of the mask value.\n    \"\"\"\n    return self.getOrDefault(self.maskValue)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/impute/#src.kamae.spark.transformers.impute.ImputeParams.setImputeValue","title":"setImputeValue","text":"<pre><code>setImputeValue(value)\n</code></pre> <p>Sets the parameter imputeValue to the given scalar value.</p> <p>:param value: List of imputeValues values. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/impute.py</code> <pre><code>def setImputeValue(self, value: Union[float, int, str]) -&gt; \"ImputeParams\":\n    \"\"\"\n    Sets the parameter imputeValue to the given scalar value.\n\n    :param value: List of imputeValues values.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if value is None:\n        raise ValueError(\"Impute value cannot be None\")\n    return self._set(imputeValue=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/impute/#src.kamae.spark.transformers.impute.ImputeParams.setMaskValue","title":"setMaskValue","text":"<pre><code>setMaskValue(value)\n</code></pre> <p>Sets the maskValue parameter.</p> <p>:param value: Float value to use as the mask value. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/impute.py</code> <pre><code>def setMaskValue(self, value: Union[float, int, str]) -&gt; \"ImputeParams\":\n    \"\"\"\n    Sets the maskValue parameter.\n\n    :param value: Float value to use as the mask value.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(maskValue=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/impute/#src.kamae.spark.transformers.impute.ImputeTransformer","title":"ImputeTransformer","text":"<pre><code>ImputeTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    imputeValue=None,\n    maskValue=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>ImputeParams</code>, <code>SingleInputSingleOutputParams</code></p> <p>Imputation transformer for use in Spark pipelines. This is used to impute the mean or median value when value is null or equalling a mask</p> <p>:param inputCol: Input column name to impute over. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. :param imputeValue: String, float or int value to impute in place of mask or nulls. :param maskValue: String, float or int value which should be ignored when computing the imputation statistic and to be replaced by the imputation. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/impute.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    imputeValue: Optional[Union[float, int, str]] = None,\n    maskValue: Optional[Union[float, int, str]] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes an ImputeTransformer transformer.\n\n    :param inputCol: Input column name to impute over.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model.\n    :param imputeValue: String, float or int value to impute in place of mask or\n    nulls.\n    :param maskValue: String, float or int value which should be ignored when\n    computing the imputation statistic and to be replaced by the imputation.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/impute/#src.kamae.spark.transformers.impute.ImputeTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/impute/#src.kamae.spark.transformers.impute.ImputeTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input data by imputing the specified statistic of the input column provided. Nulls and any values equalling the mask are imputed over.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Pyspark dataframe with the input columns imputed with the specified statistic, and names set to output column names provided</p> Source code in <code>src/kamae/spark/transformers/impute.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input data by imputing the specified\n    statistic of the input column provided.\n    Nulls and any values equalling the mask are imputed over.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Pyspark dataframe with the input columns\n    imputed with the specified statistic, and names set to\n    output column names provided\n    \"\"\"\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n    output_col = single_input_single_output_scalar_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: F.when(\n            (x == F.lit(self.getMaskValue())) | (x.isNull()),\n            F.lit(self.getImputeValue()),\n        ).otherwise(x),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/impute/#src.kamae.spark.transformers.impute.ImputeTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the imputation transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter  that performs the imputation.</p> Source code in <code>src/kamae/spark/transformers/impute.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the imputation transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter\n     that performs the imputation.\n    \"\"\"\n    mask_value = self.getMaskValue()\n\n    return ImputeLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        impute_value=self.getImputeValue(),\n        mask_value=mask_value,\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/lambda_function/","title":"lambda_function","text":""},{"location":"reference/src/kamae/spark/transformers/lambda_function/#src.kamae.spark.transformers.lambda_function.LambdaFunctionParams","title":"LambdaFunctionParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing parameters needed for the LambdaFunctionTransformer.</p>"},{"location":"reference/src/kamae/spark/transformers/lambda_function/#src.kamae.spark.transformers.lambda_function.LambdaFunctionParams.getFunction","title":"getFunction","text":"<pre><code>getFunction()\n</code></pre> <p>Gets the lambda function to apply to the input column.</p> <p>:returns: Lambda function to apply to the input column.</p> Source code in <code>src/kamae/spark/transformers/lambda_function.py</code> <pre><code>def getFunction(self) -&gt; Callable[[Any], Any]:\n    \"\"\"\n    Gets the lambda function to apply to the input column.\n\n    :returns: Lambda function to apply to the input column.\n    \"\"\"\n    utf_str = self.getOrDefault(self.function)\n    return dill.loads(base64.b64decode(utf_str))\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/lambda_function/#src.kamae.spark.transformers.lambda_function.LambdaFunctionParams.getFunctionReturnTypes","title":"getFunctionReturnTypes","text":"<pre><code>getFunctionReturnTypes()\n</code></pre> <p>Gets the return type of the lambda function.</p> <p>:returns: Return type of the lambda function.</p> Source code in <code>src/kamae/spark/transformers/lambda_function.py</code> <pre><code>def getFunctionReturnTypes(self) -&gt; List[DataType]:\n    \"\"\"\n    Gets the return type of the lambda function.\n\n    :returns: Return type of the lambda function.\n    \"\"\"\n    utf_str_list = self.getOrDefault(self.functionReturnTypes)\n    return [dill.loads(base64.b64decode(utf_str)) for utf_str in utf_str_list]\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/lambda_function/#src.kamae.spark.transformers.lambda_function.LambdaFunctionParams.setFunction","title":"setFunction","text":"<pre><code>setFunction(value)\n</code></pre> <p>Sets the lambda function to apply to the input column.</p> <p>:param value: Lambda function to apply to the input column. :returns: Class instance.</p> Source code in <code>src/kamae/spark/transformers/lambda_function.py</code> <pre><code>def setFunction(self, value: Callable[[Any], Any]) -&gt; \"LambdaFunctionParams\":\n    \"\"\"\n    Sets the lambda function to apply to the input column.\n\n    :param value: Lambda function to apply to the input column.\n    :returns: Class instance.\n    \"\"\"\n    dill_bytes = dill.dumps(value)\n    return self._set(function=base64.b64encode(dill_bytes).decode(\"utf-8\"))\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/lambda_function/#src.kamae.spark.transformers.lambda_function.LambdaFunctionParams.setFunctionReturnTypes","title":"setFunctionReturnTypes","text":"<pre><code>setFunctionReturnTypes(value)\n</code></pre> <p>Sets the return type of the lambda function.</p> <p>:param value: Return type of the lambda function. :returns: Class instance.</p> Source code in <code>src/kamae/spark/transformers/lambda_function.py</code> <pre><code>def setFunctionReturnTypes(self, value: List[DataType]) -&gt; \"LambdaFunctionParams\":\n    \"\"\"\n    Sets the return type of the lambda function.\n\n    :param value: Return type of the lambda function.\n    :returns: Class instance.\n    \"\"\"\n    serialised_dtypes = []\n    for return_type in value:\n        dill_bytes = dill.dumps(return_type)\n        serialised_dtypes.append(base64.b64encode(dill_bytes).decode(\"utf-8\"))\n\n    return self._set(functionReturnTypes=serialised_dtypes)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/lambda_function/#src.kamae.spark.transformers.lambda_function.LambdaFunctionTransformer","title":"LambdaFunctionTransformer","text":"<pre><code>LambdaFunctionTransformer(\n    inputCol=None,\n    inputCols=None,\n    outputCol=None,\n    outputCols=None,\n    function=None,\n    functionReturnTypes=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>SingleInputMultiOutputParams</code>, <code>MultiInputSingleOutputParams</code>, <code>MultiInputMultiOutputParams</code>, <code>LambdaFunctionParams</code></p> <p>Spark Transformer that applies tensorflow lambda functions to the input column(s).</p> <p>The provided function must either take a single tensor as input or take a list of tensors as input. Same for the output, either single tensor or list of tensors. In the case of multiple input/output, it is assumed that the order matches the order of the input/output columns.</p> <p>It can only use tensorflow methods, and reference them with <code>tf.</code>.</p> <p>An example of a valid lambda function is:</p> <pre><code>def my_tf_fn(x):\n    return tf.square(x) - tf.math.log(x)\n</code></pre> <p>Note: No validation is done on the lambda function. It is up to the user to ensure the lambda function is valid and works as expected. The lambda function must be serializable by dill: https://github.com/uqfoundation/dill</p> <p>The function will be used within a User Defined Function (UDF) in Spark. You need to provide the return type for the UDF computation as a string. E.g. \"float\", \"array\" etc. If you require the operation to be quicker (in Spark), consider composing it from other transformers, that are written in native Spark functions. <p>:param inputCol: Input column name. :param inputCols: List of input column names. Used for multiple input columns. inputCols and inputCol cannot be set at the same time. :param outputCol: Output column name. :param outputCols: List of output column names. Used for multiple output columns. outputCols and outputCol cannot be set at the same time. :param function: Lambda function to apply to the input column. If single input, the function should take a single tensor as input and return a single tensor. If multiple input, the function should take a list of tensors as input and return a single tensor. :param functionReturnTypes: Return type(s) of the lambda function. List of Pyspark datatypes. Used to understand the UDF return type in Spark. Keras layer does not use this. E.g. \"float\", \"array\" etc. If outputCol is set, this should be a list of length 1. If outputCols is set, this should be a list of the same length as outputCols. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :returns: None - class instantiated. Source code in <code>src/kamae/spark/transformers/lambda_function.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    outputCols: Optional[List[str]] = None,\n    function: Optional[\n        Callable[[Union[Tensor, List[Tensor]]], Union[Tensor, List[Tensor]]]\n    ] = None,\n    functionReturnTypes: Optional[List[DataType]] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes an LambdaFunctionTransformer transformer.\n\n    :param inputCol: Input column name.\n    :param inputCols: List of input column names. Used for multiple input columns.\n    inputCols and inputCol cannot be set at the same time.\n    :param outputCol: Output column name.\n    :param outputCols: List of output column names. Used for multiple output\n    columns. outputCols and outputCol cannot be set at the same time.\n    :param function: Lambda function to apply to the input column. If single input,\n    the function should take a single tensor as input and return a single tensor.\n    If multiple input, the function should take a list of tensors as input and\n    return a single tensor.\n    :param functionReturnTypes: Return type(s) of the lambda function. List of\n    Pyspark datatypes. Used to understand the UDF return type in Spark.\n    Keras layer does not use this. E.g. \"float\", \"array&lt;string&gt;\" etc. If outputCol\n    is set, this should be a list of length 1. If outputCols is set, this should be\n    a list of the same length as outputCols.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/lambda_function/#src.kamae.spark.transformers.lambda_function.LambdaFunctionTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/lambda_function/#src.kamae.spark.transformers.lambda_function.LambdaFunctionTransformer._apply_udf_func_to_dataset","title":"_apply_udf_func_to_dataset  <code>staticmethod</code>","text":"<pre><code>_apply_udf_func_to_dataset(\n    dataset,\n    func,\n    input_col_names,\n    output_col_names,\n    function_return_types,\n)\n</code></pre> <p>Gets and applies the udf to the dataset. If the output is a single column, the output is directly applied to the dataset. If the output is multiple columns, a struct column is created and then the columns are extracted.</p> <p>:param dataset: Pyspark dataframe to transform. :param func: Tensorflow function. :param input_col_names: List of input column names. :param output_col_names: List of output column names. :param function_return_types: List of return types of the lambda function. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/lambda_function.py</code> <pre><code>@staticmethod\ndef _apply_udf_func_to_dataset(\n    dataset: DataFrame,\n    func: Callable[[Union[Tensor, List[Tensor]]], Any],\n    input_col_names: List[str],\n    output_col_names: List[str],\n    function_return_types: List[DataType],\n) -&gt; DataFrame:\n    \"\"\"\n    Gets and applies the udf to the dataset. If the output is a single column, the\n    output is directly applied to the dataset. If the output is multiple columns,\n    a struct column is created and then the columns are extracted.\n\n    :param dataset: Pyspark dataframe to transform.\n    :param func: Tensorflow function.\n    :param input_col_names: List of input column names.\n    :param output_col_names: List of output column names.\n    :param function_return_types: List of return types of the lambda function.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    if len(output_col_names) == 1:\n        # Single output scenario\n        output_col = F.udf(func, function_return_types[0])(\n            *[F.col(c) for c in input_col_names]\n        )\n        return dataset.withColumn(output_col_names[0], output_col)\n    else:\n        # Multi-output scenario. UDF cannot return multiple columns, so\n        # return a struct type and then extract the columns.\n        udf_return_type = StructType(\n            [\n                StructField(name=output_col, dataType=function_return_types[i])\n                for i, output_col in enumerate(output_col_names)\n            ]\n        )\n        output_struct_col = F.udf(func, udf_return_type)(\n            *[F.col(c) for c in input_col_names]\n        )\n        for output_col in output_col_names:\n            dataset = dataset.withColumn(output_col, output_struct_col[output_col])\n        return dataset\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/lambda_function/#src.kamae.spark.transformers.lambda_function.LambdaFunctionTransformer._get_input_cols","title":"_get_input_cols","text":"<pre><code>_get_input_cols()\n</code></pre> <p>Gets the input columns for the transformer.</p> <p>:returns: List of input column names.</p> Source code in <code>src/kamae/spark/transformers/lambda_function.py</code> <pre><code>def _get_input_cols(self) -&gt; List[str]:\n    \"\"\"\n    Gets the input columns for the transformer.\n\n    :returns: List of input column names.\n    \"\"\"\n    return (\n        self.getInputCols() if self.isDefined(\"inputCols\") else [self.getInputCol()]\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/lambda_function/#src.kamae.spark.transformers.lambda_function.LambdaFunctionTransformer._get_output_cols","title":"_get_output_cols","text":"<pre><code>_get_output_cols()\n</code></pre> <p>Gets the output columns for the transformer.</p> <p>:returns: List of output column names.</p> Source code in <code>src/kamae/spark/transformers/lambda_function.py</code> <pre><code>def _get_output_cols(self) -&gt; List[str]:\n    \"\"\"\n    Gets the output columns for the transformer.\n\n    :returns: List of output column names.\n    \"\"\"\n    return (\n        self.getOutputCols()\n        if self.isDefined(\"outputCols\")\n        else [self.getOutputCol()]\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/lambda_function/#src.kamae.spark.transformers.lambda_function.LambdaFunctionTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which applies <code>function</code> to the input column(s).</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/lambda_function.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which applies `function` to the input column(s).\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    self._validate_params()\n\n    input_col_names = self._get_input_cols()\n    output_col_names = self._get_output_cols()\n    function_return_types = self.getFunctionReturnTypes()\n\n    def tf_function_wrapper(\n        fn: Callable[[Union[Tensor, List[Tensor]]], Tensor],\n    ) -&gt; Callable[[Union[Tensor, List[Tensor]]], Any]:\n        \"\"\"\n        Wraps the tensorflow function, so we can use it within a Spark UDF.\n\n        Specifically:\n\n        - Handles the fact that Spark UDFs require multiple inputs to be passed as\n        arguments, not in a list. However, the tensorflow function expects multiple\n        inputs to be passed as a list.\n        - Wraps scalar inputs in a list before creating the tensor. The tensor also\n        has a single batch dimension. Therefore, a scalar value has shape (1, 1)\n        and an array has shape (1, N).\n        - Converts the output tensor to a numpy value and extracts the single batch.\n        If value is a list of size 1, return the single value.\n        - If the output tensor is a string, decodes the bytes to a string.\n\n        :param fn: Tensorflow function.\n        :returns: Function that can be used within a Spark UDF.\n        \"\"\"\n\n        def wrapper(*args: Any) -&gt; Union[Any, tuple[Any, ...]]:\n            # Wrap args in a list if they are not already a list. This ensures\n            # that scalar inputs have shape (1, 1) in tensorflow.\n            args = [a if isinstance(a, list) else [a] for a in args]\n            # Call the function with the input tensors.\n            if len(args) == 1:\n                tf_val: Union[Tensor, list[Tensor]] = fn(tf.constant([args[0]]))\n            else:\n                tf_val: Union[Tensor, list[Tensor]] = fn(\n                    [tf.constant([a]) for a in args]\n                )\n\n            # Single output scenario\n            if len(function_return_types) == 1:\n                if \"string\" in function_return_types[0].simpleString():\n                    # If string output, decode the bytes to a string.\n                    output = tf_val.numpy().astype(str).tolist()[0]\n                else:\n                    output = tf_val.numpy().tolist()[0]\n                if len(output) == 1 and not isinstance(\n                    function_return_types[0], ArrayType\n                ):\n                    return output[0]\n                return output\n            # Multi output scenario\n            else:\n                tf_outputs = []\n                for i, tensor in enumerate(tf_val):\n                    if \"string\" in function_return_types[i].simpleString():\n                        # If string output, decode the bytes to a string.\n                        output = tensor.numpy().astype(str).tolist()[0]\n                    else:\n                        output = tensor.numpy().tolist()[0]\n                    if len(output) == 1 and not isinstance(\n                        function_return_types[i], ArrayType\n                    ):\n                        # If the output is a scalar value, extract it from the list.\n                        tf_outputs.append(output[0])\n                    else:\n                        tf_outputs.append(output)\n                # Return a tuple, which will be unpacked into multiple columns.\n                return tuple(tf_outputs)\n\n        return wrapper\n\n    func = tf_function_wrapper(self.getFunction())\n\n    return self._apply_udf_func_to_dataset(\n        dataset=dataset,\n        func=func,\n        input_col_names=input_col_names,\n        output_col_names=output_col_names,\n        function_return_types=function_return_types,\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/lambda_function/#src.kamae.spark.transformers.lambda_function.LambdaFunctionTransformer._validate_params","title":"_validate_params","text":"<pre><code>_validate_params()\n</code></pre> <p>Validates the parameters of the LambdaFunctionTransformer.</p> Source code in <code>src/kamae/spark/transformers/lambda_function.py</code> <pre><code>def _validate_params(self) -&gt; None:\n    \"\"\"\n    Validates the parameters of the LambdaFunctionTransformer.\n    \"\"\"\n    if not (self.isDefined(\"function\") and self.isDefined(\"functionReturnTypes\")):\n        raise ValueError(\"function and functionReturnTypes must be set.\")\n\n    if not (self.isDefined(\"inputCol\") or self.isDefined(\"inputCols\")):\n        raise ValueError(\"inputCol or inputCols must be set.\")\n\n    if not (self.isDefined(\"outputCol\") or self.isDefined(\"outputCols\")):\n        raise ValueError(\"outputCol or outputCols must be set.\")\n\n    output_col_names = self._get_output_cols()\n    if len(output_col_names) != len(self.getFunctionReturnTypes()):\n        raise ValueError(\n            \"Number of output columns must match number of function return types.\"\n        )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/lambda_function/#src.kamae.spark.transformers.lambda_function.LambdaFunctionTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the lambda function transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs the lambda function on the input.</p> Source code in <code>src/kamae/spark/transformers/lambda_function.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the lambda function transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs the lambda function on the input.\n    \"\"\"\n    return LambdaFunctionLayer(\n        function=self.getFunction(),\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/lambda_function/#src.kamae.spark.transformers.lambda_function.LambdaFunctionTransformer.setInputCol","title":"setInputCol","text":"<pre><code>setInputCol(value)\n</code></pre> <p>Sets the parameter inputCol to the given string value. Only allows setting if inputCols is not set.</p> <p>:param value: String to set the inputCol parameter to. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/lambda_function.py</code> <pre><code>def setInputCol(self, value: str) -&gt; \"LambdaFunctionTransformer\":\n    \"\"\"\n    Sets the parameter inputCol to the given string value.\n    Only allows setting if inputCols is not set.\n\n    :param value: String to set the inputCol parameter to.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if self.isSet(\"inputCols\"):\n        raise ValueError(\"inputCols is already set. Cannot set inputCol as well.\")\n    return self._set(inputCol=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/lambda_function/#src.kamae.spark.transformers.lambda_function.LambdaFunctionTransformer.setInputCols","title":"setInputCols","text":"<pre><code>setInputCols(value)\n</code></pre> <p>Sets the parameter inputCols to the given list of strings. Only allows setting if inputCol is not set. Only allows multiple inputs.</p> <p>:param value: List of strings to set the inputCols parameter to. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/lambda_function.py</code> <pre><code>def setInputCols(self, value: List[str]) -&gt; \"LambdaFunctionTransformer\":\n    \"\"\"\n    Sets the parameter inputCols to the given list of strings.\n    Only allows setting if inputCol is not set. Only allows multiple inputs.\n\n    :param value: List of strings to set the inputCols parameter to.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if len(value) &lt; 2:\n        raise ValueError(\n            \"\"\"inputCols must be set with more than one column.\n            Use inputCol for single input.\"\"\"\n        )\n    if self.isSet(\"inputCol\"):\n        raise ValueError(\"inputCol is already set. Cannot set inputCols as well.\")\n    return self._set(inputCols=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/lambda_function/#src.kamae.spark.transformers.lambda_function.LambdaFunctionTransformer.setOutputCol","title":"setOutputCol","text":"<pre><code>setOutputCol(value)\n</code></pre> <p>Sets the parameter outputCol to the given string value. Only allows setting if outputCols is not set.</p> <p>:param value: String to set the outputCol parameter to. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/lambda_function.py</code> <pre><code>def setOutputCol(self, value: str) -&gt; \"LambdaFunctionTransformer\":\n    \"\"\"\n    Sets the parameter outputCol to the given string value.\n    Only allows setting if outputCols is not set.\n\n    :param value: String to set the outputCol parameter to.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if self.isSet(\"outputCols\"):\n        raise ValueError(\"outputCols is already set. Cannot set outputCol as well.\")\n    return self._set(outputCol=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/lambda_function/#src.kamae.spark.transformers.lambda_function.LambdaFunctionTransformer.setOutputCols","title":"setOutputCols","text":"<pre><code>setOutputCols(value)\n</code></pre> <p>Sets the parameter outputCols to the given list of strings. Only allows setting if outputCol is not set. Only allows multiple outputs.</p> <p>:param value: List of strings to set the outputCols parameter to. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/lambda_function.py</code> <pre><code>def setOutputCols(self, value: List[str]) -&gt; \"LambdaFunctionTransformer\":\n    \"\"\"\n    Sets the parameter outputCols to the given list of strings.\n    Only allows setting if outputCol is not set. Only allows multiple outputs.\n\n    :param value: List of strings to set the outputCols parameter to.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if len(value) &lt; 2:\n        raise ValueError(\n            \"\"\"outputCols must be set with more than one column.\n            Use outputCol for single output.\"\"\"\n        )\n    if self.isSet(\"outputCol\"):\n        raise ValueError(\"outputCol is already set. Cannot set outputCols as well.\")\n    return self._set(outputCols=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/list_max/","title":"list_max","text":""},{"location":"reference/src/kamae/spark/transformers/list_max/#src.kamae.spark.transformers.list_max.ListMaxTransformer","title":"ListMaxTransformer","text":"<pre><code>ListMaxTransformer(\n    inputCol=None,\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    queryIdCol=None,\n    topN=None,\n    sortOrder=\"asc\",\n    minFilterValue=None,\n    nanFillValue=0.0,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>MultiInputSingleOutputParams</code>, <code>ListwiseStatisticsParams</code>, <code>NanFillValueParams</code></p> <p>Calculate the listwise maximum across the query id column. - If inputCol is set, the transformer calculates the maximum of the input column based on all the items in the same query id column. - If inputCols is set, the transformer calculates the maximum of the first column based on second column's topN items in the same query id column.</p> <p>By using the topN items to calculate the statistics, we can better approximate the real statistics in production. It should be used a large enough topN to get a good approximation of the statistics, and an important feature to sort on, such as item's production.</p> <p>Example: calculate the maximum price in the same query, based on the top N items sorted by descending production.</p> <p>:param inputCol: Value column, on which to calculate the maximum. :param inputCols: Input column name. - The first is the value column, on which to calculate the maximum. - The second is the sort column, based on which to sort the items. :param outputCol: Name of output col. :param inputDtype: Data Type of input. :param outputDtype: Data Type of output. :param layerName: The name of the transformer, which typically should be the name of the produced feature. :param queryIdCol: Name of column to aggregate upon. It is required. :param topN: Filter for limiting the items to calculate the statistics. :param sortOrder: Option of 'asc' or 'desc' which defines order for listwise operation. Default is 'asc'. :param minFilterValue: Minimum value to remove padded values defaults to &gt;= 0. :nanFillValue: Value to fill NaNs results with. Defaults to 0.</p> Source code in <code>src/kamae/spark/transformers/list_max.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    queryIdCol: Optional[str] = None,\n    topN: Optional[int] = None,\n    sortOrder: str = \"asc\",\n    minFilterValue: Optional[float] = None,\n    nanFillValue: float = 0.0,\n) -&gt; None:\n    super().__init__()\n    self._setDefault(\n        topN=None,\n        sortOrder=\"asc\",\n        minFilterValue=None,\n        nanFillValue=0,\n    )\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/list_max/#src.kamae.spark.transformers.list_max.ListMaxTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/list_max/#src.kamae.spark.transformers.list_max.ListMaxTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Calculate the listwise maximum, optionally sorting and filtering based on the second input column. :param dataset: The dataframe with signals and features. :returns: The dataframe dataset with the new feature.</p> Source code in <code>src/kamae/spark/transformers/list_max.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculate the listwise maximum, optionally sorting and\n    filtering based on the second input column.\n    :param dataset: The dataframe with signals and features.\n    :returns: The dataframe dataset with the new feature.\n    \"\"\"\n    if not self.isDefined(\"queryIdCol\"):\n        raise ValueError(\"queryIdCol must be set on listwise transformers.\")\n\n    # Define the columns to use for the calculation\n    with_sort = self.isDefined(\"inputCols\")\n    if with_sort:\n        val_col_name = self.getInputCols()[0]\n        sort_col_name = self.getInputCols()[1]\n    else:\n        val_col_name = self.getInputCol()\n        sort_col_name = None\n\n    check_listwise_columns(\n        dataset=dataset,\n        query_col_name=self.getQueryIdCol(),\n        value_col_name=val_col_name,\n        sort_col_name=sort_col_name,\n    )\n\n    condition_col, window_spec = get_listwise_condition_and_window(\n        query_col=F.col(self.getQueryIdCol()),\n        value_col=F.col(val_col_name),\n        sort_col=F.col(sort_col_name) if with_sort else None,\n        sort_order=self.getSortOrder(),\n        sort_top_n=self.getTopN(),\n        min_filter_value=self.getMinFilterValue(),\n    )\n\n    # Calculate the statistics under the conditions\n    dataset = dataset.withColumn(\n        self.getOutputCol(),\n        F.max(F.when(condition_col, F.col(val_col_name))).over(window_spec),\n    )\n\n    # Replace Nulls/Nans\n    dataset = dataset.fillna({self.getOutputCol(): self.getNanFillValue()})\n\n    return dataset\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/list_max/#src.kamae.spark.transformers.list_max.ListMaxTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the listwise-maximum transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs an averaging operation.</p> Source code in <code>src/kamae/spark/transformers/list_max.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the listwise-maximum transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs an averaging operation.\n    \"\"\"\n    return ListMaxLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        top_n=self.getTopN(),\n        sort_order=self.getSortOrder(),\n        min_filter_value=self.getMinFilterValue(),\n        nan_fill_value=self.getNanFillValue(),\n        axis=1,\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/list_mean/","title":"list_mean","text":""},{"location":"reference/src/kamae/spark/transformers/list_mean/#src.kamae.spark.transformers.list_mean.ListMeanTransformer","title":"ListMeanTransformer","text":"<pre><code>ListMeanTransformer(\n    inputCol=None,\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    queryIdCol=None,\n    topN=None,\n    sortOrder=\"asc\",\n    minFilterValue=None,\n    nanFillValue=0.0,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>MultiInputSingleOutputParams</code>, <code>ListwiseStatisticsParams</code>, <code>NanFillValueParams</code></p> <p>Calculate the listwise mean across the query id column. - If inputCol is set, the transformer calculates the mean of the input column based on all the items in the same query id column. - If inputCols is set, the transformer calculates the mean of the first column based on second column's topN items in the same query id column.</p> <p>By using the topN items to calculate the statistics, we can better approximate the real statistics in production. It should be used a large enough topN to get a good approximation of the statistics, and an important feature to sort on, such as item's production.</p> <p>Example: calculate the mean price in the same query, based on the top N items sorted by descending production.</p> <p>:param inputCol: Value column, on which to calculate the mean. :param inputCols: Input column name. - The first is the value column, on which to calculate the mean. - The second is the sort column, based on which to sort the items. :param outputCol: Name of output col. :param inputDtype: Data Type of input. :param outputDtype: Data Type of output. :param layerName: The name of the transformer, which typically should be the name of the produced feature. :param queryIdCol: Name of column to aggregate upon. It is required. :param topN: Filter for limiting the items to calculate the statistics. :param sortOrder: Option of 'asc' or 'desc' which defines order for listwise operation. Default is 'asc'. :param minFilterValue: Minimum value to remove padded values defaults to &gt;= 0. :nanFillValue: Value to fill NaNs results with. Defaults to 0.</p> Source code in <code>src/kamae/spark/transformers/list_mean.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    queryIdCol: Optional[str] = None,\n    topN: Optional[int] = None,\n    sortOrder: str = \"asc\",\n    minFilterValue: Optional[float] = None,\n    nanFillValue: float = 0.0,\n) -&gt; None:\n    super().__init__()\n    self._setDefault(\n        topN=None,\n        sortOrder=\"asc\",\n        minFilterValue=None,\n        nanFillValue=0,\n    )\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/list_mean/#src.kamae.spark.transformers.list_mean.ListMeanTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/list_mean/#src.kamae.spark.transformers.list_mean.ListMeanTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Calculate the listwise mean, optionally sorting and filtering based on the second input column. :param dataset: The dataframe with signals and features. :returns: The dataframe dataset with the new feature.</p> Source code in <code>src/kamae/spark/transformers/list_mean.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculate the listwise mean, optionally sorting and\n    filtering based on the second input column.\n    :param dataset: The dataframe with signals and features.\n    :returns: The dataframe dataset with the new feature.\n    \"\"\"\n    if not self.isDefined(\"queryIdCol\"):\n        raise ValueError(\"queryIdCol must be set on listwise transformers.\")\n\n    # Define the columns to use for the calculation\n    with_sort = self.isDefined(\"inputCols\")\n    if with_sort:\n        val_col_name = self.getInputCols()[0]\n        sort_col_name = self.getInputCols()[1]\n    else:\n        val_col_name = self.getInputCol()\n        sort_col_name = None\n\n    check_listwise_columns(\n        dataset=dataset,\n        query_col_name=self.getQueryIdCol(),\n        value_col_name=val_col_name,\n        sort_col_name=sort_col_name,\n    )\n\n    condition_col, window_spec = get_listwise_condition_and_window(\n        query_col=F.col(self.getQueryIdCol()),\n        value_col=F.col(val_col_name),\n        sort_col=F.col(sort_col_name) if with_sort else None,\n        sort_order=self.getSortOrder(),\n        sort_top_n=self.getTopN(),\n        min_filter_value=self.getMinFilterValue(),\n    )\n\n    # Calculate the statistics under the conditions\n    dataset = dataset.withColumn(\n        self.getOutputCol(),\n        F.avg(F.when(condition_col, F.col(val_col_name))).over(window_spec),\n    )\n\n    # Replace Nulls/Nans\n    dataset = dataset.fillna({self.getOutputCol(): self.getNanFillValue()})\n\n    return dataset\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/list_mean/#src.kamae.spark.transformers.list_mean.ListMeanTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the listwise-mean transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs an averaging operation.</p> Source code in <code>src/kamae/spark/transformers/list_mean.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the listwise-mean transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs an averaging operation.\n    \"\"\"\n    return ListMeanLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        top_n=self.getTopN(),\n        sort_order=self.getSortOrder(),\n        min_filter_value=self.getMinFilterValue(),\n        nan_fill_value=self.getNanFillValue(),\n        axis=1,\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/list_median/","title":"list_median","text":""},{"location":"reference/src/kamae/spark/transformers/list_median/#src.kamae.spark.transformers.list_median.ListMedianTransformer","title":"ListMedianTransformer","text":"<pre><code>ListMedianTransformer(\n    inputCol=None,\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    queryIdCol=None,\n    topN=None,\n    sortOrder=\"asc\",\n    minFilterValue=None,\n    nanFillValue=0.0,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>MultiInputSingleOutputParams</code>, <code>ListwiseStatisticsParams</code>, <code>NanFillValueParams</code></p> <p>Calculate the listwise median across the query id column. - If inputCol is set, the transformer calculates the median of the input column based on all the items in the same query id column. - If inputCols is set, the transformer calculates the median of the first column based on second column's topN items in the same query id column.</p> <p>By using the topN items to calculate the statistics, we can better approximate the real statistics in production. It should be used a large enough topN to get a good approximation of the statistics, and an important feature to sort on, such as item's production.</p> <p>Example: calculate the median price in the same query, based on the top N items sorted by descending production.</p> <p>:param inputCol: Value column, on which to calculate the median. :param inputCols: Input column name. - The first is the value column, on which to calculate the median. - The second is the sort column, based on which to sort the items. :param outputCol: Name of output col. :param inputDtype: Data Type of input. :param outputDtype: Data Type of output. :param layerName: The name of the transformer, which typically should be the name of the produced feature. :param queryIdCol: Name of column to aggregate upon. It is required. :param topN: Filter for limiting the items to calculate the statistics. :param sortOrder: Option of 'asc' or 'desc' which defines order for listwise operation. Default is 'asc'. :param minFilterValue: Minimum value to remove padded values defaults to &gt;= 0. :nanFillValue: Value to fill NaNs results with. Defaults to 0.</p> Source code in <code>src/kamae/spark/transformers/list_median.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    queryIdCol: Optional[str] = None,\n    topN: Optional[int] = None,\n    sortOrder: str = \"asc\",\n    minFilterValue: Optional[float] = None,\n    nanFillValue: float = 0.0,\n) -&gt; None:\n    super().__init__()\n    self._setDefault(\n        topN=None,\n        sortOrder=\"asc\",\n        minFilterValue=None,\n        nanFillValue=0,\n    )\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/list_median/#src.kamae.spark.transformers.list_median.ListMedianTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/list_median/#src.kamae.spark.transformers.list_median.ListMedianTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Calculate the listwise median, optionally sorting and filtering based on the second input column. :param dataset: The dataframe with signals and features. :returns: The dataframe dataset with the new feature.</p> Source code in <code>src/kamae/spark/transformers/list_median.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculate the listwise median, optionally sorting and\n    filtering based on the second input column.\n    :param dataset: The dataframe with signals and features.\n    :returns: The dataframe dataset with the new feature.\n    \"\"\"\n    if not self.isDefined(\"queryIdCol\"):\n        raise ValueError(\"queryIdCol must be set on listwise transformers.\")\n\n    # Define the columns to use for the calculation\n    with_sort = self.isDefined(\"inputCols\")\n    if with_sort:\n        val_col_name = self.getInputCols()[0]\n        sort_col_name = self.getInputCols()[1]\n    else:\n        val_col_name = self.getInputCol()\n        sort_col_name = None\n\n    check_listwise_columns(\n        dataset=dataset,\n        query_col_name=self.getQueryIdCol(),\n        value_col_name=val_col_name,\n        sort_col_name=sort_col_name,\n    )\n\n    cond_col, window_spec = get_listwise_condition_and_window(\n        query_col=F.col(self.getQueryIdCol()),\n        value_col=F.col(val_col_name),\n        sort_col=F.col(sort_col_name) if with_sort else None,\n        sort_order=self.getSortOrder(),\n        sort_top_n=self.getTopN(),\n        min_filter_value=self.getMinFilterValue(),\n    )\n\n    # Calculate the statistics under the conditions\n    dataset = dataset.withColumn(\n        \"sorted_values\",\n        F.sort_array(\n            F.collect_list(F.when(cond_col, F.col(val_col_name))).over(window_spec)\n        ),\n    )\n\n    # Compute median using Spark native functions\n    dataset = dataset.withColumn(\"array_size\", F.size(F.col(\"sorted_values\")))\n    mid_index_1 = (F.col(\"array_size\") / 2).cast(\"int\")\n    mid_index_2 = (F.col(\"array_size\") / 2 - 1).cast(\"int\")\n    dataset = dataset.withColumn(\n        self.getOutputCol(),\n        F.when(\n            F.col(\"array_size\") % 2 == 1,\n            F.element_at(F.col(\"sorted_values\"), mid_index_1 + 1),\n        ).otherwise(\n            (\n                F.element_at(F.col(\"sorted_values\"), mid_index_1 + 1)\n                + F.element_at(F.col(\"sorted_values\"), mid_index_2 + 1)\n            )\n            / 2.0\n        ),\n    )\n\n    # Replace Nulls/Nans\n    dataset = dataset.fillna({self.getOutputCol(): self.getNanFillValue()})\n\n    return dataset\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/list_median/#src.kamae.spark.transformers.list_median.ListMedianTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the listwise-median transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs a median operation.</p> Source code in <code>src/kamae/spark/transformers/list_median.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the listwise-median transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs a median operation.\n    \"\"\"\n    return ListMedianLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        top_n=self.getTopN(),\n        sort_order=self.getSortOrder(),\n        min_filter_value=self.getMinFilterValue(),\n        nan_fill_value=self.getNanFillValue(),\n        axis=1,\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/list_min/","title":"list_min","text":""},{"location":"reference/src/kamae/spark/transformers/list_min/#src.kamae.spark.transformers.list_min.ListMinTransformer","title":"ListMinTransformer","text":"<pre><code>ListMinTransformer(\n    inputCol=None,\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    queryIdCol=None,\n    topN=None,\n    sortOrder=\"asc\",\n    minFilterValue=None,\n    nanFillValue=0.0,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>MultiInputSingleOutputParams</code>, <code>ListwiseStatisticsParams</code>, <code>NanFillValueParams</code></p> <p>Calculate the listwise minimum across the query id column. - If inputCol is set, the transformer calculates the minimum of the input column based on all the items in the same query id column. - If inputCols is set, the transformer calculates the minimum of the first column based on second column's topN items in the same query id column.</p> <p>By using the topN items to calculate the statistics, we can better approximate the real statistics in production. It should be used a large enough topN to get a good approximation of the statistics, and an important feature to sort on, such as item's production.</p> <p>Example: calculate the minimum price in the same query, based on the top N items sorted by descending production.</p> <p>:param inputCol: Value column, on which to calculate the minimum. :param inputCols: Input column name. - The first is the value column, on which to calculate the minimum. - The second is the sort column, based on which to sort the items. :param outputCol: Name of output col. :param inputDtype: Data Type of input. :param outputDtype: Data Type of output. :param layerName: The name of the transformer, which typically should be the name of the produced feature. :param queryIdCol: Name of column to aggregate upon. It is required. :param topN: Filter for limiting the items to calculate the statistics. :param sortOrder: Option of 'asc' or 'desc' which defines order for listwise operation. Default is 'asc'. :param minFilterValue: Minimum value to remove padded values defaults to &gt;= 0. :nanFillValue: Value to fill NaNs results with. Defaults to 0.</p> Source code in <code>src/kamae/spark/transformers/list_min.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    queryIdCol: Optional[str] = None,\n    topN: Optional[int] = None,\n    sortOrder: str = \"asc\",\n    minFilterValue: Optional[float] = None,\n    nanFillValue: float = 0.0,\n) -&gt; None:\n    super().__init__()\n    self._setDefault(\n        topN=None,\n        sortOrder=\"asc\",\n        minFilterValue=None,\n        nanFillValue=0,\n    )\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/list_min/#src.kamae.spark.transformers.list_min.ListMinTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/list_min/#src.kamae.spark.transformers.list_min.ListMinTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Calculate the listwise minimum, optionally sorting and filtering based on the second input column. :param dataset: The dataframe with signals and features. :returns: The dataframe dataset with the new feature.</p> Source code in <code>src/kamae/spark/transformers/list_min.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculate the listwise minimum, optionally sorting and\n    filtering based on the second input column.\n    :param dataset: The dataframe with signals and features.\n    :returns: The dataframe dataset with the new feature.\n    \"\"\"\n    if not self.isDefined(\"queryIdCol\"):\n        raise ValueError(\"queryIdCol must be set on listwise transformers.\")\n\n    # Define the columns to use for the calculation\n    with_sort = self.isDefined(\"inputCols\")\n    if with_sort:\n        val_col_name = self.getInputCols()[0]\n        sort_col_name = self.getInputCols()[1]\n    else:\n        val_col_name = self.getInputCol()\n        sort_col_name = None\n\n    check_listwise_columns(\n        dataset=dataset,\n        query_col_name=self.getQueryIdCol(),\n        value_col_name=val_col_name,\n        sort_col_name=sort_col_name,\n    )\n\n    condition_col, window_spec = get_listwise_condition_and_window(\n        query_col=F.col(self.getQueryIdCol()),\n        value_col=F.col(val_col_name),\n        sort_col=F.col(sort_col_name) if with_sort else None,\n        sort_order=self.getSortOrder(),\n        sort_top_n=self.getTopN(),\n        min_filter_value=self.getMinFilterValue(),\n    )\n\n    # Calculate the statistics under the conditions\n    dataset = dataset.withColumn(\n        self.getOutputCol(),\n        F.min(F.when(condition_col, F.col(val_col_name))).over(window_spec),\n    )\n\n    # Replace Nulls/Nans\n    dataset = dataset.fillna({self.getOutputCol(): self.getNanFillValue()})\n\n    return dataset\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/list_min/#src.kamae.spark.transformers.list_min.ListMinTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the listwise-minimum transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs an averaging operation.</p> Source code in <code>src/kamae/spark/transformers/list_min.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the listwise-minimum transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs an averaging operation.\n    \"\"\"\n    return ListMinLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        top_n=self.getTopN(),\n        sort_order=self.getSortOrder(),\n        min_filter_value=self.getMinFilterValue(),\n        nan_fill_value=self.getNanFillValue(),\n        axis=1,\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/list_rank/","title":"list_rank","text":""},{"location":"reference/src/kamae/spark/transformers/list_rank/#src.kamae.spark.transformers.list_rank.ListRankTransformer","title":"ListRankTransformer","text":"<pre><code>ListRankTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    queryIdCol=None,\n    sortOrder=\"desc\",\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>ListwiseParams</code></p> <p>Calculate the listwise rank across the query id column.</p> <p>Example: calculate the rank of items within a query, given the score.</p> <p>:param inputCol: Value column, on which to calculate the rank. :param outputCol: Name of output col. :param inputDtype: Data Type of input. :param outputDtype: Data Type of output. :param layerName: The name of the transformer, which typically should be the name of the produced feature. :param queryIdCol: Name of column to aggregate upon. It is required. :param sortOrder: Option of 'asc' or 'desc' which defines order for listwise operation. Default is 'desc'.</p> Source code in <code>src/kamae/spark/transformers/list_rank.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    queryIdCol: Optional[str] = None,\n    sortOrder: str = \"desc\",\n) -&gt; None:\n    super().__init__()\n    self._setDefault(sortOrder=\"desc\")\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/list_rank/#src.kamae.spark.transformers.list_rank.ListRankTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/list_rank/#src.kamae.spark.transformers.list_rank.ListRankTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Calculate the rank. :param dataset: The dataframe with signals and features. :returns: The dataframe dataset with the new feature.</p> Source code in <code>src/kamae/spark/transformers/list_rank.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculate the rank.\n    :param dataset: The dataframe with signals and features.\n    :returns: The dataframe dataset with the new feature.\n    \"\"\"\n    if not self.isDefined(\"queryIdCol\"):\n        raise ValueError(\"queryIdCol must be set on listwise transformers.\")\n\n    # Get params\n    input_col = self.getInputCol()\n    query_id_col = self.getQueryIdCol()\n    output_col = self.getOutputCol()\n    sort_order = self.getSortOrder()\n\n    # Validate listwise cols\n    check_listwise_columns(\n        dataset=dataset,\n        query_col_name=query_id_col,\n        value_col_name=input_col,\n        sort_col_name=None,\n    )\n\n    # Set sort order\n    if sort_order == \"asc\":\n        sort_col = F.col(input_col).asc()\n    elif sort_order == \"desc\":\n        sort_col = F.col(input_col).desc()\n    else:\n        raise ValueError(f\"Invalid sortOrder: {sort_order}\")\n    # Define window spec\n    window_spec = Window.partitionBy(query_id_col).orderBy(sort_col)\n\n    # Calculate the rank\n    dataset = dataset.withColumn(output_col, F.row_number().over(window_spec))\n\n    return dataset\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/list_rank/#src.kamae.spark.transformers.list_rank.ListRankTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the listwise-rank transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs a ranking operation.</p> Source code in <code>src/kamae/spark/transformers/list_rank.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the listwise-rank transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs a ranking operation.\n    \"\"\"\n    return ListRankLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        sort_order=self.getSortOrder(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/list_std_dev/","title":"list_std_dev","text":""},{"location":"reference/src/kamae/spark/transformers/list_std_dev/#src.kamae.spark.transformers.list_std_dev.ListStdDevTransformer","title":"ListStdDevTransformer","text":"<pre><code>ListStdDevTransformer(\n    inputCol=None,\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    queryIdCol=None,\n    topN=None,\n    sortOrder=\"asc\",\n    minFilterValue=None,\n    nanFillValue=0.0,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>MultiInputSingleOutputParams</code>, <code>ListwiseStatisticsParams</code>, <code>NanFillValueParams</code></p> <p>Calculate the listwise standard deviation across the query id column. - If inputCol is set, the transformer calculates the stddev of the input column based on all the items in the same query id column. - If inputCols is set, the transformer calculates the stddev of the first column based on second column's topN items in the same query id column.</p> <p>By using the topN items to calculate the statistics, we can better approximate the real statistics in production. It should be used a large enough topN to get a good approximation of the statistics, and an important feature to sort on, such as item's production.</p> <p>Example: calculate the stddev price in the same query, based on the top N items sorted by descending production.</p> <p>:param inputCol: Value column, on which to calculate the stddev. :param inputCols: Input column name. - The first is the value column, on which to calculate the stddev. - The second is the sort column, based on which to sort the items. :param outputCol: Name of output col. :param inputDtype: Data Type of input. :param outputDtype: Data Type of output. :param queryIdCol: Name of column to aggregate upon. It is required. :param layerName: The name of the transformer, which typically should be the name of the produced feature. :param topN: Filter for limiting the items to calculate the statistics. :param sortOrder: Option of 'asc' or 'desc' which defines order for listwise operation. Default is 'asc'. :param minFilterValue: Minimum value to remove padded values defaults to &gt;= 0. :nanFillValue: Value to fill NaNs results with. Defaults to 0.</p> Source code in <code>src/kamae/spark/transformers/list_std_dev.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    queryIdCol: Optional[str] = None,\n    topN: Optional[int] = None,\n    sortOrder: str = \"asc\",\n    minFilterValue: Optional[float] = None,\n    nanFillValue: float = 0.0,\n) -&gt; None:\n    super().__init__()\n    self._setDefault(\n        topN=None,\n        sortOrder=\"asc\",\n        minFilterValue=None,\n        nanFillValue=0,\n    )\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/list_std_dev/#src.kamae.spark.transformers.list_std_dev.ListStdDevTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/list_std_dev/#src.kamae.spark.transformers.list_std_dev.ListStdDevTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Calculate the listwise stddev, optionally sorting and filtering based on the second input column. :param dataset: The dataframe with signals and features. :returns: The dataframe dataset with the new feature.</p> Source code in <code>src/kamae/spark/transformers/list_std_dev.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculate the listwise stddev, optionally sorting and\n    filtering based on the second input column.\n    :param dataset: The dataframe with signals and features.\n    :returns: The dataframe dataset with the new feature.\n    \"\"\"\n    if not self.isDefined(\"queryIdCol\"):\n        raise ValueError(\"queryIdCol must be set on listwise transformers.\")\n\n    # Define the columns to use for the calculation\n    with_sort = self.isDefined(\"inputCols\")\n    if with_sort:\n        val_col_name = self.getInputCols()[0]\n        sort_col_name = self.getInputCols()[1]\n    else:\n        val_col_name = self.getInputCol()\n        sort_col_name = None\n\n    check_listwise_columns(\n        dataset=dataset,\n        query_col_name=self.getQueryIdCol(),\n        value_col_name=val_col_name,\n        sort_col_name=sort_col_name,\n    )\n\n    condition_col, window_spec = get_listwise_condition_and_window(\n        query_col=F.col(self.getQueryIdCol()),\n        value_col=F.col(val_col_name),\n        sort_col=F.col(sort_col_name) if with_sort else None,\n        sort_order=self.getSortOrder(),\n        sort_top_n=self.getTopN(),\n        min_filter_value=self.getMinFilterValue(),\n    )\n\n    # Calculate the statistics under the conditions\n    dataset = dataset.withColumn(\n        self.getOutputCol(),\n        F.stddev(F.when(condition_col, F.col(val_col_name))).over(window_spec),\n    )\n\n    # Replace Nulls/Nans\n    dataset = dataset.fillna({self.getOutputCol(): self.getNanFillValue()})\n\n    return dataset\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/list_std_dev/#src.kamae.spark.transformers.list_std_dev.ListStdDevTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the listwise-stddev transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs an averaging operation.</p> Source code in <code>src/kamae/spark/transformers/list_std_dev.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the listwise-stddev transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs an averaging operation.\n    \"\"\"\n    return ListStdDevLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        top_n=self.getTopN(),\n        sort_order=self.getSortOrder(),\n        min_filter_value=self.getMinFilterValue(),\n        nan_fill_value=self.getNanFillValue(),\n        axis=1,\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/log/","title":"log","text":""},{"location":"reference/src/kamae/spark/transformers/log/#src.kamae.spark.transformers.log.LogParams","title":"LogParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing alpha parameter needed for log transform layers.</p>"},{"location":"reference/src/kamae/spark/transformers/log/#src.kamae.spark.transformers.log.LogParams.getAlpha","title":"getAlpha","text":"<pre><code>getAlpha()\n</code></pre> <p>Gets the alpha parameter.</p> <p>:returns: Float value of alpha used in log transform.</p> Source code in <code>src/kamae/spark/transformers/log.py</code> <pre><code>def getAlpha(self) -&gt; float:\n    \"\"\"\n    Gets the alpha parameter.\n\n    :returns: Float value of alpha used in log transform.\n    \"\"\"\n    return self.getOrDefault(self.alpha)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/log/#src.kamae.spark.transformers.log.LogParams.setAlpha","title":"setAlpha","text":"<pre><code>setAlpha(value)\n</code></pre> <p>Sets the alpha parameter.</p> <p>:param value: Float value to use in log transform: log(alpha + x). :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/log.py</code> <pre><code>def setAlpha(self, value: float) -&gt; \"LogParams\":\n    \"\"\"\n    Sets the alpha parameter.\n\n    :param value: Float value to use in log transform: log(alpha + x).\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(alpha=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/log/#src.kamae.spark.transformers.log.LogTransformer","title":"LogTransformer","text":"<pre><code>LogTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    alpha=0.0,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>LogParams</code>, <code>SingleInputSingleOutputParams</code></p> <p>Log Spark Transformer for use in Spark pipelines. This transformer applies a log(alpha + x) transform to the input column.</p> <ul> <li>alpha: 0</li> </ul> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param alpha: Value to use in log transform: log(alpha + x). Default is 0. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/log.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    alpha: float = 0.0,\n) -&gt; None:\n    \"\"\"\n    Instantiates a LogTransformer transformer. Sets the default values of:\n\n    - alpha: 0\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param alpha: Value to use in log transform: log(alpha + x). Default is 0.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(alpha=0.0)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/log/#src.kamae.spark.transformers.log.LogTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/log/#src.kamae.spark.transformers.log.LogTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column named outputCol with the log transform of the inputCol.</p> <p>:param dataset: Pyspark DataFrame to transform. :returns: Transformed pyspark dataFrame.</p> Source code in <code>src/kamae/spark/transformers/log.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column named outputCol with the\n    log transform of the inputCol.\n\n    :param dataset: Pyspark DataFrame to transform.\n    :returns: Transformed pyspark dataFrame.\n    \"\"\"\n    alpha = self.getAlpha()\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n    output_col = single_input_single_output_scalar_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: F.log(x + F.lit(alpha)),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/log/#src.kamae.spark.transformers.log.LogTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer that performs the log transform.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that performs the log(alpha + x) operation.</p> Source code in <code>src/kamae/spark/transformers/log.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer that performs the log transform.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter\n    that performs the log(alpha + x) operation.\n    \"\"\"\n    return LogLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        alpha=self.getAlpha(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/logical_and/","title":"logical_and","text":""},{"location":"reference/src/kamae/spark/transformers/logical_and/#src.kamae.spark.transformers.logical_and.LogicalAndTransformer","title":"LogicalAndTransformer","text":"<pre><code>LogicalAndTransformer(\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>MultiInputSingleOutputParams</code></p> <p>Logical And Spark Transformer for use in Spark pipelines. This transformer performs an element-wise logical and operation on multiple columns.</p> <p>:param inputCols: Input column names. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/logical_and.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes a LogicalAndTransformer transformer.\n\n    :param inputCols: Input column names.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/logical_and/#src.kamae.spark.transformers.logical_and.LogicalAndTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/logical_and/#src.kamae.spark.transformers.logical_and.LogicalAndTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which is the logical and of the input columns.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/logical_and.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which is the logical and of the input columns.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_col_names = self.getInputCols()\n    input_cols = [F.col(c) for c in input_col_names]\n    input_col_datatypes = [\n        self.get_column_datatype(dataset=dataset, column_name=c)\n        for c in input_col_names\n    ]\n    output_col = multi_input_single_output_scalar_transform(\n        input_cols=input_cols,\n        input_col_datatypes=input_col_datatypes,\n        input_col_names=input_col_names,\n        func=lambda x: reduce(and_, [x[c] for c in input_col_names]),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/logical_and/#src.kamae.spark.transformers.logical_and.LogicalAndTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the logical and transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs a logical and operation.</p> Source code in <code>src/kamae/spark/transformers/logical_and.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the logical and transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs a logical and operation.\n    \"\"\"\n    return LogicalAndLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/logical_and/#src.kamae.spark.transformers.logical_and.LogicalAndTransformer.setInputCols","title":"setInputCols","text":"<pre><code>setInputCols(value)\n</code></pre> <p>Sets the inputCols parameter. Raises an error if the value is a list of length 1.</p> <p>:param value: List of input column names. :returns: Instance of class with inputCols parameter set.</p> Source code in <code>src/kamae/spark/transformers/logical_and.py</code> <pre><code>def setInputCols(self, value: List[str]) -&gt; \"LogicalAndTransformer\":\n    \"\"\"\n    Sets the inputCols parameter. Raises an error if the value is a list of\n    length 1.\n\n    :param value: List of input column names.\n    :returns: Instance of class with inputCols parameter set.\n    \"\"\"\n    if len(value) == 1:\n        raise ValueError(\"inputCols must be a list of length &gt; 1\")\n    return self._set(inputCols=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/logical_not/","title":"logical_not","text":""},{"location":"reference/src/kamae/spark/transformers/logical_not/#src.kamae.spark.transformers.logical_not.LogicalNotTransformer","title":"LogicalNotTransformer","text":"<pre><code>LogicalNotTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code></p> <p>Logical Not Spark Transformer for use in Spark pipelines. This transformer performs a logical not operation on a single column.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/logical_not.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes a LogicalNotTransformer transformer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/logical_not/#src.kamae.spark.transformers.logical_not.LogicalNotTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/logical_not/#src.kamae.spark.transformers.logical_not.LogicalNotTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which is the logical not of the input column.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/logical_not.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which is the logical not of the input column.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n    output_col = single_input_single_output_scalar_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: ~x,\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/logical_not/#src.kamae.spark.transformers.logical_not.LogicalNotTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the logical not transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs a logical not operation.</p> Source code in <code>src/kamae/spark/transformers/logical_not.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the logical not transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs a logical not operation.\n    \"\"\"\n    return LogicalNotLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/logical_or/","title":"logical_or","text":""},{"location":"reference/src/kamae/spark/transformers/logical_or/#src.kamae.spark.transformers.logical_or.LogicalOrTransformer","title":"LogicalOrTransformer","text":"<pre><code>LogicalOrTransformer(\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>MultiInputSingleOutputParams</code></p> <p>Logical Or Spark Transformer for use in Spark pipelines. This transformer performs an element-wise logical or operation on multiple columns.</p> <p>:param inputCols: Input column names. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/logical_or.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes a LogicalOrTransformer transformer.\n\n    :param inputCols: Input column names.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/logical_or/#src.kamae.spark.transformers.logical_or.LogicalOrTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/logical_or/#src.kamae.spark.transformers.logical_or.LogicalOrTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which is the logical or of the input columns.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/logical_or.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which is the logical or of the input columns.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_col_names = self.getInputCols()\n    input_cols = [F.col(c) for c in input_col_names]\n    input_col_datatypes = [\n        self.get_column_datatype(dataset=dataset, column_name=c)\n        for c in input_col_names\n    ]\n    output_col = multi_input_single_output_scalar_transform(\n        input_cols=input_cols,\n        input_col_datatypes=input_col_datatypes,\n        input_col_names=input_col_names,\n        func=lambda x: reduce(or_, [x[c] for c in input_col_names]),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/logical_or/#src.kamae.spark.transformers.logical_or.LogicalOrTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the logical or transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs a logical or operation.</p> Source code in <code>src/kamae/spark/transformers/logical_or.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the logical or transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs a logical or operation.\n    \"\"\"\n    return LogicalOrLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/logical_or/#src.kamae.spark.transformers.logical_or.LogicalOrTransformer.setInputCols","title":"setInputCols","text":"<pre><code>setInputCols(value)\n</code></pre> <p>Sets the inputCols parameter. Raises an error if the value is a list of length 1.</p> <p>:param value: List of input column names. :returns: Instance of class with inputCols parameter set.</p> Source code in <code>src/kamae/spark/transformers/logical_or.py</code> <pre><code>def setInputCols(self, value: List[str]) -&gt; \"LogicalOrTransformer\":\n    \"\"\"\n    Sets the inputCols parameter. Raises an error if the value is a list of\n    length 1.\n\n    :param value: List of input column names.\n    :returns: Instance of class with inputCols parameter set.\n    \"\"\"\n    if len(value) == 1:\n        raise ValueError(\"inputCols must be a list of length &gt; 1\")\n    return self._set(inputCols=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/max/","title":"max","text":""},{"location":"reference/src/kamae/spark/transformers/max/#src.kamae.spark.transformers.max.MaxTransformer","title":"MaxTransformer","text":"<pre><code>MaxTransformer(\n    inputCol=None,\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    mathFloatConstant=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>MultiInputSingleOutputParams</code>, <code>MathFloatConstantParams</code></p> <p>MaxLayer Spark Transformer for use in Spark pipelines. This transformer gets the max of a column and a constant or another column.</p> <p>:param inputCol: Input column name. Only used if inputCols is not specified. If specified, we max this column by the mathFloatConstant. :param inputCols: Input column names. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param mathFloatConstant: Optional constant to use for max op. If not provided, then two input columns are required. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/max.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    mathFloatConstant: Optional[float] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes an MaxTransformer transformer.\n\n    :param inputCol: Input column name. Only used if inputCols is not specified.\n    If specified, we max this column by the mathFloatConstant.\n    :param inputCols: Input column names.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param mathFloatConstant: Optional constant to use for max op. If not provided,\n    then two input columns are required.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(mathFloatConstant=None)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/max/#src.kamae.spark.transformers.max.MaxTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/max/#src.kamae.spark.transformers.max.MaxTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which is the maximum of either the <code>inputCols</code> if specified, or the <code>inputCol</code> and the <code>mathFloatConstant</code></p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/max.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which is the maximum of either the `inputCols` if specified, or the `inputCol`\n    and the `mathFloatConstant`\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_cols = self.get_multiple_input_cols(\n        constant_param_name=\"mathFloatConstant\"\n    )\n    # input_cols can contain either actual columns or lit(constants). In order to\n    # determine the datatype of the input columns, we select them from the dataset\n    # first.\n    input_col_names = dataset.select(input_cols).columns\n    input_col_datatypes = [\n        self.get_column_datatype(dataset=dataset.select(input_cols), column_name=c)\n        for c in input_col_names\n    ]\n\n    output_col = multi_input_single_output_scalar_transform(\n        input_cols=input_cols,\n        input_col_names=input_col_names,\n        input_col_datatypes=input_col_datatypes,\n        func=lambda x: F.greatest(*[x[c] for c in input_col_names]),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/max/#src.kamae.spark.transformers.max.MaxTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the max transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs a max operation.</p> Source code in <code>src/kamae/spark/transformers/max.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the max transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs a max operation.\n    \"\"\"\n    return MaxLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        max_constant=self.getMathFloatConstant(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/mean/","title":"mean","text":""},{"location":"reference/src/kamae/spark/transformers/mean/#src.kamae.spark.transformers.mean.MeanTransformer","title":"MeanTransformer","text":"<pre><code>MeanTransformer(\n    inputCol=None,\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    mathFloatConstant=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>MultiInputSingleOutputParams</code>, <code>MathFloatConstantParams</code></p> <p>Mean Spark Transformer for use in Spark pipelines. This transformer gets the mean of a column and a constant or another column.</p> <p>:param inputCol: Input column name. Only used if inputCols is not specified. If specified, we calculate the mean of this column and the mathFloatConstant. :param inputCols: Input column names. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param mathFloatConstant: Optional constant to use for min op. If not provided, then two input columns are required. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/mean.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    mathFloatConstant: Optional[float] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes a Mean transformer.\n\n    :param inputCol: Input column name. Only used if inputCols is not specified.\n    If specified, we calculate the mean of this column and the mathFloatConstant.\n    :param inputCols: Input column names.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param mathFloatConstant: Optional constant to use for min op. If not provided,\n    then two input columns are required.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(mathFloatConstant=None)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/mean/#src.kamae.spark.transformers.mean.MeanTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/mean/#src.kamae.spark.transformers.mean.MeanTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which is the mean of either the <code>inputCols</code> if specified, or the <code>inputCol</code> and the <code>mathFloatConstant</code></p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/mean.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which is the mean of either the `inputCols` if specified, or the `inputCol`\n    and the `mathFloatConstant`\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_cols = self.get_multiple_input_cols(\n        constant_param_name=\"mathFloatConstant\"\n    )\n    # input_cols can contain either actual columns or lit(constants). In order to\n    # determine the datatype of the input columns, we select them from the dataset\n    # first.\n    input_col_names = dataset.select(input_cols).columns\n    input_col_datatypes = [\n        self.get_column_datatype(dataset=dataset.select(input_cols), column_name=c)\n        for c in input_col_names\n    ]\n\n    output_col = multi_input_single_output_scalar_transform(\n        input_cols=input_cols,\n        input_col_names=input_col_names,\n        input_col_datatypes=input_col_datatypes,\n        func=lambda x: (\n            reduce(add, [x[c] for c in input_col_names]) / len(input_col_names)\n        ),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/mean/#src.kamae.spark.transformers.mean.MeanTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the mean transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs a min operation.</p> Source code in <code>src/kamae/spark/transformers/mean.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the mean transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs a min operation.\n    \"\"\"\n    return MeanLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        mean_constant=self.getMathFloatConstant(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/min/","title":"min","text":""},{"location":"reference/src/kamae/spark/transformers/min/#src.kamae.spark.transformers.min.MinTransformer","title":"MinTransformer","text":"<pre><code>MinTransformer(\n    inputCol=None,\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    mathFloatConstant=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>MultiInputSingleOutputParams</code>, <code>MathFloatConstantParams</code></p> <p>MinLayer Spark Transformer for use in Spark pipelines. This transformer gets the min of a column and a constant or another column.</p> <p>:param inputCol: Input column name. Only used if inputCols is not specified. If specified, we min this column by the mathFloatConstant. :param inputCols: Input column names. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param mathFloatConstant: Optional constant to use for min op. If not provided, then two input columns are required. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/min.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    mathFloatConstant: Optional[float] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes an MinTransformer transformer.\n\n    :param inputCol: Input column name. Only used if inputCols is not specified.\n    If specified, we min this column by the mathFloatConstant.\n    :param inputCols: Input column names.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param mathFloatConstant: Optional constant to use for min op. If not provided,\n    then two input columns are required.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(mathFloatConstant=None)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/min/#src.kamae.spark.transformers.min.MinTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/min/#src.kamae.spark.transformers.min.MinTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which is the minimum of either the <code>inputCols</code> if specified, or the <code>inputCol</code> and the <code>mathFloatConstant</code></p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/min.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which is the minimum of either the `inputCols` if specified, or the `inputCol`\n    and the `mathFloatConstant`\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_cols = self.get_multiple_input_cols(\n        constant_param_name=\"mathFloatConstant\"\n    )\n    # input_cols can contain either actual columns or lit(constants). In order to\n    # determine the datatype of the input columns, we select them from the dataset\n    # first.\n    input_col_names = dataset.select(input_cols).columns\n    input_col_datatypes = [\n        self.get_column_datatype(dataset=dataset.select(input_cols), column_name=c)\n        for c in input_col_names\n    ]\n\n    output_col = multi_input_single_output_scalar_transform(\n        input_cols=input_cols,\n        input_col_names=input_col_names,\n        input_col_datatypes=input_col_datatypes,\n        func=lambda x: F.least(*[x[c] for c in input_col_names]),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/min/#src.kamae.spark.transformers.min.MinTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the min transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs a min operation.</p> Source code in <code>src/kamae/spark/transformers/min.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the min transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs a min operation.\n    \"\"\"\n    return MinLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        min_constant=self.getMathFloatConstant(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/min_hash_index/","title":"min_hash_index","text":""},{"location":"reference/src/kamae/spark/transformers/min_hash_index/#src.kamae.spark.transformers.min_hash_index.MinHashIndexParams","title":"MinHashIndexParams","text":"<p>               Bases: <code>MaskStringValueParams</code></p> <p>Mixin class containing bin parameter needed for the MinHashIndexTransformer.</p>"},{"location":"reference/src/kamae/spark/transformers/min_hash_index/#src.kamae.spark.transformers.min_hash_index.MinHashIndexParams.getNumPermutations","title":"getNumPermutations","text":"<pre><code>getNumPermutations()\n</code></pre> <p>Gets the numPermutations parameter.</p> <p>:returns: Integer value for the number of bins to use for hash indexing.</p> Source code in <code>src/kamae/spark/transformers/min_hash_index.py</code> <pre><code>def getNumPermutations(self) -&gt; int:\n    \"\"\"\n    Gets the numPermutations parameter.\n\n    :returns: Integer value for the number of bins to use for hash indexing.\n    \"\"\"\n    return self.getOrDefault(self.numPermutations)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/min_hash_index/#src.kamae.spark.transformers.min_hash_index.MinHashIndexParams.setNumPermutations","title":"setNumPermutations","text":"<pre><code>setNumPermutations(value)\n</code></pre> <p>Sets the numPermutations parameter.</p> <p>:param value: Integer value for the number of bins to use for hash indexing. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/min_hash_index.py</code> <pre><code>def setNumPermutations(self, value: int) -&gt; \"MinHashIndexParams\":\n    \"\"\"\n    Sets the numPermutations parameter.\n\n    :param value: Integer value for the number of bins to use for hash indexing.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if value &lt;= 0:\n        raise ValueError(\"Number of permutations must be greater than 0.\")\n    return self._set(numPermutations=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/min_hash_index/#src.kamae.spark.transformers.min_hash_index.MinHashIndexTransformer","title":"MinHashIndexTransformer","text":"<pre><code>MinHashIndexTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    numPermutations=128,\n    maskValue=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>MinHashIndexParams</code>, <code>SingleInputSingleOutputParams</code></p> <p>MinHash indexer Spark Transformer for use in Spark pipelines. This transformer hashes the input string set using the MinHash algorithm: https://en.wikipedia.org/wiki/MinHash</p> <p>MinHash approximates the Jaccard similarity between sets by hashing the elements of the sets and returning a fixed-length signature. This length is determined by the numPermutations parameter, which defaults to 128. The output is an array of integer bits.</p> <p>Setting the maskValue parameter allows you to ignore a specific value in the input column when computing the min hash. This is useful if you have padded arrays as then a padded array with the same unique elements as another non-padded array will be considered equal.</p> <p>NOTE: If your data contains null characters: https://en.wikipedia.org/wiki/Null_character This transformer could fail since the hashing algorithm used cannot accept null characters. If you have null characters in your data, you should remove them.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param numPermutations: Number of permutations of your output min hash. Defaults to 128. This is the length of the output array. :param maskValue: Mask value to use when indexing the input column. If set, the mask value will be ignored when computing the min hash. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/min_hash_index.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    numPermutations: int = 128,\n    maskValue: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Instantiates a MinHashIndexTransformer transformer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param numPermutations: Number of permutations of your output min hash.\n    Defaults to 128. This is the length of the output array.\n    :param maskValue: Mask value to use when indexing the input column.\n    If set, the mask value will be ignored when computing the min hash.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(numPermutations=128, maskValue=None)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/min_hash_index/#src.kamae.spark.transformers.min_hash_index.MinHashIndexTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/min_hash_index/#src.kamae.spark.transformers.min_hash_index.MinHashIndexTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column named outputCol with the min hash indexed input column.</p> <p>:param dataset: Pyspark DataFrame to transform. :returns: Transformed pyspark dataFrame.</p> Source code in <code>src/kamae/spark/transformers/min_hash_index.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column named outputCol with the\n    min hash indexed input column.\n\n    :param dataset: Pyspark DataFrame to transform.\n    :returns: Transformed pyspark dataFrame.\n    \"\"\"\n    if not self.isDefined(\"numPermutations\"):\n        raise ValueError(\"numPermutations parameter must be set.\")\n    num_permutations = self.getNumPermutations()\n    mask_value = self.getMaskValue()\n\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n    if not isinstance(input_datatype, ArrayType):\n        raise ValueError(\n            f\"\"\"Input column {self.getInputCol()} must be of type ArrayType,\n            but got {input_datatype}.\"\"\"\n        )\n    output_col = single_input_single_output_array_udf_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: min_hash_udf(\n            labels=x, num_permutations=num_permutations, mask_value=mask_value\n        ),\n        udf_return_element_datatype=IntegerType(),\n    )\n    return dataset.withColumn(\n        self.getOutputCol(),\n        output_col,\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/min_hash_index/#src.kamae.spark.transformers.min_hash_index.MinHashIndexTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer that performs the min hash indexing.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that performs the hash indexing operation.</p> Source code in <code>src/kamae/spark/transformers/min_hash_index.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer that performs the min hash indexing.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter\n    that performs the hash indexing operation.\n    \"\"\"\n    return MinHashIndexLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        num_permutations=self.getNumPermutations(),\n        mask_value=self.getMaskValue(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/min_max_scale/","title":"min_max_scale","text":""},{"location":"reference/src/kamae/spark/transformers/min_max_scale/#src.kamae.spark.transformers.min_max_scale.MinMaxScaleParams","title":"MinMaxScaleParams","text":"<p>               Bases: <code>MaskValueParams</code></p> <p>Mixin class containing minimum and maximum parameters needed for min/max scaler transformers.</p>"},{"location":"reference/src/kamae/spark/transformers/min_max_scale/#src.kamae.spark.transformers.min_max_scale.MinMaxScaleParams.getMax","title":"getMax","text":"<pre><code>getMax()\n</code></pre> <p>Gets the max parameter.</p> <p>:returns: List of float standard deviation values.</p> Source code in <code>src/kamae/spark/transformers/min_max_scale.py</code> <pre><code>def getMax(self) -&gt; List[float]:\n    \"\"\"\n    Gets the max parameter.\n\n    :returns: List of float standard deviation values.\n    \"\"\"\n    return self.getOrDefault(self.max)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/min_max_scale/#src.kamae.spark.transformers.min_max_scale.MinMaxScaleParams.getMin","title":"getMin","text":"<pre><code>getMin()\n</code></pre> <p>Gets the min parameter.</p> <p>:returns: List of float min values.</p> Source code in <code>src/kamae/spark/transformers/min_max_scale.py</code> <pre><code>def getMin(self) -&gt; List[float]:\n    \"\"\"\n    Gets the min parameter.\n\n    :returns: List of float min values.\n    \"\"\"\n    return self.getOrDefault(self.min)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/min_max_scale/#src.kamae.spark.transformers.min_max_scale.MinMaxScaleParams.setMax","title":"setMax","text":"<pre><code>setMax(value)\n</code></pre> <p>Sets the parameter max to the given list value. Saves the max as a list of floats.</p> <p>:param value: List of max values. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/min_max_scale.py</code> <pre><code>def setMax(self, value: List[float]) -&gt; \"MinMaxScaleParams\":\n    \"\"\"\n    Sets the parameter max to the given list value.\n    Saves the max as a list of floats.\n\n    :param value: List of max values.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if None in set(value):\n        ids = [i for i, x in enumerate(value) if x is None]\n        raise ValueError(\"Got null Max values at positions: \", ids)\n    return self._set(max=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/min_max_scale/#src.kamae.spark.transformers.min_max_scale.MinMaxScaleParams.setMin","title":"setMin","text":"<pre><code>setMin(value)\n</code></pre> <p>Sets the parameter min to the given list value. Saves the min as a list of floats.</p> <p>:param value: List of min values. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/min_max_scale.py</code> <pre><code>def setMin(self, value: List[float]) -&gt; \"MinMaxScaleParams\":\n    \"\"\"\n    Sets the parameter min to the given list value.\n    Saves the min as a list of floats.\n\n    :param value: List of min values.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if None in set(value):\n        ids = [i for i, x in enumerate(value) if x is None]\n        raise ValueError(\"Got null Min values at positions: \", ids)\n    return self._set(min=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/min_max_scale/#src.kamae.spark.transformers.min_max_scale.MinMaxScaleTransformer","title":"MinMaxScaleTransformer","text":"<pre><code>MinMaxScaleTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    min=None,\n    max=None,\n    maskValue=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>MinMaxScaleParams</code>, <code>SingleInputSingleOutputParams</code></p> <p>MinMax scale transformer for use in Spark pipelines. This is used to standardize/transform the input column to the range [0, 1] using the minimum and maximum values.</p> <p>Formula: (x - min)/(max - min)</p> <p>WARNING: If the input is an array, we assume that the array has a constant shape across all rows.</p> <p>:param inputCol: Input column name to standardize. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. :param min: List of minimum values corresponding to the input column. :param max: List of maximum values corresponding to the input column. :param maskValue: Value which should be ignored in the min/max scaling process. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/min_max_scale.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    min: Optional[List[float]] = None,\n    max: Optional[List[float]] = None,\n    maskValue: Optional[float] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes a MinMaxScaleTransformer transformer.\n\n    :param inputCol: Input column name to standardize.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model.\n    :param min: List of minimum values corresponding to the input column.\n    :param max: List of maximum values corresponding to the\n    input column.\n    :param maskValue: Value which should be ignored in the min/max scaling process.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(maskValue=None)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/min_max_scale/#src.kamae.spark.transformers.min_max_scale.MinMaxScaleTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/min_max_scale/#src.kamae.spark.transformers.min_max_scale.MinMaxScaleTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset using the minimum and maximum values to standardize the input column.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Pyspark dataframe with the input column standardized,  named as the output column.</p> Source code in <code>src/kamae/spark/transformers/min_max_scale.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset using the minimum and maximum values\n    to standardize the input column.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Pyspark dataframe with the input column standardized,\n     named as the output column.\n    \"\"\"\n    original_input_datatype = self.get_column_datatype(dataset, self.getInputCol())\n    if not isinstance(original_input_datatype, ArrayType):\n        input_col = F.array(F.col(self.getInputCol()))\n        input_datatype = ArrayType(original_input_datatype)\n    else:\n        input_col = F.col(self.getInputCol())\n        input_datatype = original_input_datatype\n\n    shift = F.array([F.lit(m) for m in self.getMin()])\n    scale = F.array(\n        [\n            F.lit(1.0 / (m1 - m0) if m1 != m0 else 0.0)\n            for m0, m1 in zip(self.getMin(), self.getMax())\n        ]\n    )\n\n    output_col = single_input_single_output_array_transform(\n        input_col=input_col,\n        input_col_datatype=input_datatype,\n        func=lambda x: F.transform(\n            x,\n            lambda y, i: F.when(y == self.getMaskValue(), y).otherwise(\n                (y - F.lit(shift)[i]) * F.lit(scale)[i]\n            ),\n        ),\n    )\n\n    if not isinstance(original_input_datatype, ArrayType):\n        output_col = output_col.getItem(0)\n\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/min_max_scale/#src.kamae.spark.transformers.min_max_scale.MinMaxScaleTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the min max transformation.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter  that performs the standardization.</p> Source code in <code>src/kamae/spark/transformers/min_max_scale.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the min max transformation.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter\n     that performs the standardization.\n    \"\"\"\n    np_min = np.array(self.getMin())\n    np_max = np.array(self.getMax())\n    mask_value = self.getMaskValue()\n    return MinMaxScaleLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        min=np_min,\n        max=np_max,\n        mask_value=mask_value,\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/modulo/","title":"modulo","text":""},{"location":"reference/src/kamae/spark/transformers/modulo/#src.kamae.spark.transformers.modulo.ModuloParams","title":"ModuloParams","text":"<pre><code>ModuloParams()\n</code></pre> <p>               Bases: <code>Params</code></p> <p>Mixin class for divisor used in modulo transform layers.</p> Source code in <code>src/kamae/spark/transformers/modulo.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__()\n    self._setDefault(layerName=self.uid)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/modulo/#src.kamae.spark.transformers.modulo.ModuloParams.getDivisor","title":"getDivisor","text":"<pre><code>getDivisor()\n</code></pre> <p>Gets the value of the divisor parameter.</p> <p>:returns: Float divisor used in modulo transform.</p> Source code in <code>src/kamae/spark/transformers/modulo.py</code> <pre><code>def getDivisor(self) -&gt; float:\n    \"\"\"\n    Gets the value of the divisor parameter.\n\n    :returns: Float divisor used in modulo transform.\n    \"\"\"\n    return self.getOrDefault(self.divisor)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/modulo/#src.kamae.spark.transformers.modulo.ModuloParams.setDivisor","title":"setDivisor","text":"<pre><code>setDivisor(value)\n</code></pre> <p>Sets the value of the divisor parameter.</p> <p>:param value: Float constant used for math operations. :returns: Class instance.</p> Source code in <code>src/kamae/spark/transformers/modulo.py</code> <pre><code>def setDivisor(self, value: float) -&gt; \"ModuloParams\":\n    \"\"\"\n    Sets the value of the divisor parameter.\n\n    :param value: Float constant used for math operations.\n    :returns: Class instance.\n    \"\"\"\n    return self._set(divisor=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/modulo/#src.kamae.spark.transformers.modulo.ModuloTransformer","title":"ModuloTransformer","text":"<pre><code>ModuloTransformer(\n    inputCol=None,\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    divisor=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>MultiInputSingleOutputParams</code>, <code>ModuloParams</code></p> <p>ModuloLayer Spark Transformer for use in Spark pipelines. This transformer applies a modulo transform to the input column by dividing by the divisor parameter or another column.</p> <p>:param inputCol: Input column name. Only used if inputCols is not specified. If specified, then we use the <code>divisor</code> parameter as our modulo divisor. :param inputCols: Input column names. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param divisor: Optional constant to use in modulo operation. If not provided, then two input columns are required. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/modulo.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    divisor: Optional[float] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes an ModuloTransformer transformer.\n\n    :param inputCol: Input column name. Only used if inputCols is not specified.\n    If specified, then we use the `divisor` parameter as our modulo divisor.\n    :param inputCols: Input column names.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param divisor: Optional constant to use in modulo operation. If not provided,\n    then two input columns are required.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(divisor=None)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/modulo/#src.kamae.spark.transformers.modulo.ModuloTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/modulo/#src.kamae.spark.transformers.modulo.ModuloTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which is result of the modulo operation on the input column.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/modulo.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which is result of the modulo operation on the input column.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_cols = self.get_multiple_input_cols(\n        constant_param_name=\"divisor\",\n        input_cols_limit=2,\n    )\n    # input_cols can contain either actual columns or lit(constants). In order to\n    # determine the datatype of the input columns, we select them from the dataset\n    # first.\n    input_col_names = dataset.select(input_cols).columns\n    input_col_datatypes = [\n        self.get_column_datatype(dataset=dataset.select(input_cols), column_name=c)\n        for c in input_col_names\n    ]\n\n    output_col = multi_input_single_output_scalar_transform(\n        input_cols=input_cols,\n        input_col_names=input_col_names,\n        input_col_datatypes=input_col_datatypes,\n        func=lambda x: F.when(\n            # If the result of the modulo operation is negative,\n            # add the divisor to the result. This is to match the behavior of the\n            # tensorflow modulo layer.\n            x[input_col_names[0]] % x[input_col_names[1]] &gt;= 0,\n            x[input_col_names[0]] % x[input_col_names[1]],\n        ).otherwise(\n            (x[input_col_names[0]] % x[input_col_names[1]]) + x[input_col_names[1]]\n        ),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/modulo/#src.kamae.spark.transformers.modulo.ModuloTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the modulo transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs a modulo operation.</p> Source code in <code>src/kamae/spark/transformers/modulo.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the modulo transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs a modulo operation.\n    \"\"\"\n    return ModuloLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        divisor=self.getDivisor(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/modulo/#src.kamae.spark.transformers.modulo.ModuloTransformer.setInputCols","title":"setInputCols","text":"<pre><code>setInputCols(value)\n</code></pre> <p>Sets the value of the inputCols parameter.</p> <p>:param value: List of input column names. :returns: Class instance.</p> Source code in <code>src/kamae/spark/transformers/modulo.py</code> <pre><code>def setInputCols(self, value: List[str]) -&gt; \"ModuloTransformer\":\n    \"\"\"\n    Sets the value of the inputCols parameter.\n\n    :param value: List of input column names.\n    :returns: Class instance.\n    \"\"\"\n    if len(value) != 2:\n        raise ValueError(\"ModuloTransformer requires exactly two input columns.\")\n    return self._set(inputCols=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/multiply/","title":"multiply","text":""},{"location":"reference/src/kamae/spark/transformers/multiply/#src.kamae.spark.transformers.multiply.MultiplyTransformer","title":"MultiplyTransformer","text":"<pre><code>MultiplyTransformer(\n    inputCol=None,\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    mathFloatConstant=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>MultiInputSingleOutputParams</code>, <code>MathFloatConstantParams</code></p> <p>MultiplyLayer Spark Transformer for use in Spark pipelines. This transformer multiplies a column by a constant or another column.</p> <p>:param inputCol: Input column name. Only used if inputCols is not specified. If specified, we multiply this column by the mathFloatConstant. :param inputCols: Input column names. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param mathFloatConstant: Optional constant to multiply by. If not provided, then input columns are required. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/multiply.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    mathFloatConstant: Optional[float] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes an MultiplyTransformer transformer.\n\n    :param inputCol: Input column name. Only used if inputCols is not specified.\n    If specified, we multiply this column by the mathFloatConstant.\n    :param inputCols: Input column names.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param mathFloatConstant: Optional constant to multiply by. If not provided,\n    then input columns are required.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(mathFloatConstant=None)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/multiply/#src.kamae.spark.transformers.multiply.MultiplyTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/multiply/#src.kamae.spark.transformers.multiply.MultiplyTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which is the same as the column with name <code>inputCol</code>.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/multiply.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which is the same as the column with name `inputCol`.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_cols = self.get_multiple_input_cols(\n        constant_param_name=\"mathFloatConstant\"\n    )\n    # input_cols can contain either actual columns or lit(constants). In order to\n    # determine the datatype of the input columns, we select them from the dataset\n    # first.\n    input_col_names = dataset.select(input_cols).columns\n    input_col_datatypes = [\n        self.get_column_datatype(dataset=dataset.select(input_cols), column_name=c)\n        for c in input_col_names\n    ]\n\n    output_col = multi_input_single_output_scalar_transform(\n        input_cols=input_cols,\n        input_col_names=input_col_names,\n        input_col_datatypes=input_col_datatypes,\n        func=lambda x: reduce(mul, [x[c] for c in input_col_names]),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/multiply/#src.kamae.spark.transformers.multiply.MultiplyTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the multiply transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs a multiply operation.</p> Source code in <code>src/kamae/spark/transformers/multiply.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the multiply transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs a multiply operation.\n    \"\"\"\n    return MultiplyLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        multiplier=self.getMathFloatConstant(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/numerical_if_statement/","title":"numerical_if_statement","text":""},{"location":"reference/src/kamae/spark/transformers/numerical_if_statement/#src.kamae.spark.transformers.numerical_if_statement.NumericalIfStatementParams","title":"NumericalIfStatementParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing parameters needed for NumericalIfStatementTransformer transform layers.</p>"},{"location":"reference/src/kamae/spark/transformers/numerical_if_statement/#src.kamae.spark.transformers.numerical_if_statement.NumericalIfStatementParams.getConditionOperator","title":"getConditionOperator","text":"<pre><code>getConditionOperator()\n</code></pre> <p>Gets the conditionOperator parameter.</p> <p>:returns: String value describing the operator to use in condition: - eq - neq - lt - gt - leq - geq</p> Source code in <code>src/kamae/spark/transformers/numerical_if_statement.py</code> <pre><code>def getConditionOperator(self) -&gt; str:\n    \"\"\"\n    Gets the conditionOperator parameter.\n\n    :returns: String value describing the operator to use in condition:\n    - eq\n    - neq\n    - lt\n    - gt\n    - leq\n    - geq\n    \"\"\"\n    return self.getOrDefault(self.conditionOperator)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/numerical_if_statement/#src.kamae.spark.transformers.numerical_if_statement.NumericalIfStatementParams.getResultIfFalse","title":"getResultIfFalse","text":"<pre><code>getResultIfFalse()\n</code></pre> <p>Gets the resultIfFalse parameter.</p> <p>:returns: Float value to return if condition is false.</p> Source code in <code>src/kamae/spark/transformers/numerical_if_statement.py</code> <pre><code>def getResultIfFalse(self) -&gt; float:\n    \"\"\"\n    Gets the resultIfFalse parameter.\n\n    :returns: Float value to return if condition is false.\n    \"\"\"\n    return self.getOrDefault(self.resultIfFalse)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/numerical_if_statement/#src.kamae.spark.transformers.numerical_if_statement.NumericalIfStatementParams.getResultIfTrue","title":"getResultIfTrue","text":"<pre><code>getResultIfTrue()\n</code></pre> <p>Gets the resultIfTrue parameter.</p> <p>:returns: Float value to return if condition is true.</p> Source code in <code>src/kamae/spark/transformers/numerical_if_statement.py</code> <pre><code>def getResultIfTrue(self) -&gt; float:\n    \"\"\"\n    Gets the resultIfTrue parameter.\n\n    :returns: Float value to return if condition is true.\n    \"\"\"\n    return self.getOrDefault(self.resultIfTrue)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/numerical_if_statement/#src.kamae.spark.transformers.numerical_if_statement.NumericalIfStatementParams.getValueToCompare","title":"getValueToCompare","text":"<pre><code>getValueToCompare()\n</code></pre> <p>Gets the valueToCompare parameter.</p> <p>:returns: Float value to compare to input column.</p> Source code in <code>src/kamae/spark/transformers/numerical_if_statement.py</code> <pre><code>def getValueToCompare(self) -&gt; float:\n    \"\"\"\n    Gets the valueToCompare parameter.\n\n    :returns: Float value to compare to input column.\n    \"\"\"\n    return self.getOrDefault(self.valueToCompare)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/numerical_if_statement/#src.kamae.spark.transformers.numerical_if_statement.NumericalIfStatementParams.setConditionOperator","title":"setConditionOperator","text":"<pre><code>setConditionOperator(value)\n</code></pre> <p>Sets the conditionOperator parameter.</p> <p>:param value: String value describing the operator to use in condition: - eq - neq - lt - gt - leq - geq :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/numerical_if_statement.py</code> <pre><code>def setConditionOperator(self, value: str) -&gt; \"NumericalIfStatementParams\":\n    \"\"\"\n    Sets the conditionOperator parameter.\n\n    :param value: String value describing the operator to use in condition:\n    - eq\n    - neq\n    - lt\n    - gt\n    - leq\n    - geq\n    :returns: Instance of class mixed in.\n    \"\"\"\n    allowed_operators = [\"eq\", \"neq\", \"lt\", \"gt\", \"leq\", \"geq\"]\n    if value not in allowed_operators:\n        raise ValueError(\n            f\"conditionOperator must be one of {allowed_operators}, but got {value}\"\n        )\n    return self._set(conditionOperator=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/numerical_if_statement/#src.kamae.spark.transformers.numerical_if_statement.NumericalIfStatementParams.setResultIfFalse","title":"setResultIfFalse","text":"<pre><code>setResultIfFalse(value)\n</code></pre> <p>Sets the resultIfFalse parameter.</p> <p>:param value: Float value to return if condition is false. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/numerical_if_statement.py</code> <pre><code>def setResultIfFalse(self, value: float) -&gt; \"NumericalIfStatementParams\":\n    \"\"\"\n    Sets the resultIfFalse parameter.\n\n    :param value: Float value to return if condition is false.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(resultIfFalse=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/numerical_if_statement/#src.kamae.spark.transformers.numerical_if_statement.NumericalIfStatementParams.setResultIfTrue","title":"setResultIfTrue","text":"<pre><code>setResultIfTrue(value)\n</code></pre> <p>Sets the resultIfTrue parameter.</p> <p>:param value: Float value to return if condition is true. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/numerical_if_statement.py</code> <pre><code>def setResultIfTrue(self, value: float) -&gt; \"NumericalIfStatementParams\":\n    \"\"\"\n    Sets the resultIfTrue parameter.\n\n    :param value: Float value to return if condition is true.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(resultIfTrue=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/numerical_if_statement/#src.kamae.spark.transformers.numerical_if_statement.NumericalIfStatementParams.setValueToCompare","title":"setValueToCompare","text":"<pre><code>setValueToCompare(value)\n</code></pre> <p>Sets the valueToCompare parameter.</p> <p>:param value: Float value to compare to input column. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/numerical_if_statement.py</code> <pre><code>def setValueToCompare(self, value: float) -&gt; \"NumericalIfStatementParams\":\n    \"\"\"\n    Sets the valueToCompare parameter.\n\n    :param value: Float value to compare to input column.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(valueToCompare=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/numerical_if_statement/#src.kamae.spark.transformers.numerical_if_statement.NumericalIfStatementTransformer","title":"NumericalIfStatementTransformer","text":"<pre><code>NumericalIfStatementTransformer(\n    inputCol=None,\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    conditionOperator=None,\n    valueToCompare=None,\n    resultIfTrue=None,\n    resultIfFalse=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>MultiInputSingleOutputParams</code>, <code>NumericalIfStatementParams</code></p> <p>NumericalIfStatement Spark Transformer for use in Spark pipelines. This transformer computes an if statement between a set of numerical constants and columns.</p> <p>:param inputCol: Input column name. Only used if inputCols is not specified. If specified, then all other aspects of the if statement are constant. :param inputCols: Input column names. List of input columns to in the case where the if statement is not constant. Must be specified in the order [valueToCompare, resultIfTrue, resultIfFalse]. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param conditionOperator: Operator to use in condition: eq, neq, lt, gt, leq, geq. :param valueToCompare: Optional float value to compare to input column. If not specified, then assumed to be the first input column. :param resultIfTrue: Optional float value to return if condition is true. If not specified, then assumed to be the second input column. :param resultIfFalse: Optional float value to return if condition is false. If not specified, then assumed to be the third input column. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/numerical_if_statement.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    conditionOperator: Optional[str] = None,\n    valueToCompare: Optional[float] = None,\n    resultIfTrue: Optional[float] = None,\n    resultIfFalse: Optional[float] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes a NumericalIfStatementTransformer transformer.\n\n    :param inputCol: Input column name. Only used if inputCols is not specified.\n    If specified, then all other aspects of the if statement are constant.\n    :param inputCols: Input column names. List of input columns to in the case\n    where the if statement is not constant. Must be specified in the order\n    [valueToCompare, resultIfTrue, resultIfFalse].\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param conditionOperator: Operator to use in condition:\n    eq, neq, lt, gt, leq, geq.\n    :param valueToCompare: Optional float value to compare to input column.\n    If not specified, then assumed to be the first input column.\n    :param resultIfTrue: Optional float value to return if condition is true.\n    If not specified, then assumed to be the second input column.\n    :param resultIfFalse: Optional float value to return if condition is false.\n    If not specified, then assumed to be the third input column.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(\n        valueToCompare=None,\n        resultIfTrue=None,\n        resultIfFalse=None,\n    )\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/numerical_if_statement/#src.kamae.spark.transformers.numerical_if_statement.NumericalIfStatementTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/numerical_if_statement/#src.kamae.spark.transformers.numerical_if_statement.NumericalIfStatementTransformer._construct_input_cols","title":"_construct_input_cols","text":"<pre><code>_construct_input_cols()\n</code></pre> <p>Constructs the input columns for the transformer. This is fairly complex since the user can set any of the following: - valueToCompare - resultIfTrue - resultIfFalse</p> <p>If all of these are set, then the user should have provided a single <code>inputCol</code>. Otherwise, if any of these are not set, then the user should have provided <code>inputCols</code> containing the missing values. The <code>inputCols</code> should be in the order [valueToCompare, resultIfTrue, resultIfFalse]. But if the user has specified, for example, <code>resultIfTrue</code> as a constant then the <code>inputCols</code> should be in the order [valueToCompare, resultIfFalse].</p> <p>:returns: Tuple of 4 pyspark columns.</p> Source code in <code>src/kamae/spark/transformers/numerical_if_statement.py</code> <pre><code>def _construct_input_cols(self) -&gt; List[Column]:\n    \"\"\"\n    Constructs the input columns for the transformer. This is fairly complex\n    since the user can set any of the following:\n    - valueToCompare\n    - resultIfTrue\n    - resultIfFalse\n\n    If all of these are set, then the user should have provided a single `inputCol`.\n    Otherwise, if any of these are not set, then the user should have provided\n    `inputCols` containing the missing values. The `inputCols` should be in the\n    order [valueToCompare, resultIfTrue, resultIfFalse]. But if the user has\n    specified, for example, `resultIfTrue` as a constant then the\n    `inputCols` should be in the order [valueToCompare, resultIfFalse].\n\n    :returns: Tuple of 4 pyspark columns.\n    \"\"\"\n    optional_constant_cols = [\"valueToCompare\", \"resultIfTrue\", \"resultIfFalse\"]\n    optional_constants_defined = {\n        const: self.getOrDefault(const) is not None\n        for const in optional_constant_cols\n    }\n\n    if self.isDefined(\"inputCols\"):\n        # If the user has set inputCols, then some or all of the optional constants\n        # are defined as input column variables\n        input_cols = self.getInputCols()\n        if len(input_cols) + sum(optional_constants_defined.values()) != 4:\n            raise ValueError(\n                f\"\"\"Total number of input columns and constants must be 4,\n                but got {len(input_cols)} input columns and\n                {sum(optional_constants_defined.values())} constants.\"\"\"\n            )\n        # The first input column is always the value to compare\n        input_col_list = [\n            F.col(input_cols[0]),\n        ]\n        input_col_counter = 1\n        for const_col in optional_constant_cols:\n            # Loop through the optional constant names.\n            # If the constant is not defined then it must be an input column.\n            # Otherwise, it is a literal value.\n            if not optional_constants_defined[const_col]:\n                input_col_list.append(F.col(input_cols[input_col_counter]))\n                input_col_counter += 1\n            else:\n                input_col_list.append(F.lit(self.getOrDefault(const_col)))\n        return [\n            input_col_list[0].alias(self.uid + \"_inputCol\"),\n            input_col_list[1].alias(self.uid + \"_valueToCompare\"),\n            input_col_list[2].alias(self.uid + \"_resultIfTrue\"),\n            input_col_list[3].alias(self.uid + \"_resultIfFalse\"),\n        ]\n    elif self.isDefined(\"inputCol\"):\n        # If the user has set inputCol, then all the optional constants\n        # must be defined.\n        if not all(\n            [\n                self.getOrDefault(const) is not None\n                for const in optional_constant_cols\n            ]\n        ):\n            raise ValueError(\n                f\"\"\"Must specify all of {optional_constant_cols}\"\n                if inputCol is specified.\"\"\"\n            )\n        return [\n            F.col(self.getInputCol()).alias(self.uid + \"_inputCol\"),\n            F.lit(self.getValueToCompare()).alias(self.uid + \"_valueToCompare\"),\n            F.lit(self.getResultIfTrue()).alias(self.uid + \"_resultIfTrue\"),\n            F.lit(self.getResultIfFalse()).alias(self.uid + \"_resultIfFalse\"),\n        ]\n    else:\n        raise ValueError(\"Must specify either inputCol or inputCols.\")\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/numerical_if_statement/#src.kamae.spark.transformers.numerical_if_statement.NumericalIfStatementTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which is the output of an if statement with condition:</p> <p>IF <code>inputCol</code> <code>conditionOperator</code> <code>valueToCompare</code> THEN <code>resultIfTrue</code> ELSE <code>resultIfFalse</code></p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/numerical_if_statement.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which is the output of an if statement with condition:\n\n    IF `inputCol` `conditionOperator` `valueToCompare`\n    THEN `resultIfTrue`\n    ELSE `resultIfFalse`\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_cols = self._construct_input_cols()\n    # input_cols can contain either actual columns or lit(constants). In order to\n    # determine the datatype of the input columns, we select them from the dataset\n    # first.\n    input_col_names = dataset.select(input_cols).columns\n    input_col_datatypes = [\n        self.get_column_datatype(dataset=dataset.select(input_cols), column_name=c)\n        for c in input_col_names\n    ]\n    condition_operator = get_condition_operator(self.getConditionOperator())\n\n    output_col = multi_input_single_output_scalar_transform(\n        input_cols=input_cols,\n        input_col_names=input_col_names,\n        input_col_datatypes=input_col_datatypes,\n        func=lambda x: F.when(\n            condition_operator(x[input_col_names[0]], x[input_col_names[1]]),\n            x[input_col_names[2]],\n        ).otherwise(x[input_col_names[3]]),\n    )\n\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/numerical_if_statement/#src.kamae.spark.transformers.numerical_if_statement.NumericalIfStatementTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the numerical if statement transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs the numerical if statement.</p> Source code in <code>src/kamae/spark/transformers/numerical_if_statement.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the numerical if statement transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs the numerical if statement.\n    \"\"\"\n    if not self.isDefined(\"conditionOperator\"):\n        raise ValueError(\"Must specify conditionOperator to use tensorflow layer.\")\n\n    return NumericalIfStatementLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        condition_operator=self.getConditionOperator(),\n        value_to_compare=self.getValueToCompare(),\n        result_if_true=self.getResultIfTrue(),\n        result_if_false=self.getResultIfFalse(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/numerical_if_statement/#src.kamae.spark.transformers.numerical_if_statement.NumericalIfStatementTransformer.setInputCols","title":"setInputCols","text":"<pre><code>setInputCols(value)\n</code></pre> <p>Sets the inputCols parameter, throwing an error if the total length of the inputCols and the constants are more than 4.</p> <p>:param value: List of input column names. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/numerical_if_statement.py</code> <pre><code>def setInputCols(self, value: List[str]) -&gt; \"NumericalIfStatementTransformer\":\n    \"\"\"\n    Sets the inputCols parameter, throwing an error if the total length of the\n    inputCols and the constants are more than 4.\n\n    :param value: List of input column names.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    optional_constant_cols = [\"valueToCompare\", \"resultIfTrue\", \"resultIfFalse\"]\n    num_defined_constants = len(\n        [c for c in optional_constant_cols if self.getOrDefault(c) is not None]\n    )\n    if len(value) + num_defined_constants &gt; 4:\n        raise ValueError(\n            f\"\"\"Total number of input columns and constants cannot be more than 4,\n            but got {len(value)} input columns and\n            {num_defined_constants} constants.\"\"\"\n        )\n\n    return self._set(inputCols=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/one_hot_encode/","title":"one_hot_encode","text":""},{"location":"reference/src/kamae/spark/transformers/one_hot_encode/#src.kamae.spark.transformers.one_hot_encode.OneHotEncodeTransformer","title":"OneHotEncodeTransformer","text":"<pre><code>OneHotEncodeTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    labelsArray=None,\n    stringOrderType=\"frequencyDesc\",\n    maskToken=None,\n    numOOVIndices=1,\n    dropUnseen=False,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>DropUnseenParams</code>, <code>StringIndexParams</code>, <code>SingleInputSingleOutputParams</code></p> <p>OneHotEncodeTransformer Spark Transformer for use in Spark pipelines. This transformer is used to one-hot feature columns using the string labels collected by the OneHotEncodeEstimator.</p> <p>NOTE: If your data contains null characters: https://en.wikipedia.org/wiki/Null_character This transformer could fail since the hashing algorithm uses cannot accept null characters. If you have null characters in your data, you should remove them.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param labelsArray: List of string labels to use for one-hot encoding. :param: stringOrderType: How to order the string indices. Options are 'frequencyAsc', 'frequencyDesc', 'alphabeticalAsc', 'alphabeticalDesc'. Defaults to 'frequencyDesc'. :param maskToken: Token to use for masking. If set, the token will be indexed as 0. :param numOOVIndices: Number of out of vocabulary indices to use. The out of vocabulary indices are used to represent unseen labels and are placed at the beginning of the one-hot encoding. Defaults to 1. :param dropUnseen: Whether to drop unseen label indices. If set to True, the transformer will not add an extra dimension for unseen labels in the one-hot encoding. Defaults to False. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/one_hot_encode.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    labelsArray: Optional[List[str]] = None,\n    stringOrderType: str = \"frequencyDesc\",\n    maskToken: Optional[str] = None,\n    numOOVIndices: int = 1,\n    dropUnseen: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initializes the OneHotEncodeTransformer transformer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param labelsArray: List of string labels to use for one-hot encoding.\n    :param: stringOrderType: How to order the string indices.\n    Options are 'frequencyAsc', 'frequencyDesc', 'alphabeticalAsc',\n    'alphabeticalDesc'. Defaults to 'frequencyDesc'.\n    :param maskToken: Token to use for masking.\n    If set, the token will be indexed as 0.\n    :param numOOVIndices: Number of out of vocabulary indices to use. The\n    out of vocabulary indices are used to represent unseen labels and are\n    placed at the beginning of the one-hot encoding. Defaults to 1.\n    :param dropUnseen: Whether to drop unseen label indices. If set to True,\n    the transformer will not add an extra dimension for unseen labels in the\n    one-hot encoding. Defaults to False.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(\n        stringOrderType=\"frequencyDesc\",\n        numOOVIndices=1,\n        dropUnseen=False,\n        maskToken=None,\n    )\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/one_hot_encode/#src.kamae.spark.transformers.one_hot_encode.OneHotEncodeTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/one_hot_encode/#src.kamae.spark.transformers.one_hot_encode.OneHotEncodeTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset using the string index labels assigning array of one-hot encoded values to the output column.</p> <p>:param dataset: Pyspark dataframe to transform.</p> <p>:returns: Pyspark dataframe with the input column one-hot encoded,  named as the output column.</p> Source code in <code>src/kamae/spark/transformers/one_hot_encode.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset using the string index labels assigning array of\n    one-hot encoded values to the output column.\n\n    :param dataset: Pyspark dataframe to transform.\n\n    :returns: Pyspark dataframe with the input column one-hot encoded,\n     named as the output column.\n    \"\"\"\n    labels = self.getLabelsArray()\n    ohe_num_oov_indices = self.getNumOOVIndices()\n    mask_token = self.getMaskToken()\n    drop_unseen = self.getDropUnseen()\n\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n    output_col = single_input_single_output_scalar_udf_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: one_hot_encoding_udf(\n            label=x,\n            labels=labels,\n            num_oov_indices=ohe_num_oov_indices,\n            mask_token=mask_token,\n            drop_unseen=drop_unseen,\n        ),\n        udf_return_element_datatype=ArrayType(FloatType()),\n    )\n\n    return dataset.withColumn(\n        self.getOutputCol(),\n        output_col,\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/one_hot_encode/#src.kamae.spark.transformers.one_hot_encode.OneHotEncodeTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the one-hot encoder transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that performs the one-hot encoding.</p> Source code in <code>src/kamae/spark/transformers/one_hot_encode.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the one-hot encoder transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter\n    that performs the one-hot encoding.\n    \"\"\"\n    return OneHotEncodeLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        vocabulary=self.getLabelsArray(),\n        num_oov_indices=self.getNumOOVIndices(),\n        mask_token=self.getMaskToken(),\n        drop_unseen=self.getDropUnseen(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/ordinal_array_encode/","title":"ordinal_array_encode","text":""},{"location":"reference/src/kamae/spark/transformers/ordinal_array_encode/#src.kamae.spark.transformers.ordinal_array_encode.OrdinalArrayEncodeTransformer","title":"OrdinalArrayEncodeTransformer","text":"<pre><code>OrdinalArrayEncodeTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    padValue=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>PadValueParams</code></p> <p>Transformer that encodes an array of strings into an array of integers.</p> <p>The transformer will map each unique string in the array to an integer, according to the order in which they appear in the array. It will also ignore the pad value if specified.</p> <p>:param outputCol: Output column name. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer :param padValue: The value to be considered as padding. Defaults to <code>None</code>. :returns: None</p> Source code in <code>src/kamae/spark/transformers/ordinal_array_encode.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    padValue: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialises the OrdinalArrayEncodeTransformer\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    :param padValue: The value to be considered as padding. Defaults to `None`.\n    :returns: None\n    \"\"\"\n    super().__init__()\n    self._setDefault(padValue=None)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/ordinal_array_encode/#src.kamae.spark.transformers.ordinal_array_encode.OrdinalArrayEncodeTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/ordinal_array_encode/#src.kamae.spark.transformers.ordinal_array_encode.OrdinalArrayEncodeTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Performs the ordinal encoding on the input dataset. Example:  dataset = spark.Dataframe(     [         ['a', 'a', 'a', 'b', 'c', '-1', '-1', '-1'],         ['x', 'x', 'x', 'x', 'y', 'z', '-1', '-1'],     ],     'input_col'  )  Output: spark.Dataframe(     [         ['a', 'a', 'a', 'b', 'c', '-1', '-1', '-1'],         ['x', 'x', 'x', 'x', 'y', 'z', '-1', '-1'],     ],     [         [0, 0, 0, 1, 2, -1, -1, -1],         [0, 0, 0, 0, 1, 2, -1, -1],     ],     'input_col', 'output_col' ) :param dataset: The input dataframe. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/ordinal_array_encode.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Performs the ordinal encoding on the input dataset.\n    Example:\n     dataset = spark.Dataframe(\n        [\n            ['a', 'a', 'a', 'b', 'c', '-1', '-1', '-1'],\n            ['x', 'x', 'x', 'x', 'y', 'z', '-1', '-1'],\n        ],\n        'input_col'\n     )\n     Output: spark.Dataframe(\n        [\n            ['a', 'a', 'a', 'b', 'c', '-1', '-1', '-1'],\n            ['x', 'x', 'x', 'x', 'y', 'z', '-1', '-1'],\n        ],\n        [\n            [0, 0, 0, 1, 2, -1, -1, -1],\n            [0, 0, 0, 0, 1, 2, -1, -1],\n        ],\n        'input_col', 'output_col'\n    )\n    :param dataset: The input dataframe.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n    if not isinstance(input_datatype, ArrayType):\n        raise ValueError(\n            f\"\"\"Input column {self.getInputCol()} must be of ArrayType.\n                    Got {input_datatype} instead.\"\"\"\n        )\n\n    output_col = single_input_single_output_array_udf_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: ordinal_array_encode_udf(x, self.getPadValue()),\n        udf_return_element_datatype=IntegerType(),\n    )\n\n    return dataset.withColumn(\n        self.getOutputCol(),\n        output_col,\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/ordinal_array_encode/#src.kamae.spark.transformers.ordinal_array_encode.OrdinalArrayEncodeTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer that performs the ordinal array encoding.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that performs the ordinal array encoding operation.</p> Source code in <code>src/kamae/spark/transformers/ordinal_array_encode.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer that performs the ordinal array encoding.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter\n    that performs the ordinal array encoding operation.\n    \"\"\"\n    return OrdinalArrayEncodeLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        pad_value=self.getPadValue(),\n        axis=-1,\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/round/","title":"round","text":""},{"location":"reference/src/kamae/spark/transformers/round/#src.kamae.spark.transformers.round.RoundParams","title":"RoundParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing roundType parameter needed for rounding transform layers.</p>"},{"location":"reference/src/kamae/spark/transformers/round/#src.kamae.spark.transformers.round.RoundParams.getRoundType","title":"getRoundType","text":"<pre><code>getRoundType()\n</code></pre> <p>Gets the roundType parameter.</p> <p>:returns: Rounding type to use in round transform, one of 'floor', 'ceil' or 'round'.</p> Source code in <code>src/kamae/spark/transformers/round.py</code> <pre><code>def getRoundType(self) -&gt; str:\n    \"\"\"\n    Gets the roundType parameter.\n\n    :returns: Rounding type to use in round transform,\n    one of 'floor', 'ceil' or 'round'.\n    \"\"\"\n    return self.getOrDefault(self.roundType)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/round/#src.kamae.spark.transformers.round.RoundParams.setRoundType","title":"setRoundType","text":"<pre><code>setRoundType(value)\n</code></pre> <p>Sets the roundType parameter.</p> <p>:param value: Rounding type to use in round transform, one of 'floor', 'ceil' or 'round'. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/round.py</code> <pre><code>def setRoundType(self, value: str) -&gt; \"RoundParams\":\n    \"\"\"\n    Sets the roundType parameter.\n\n    :param value: Rounding type to use in round transform,\n    one of 'floor', 'ceil' or 'round'.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if value not in [\"floor\", \"ceil\", \"round\"]:\n        raise ValueError(\"roundType must be one of 'floor', 'ceil' or 'round'\")\n    return self._set(roundType=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/round/#src.kamae.spark.transformers.round.RoundTransformer","title":"RoundTransformer","text":"<pre><code>RoundTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    roundType=\"round\",\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>RoundParams</code></p> <p>Round Spark Transformer for use in Spark pipelines. This transformer rounds the input column to the nearest integer using the specified rounding type.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param roundType: Rounding type to use in round transform, one of 'floor', 'ceil' or 'round'. Defaults to 'round'. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/round.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    roundType: str = \"round\",\n) -&gt; None:\n    \"\"\"\n    Initializes an RoundTransformer transformer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param roundType: Rounding type to use in round transform,\n    one of 'floor', 'ceil' or 'round'. Defaults to 'round'.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    kwargs = self._input_kwargs\n    self._setDefault(roundType=\"round\")\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/round/#src.kamae.spark.transformers.round.RoundTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/round/#src.kamae.spark.transformers.round.RoundTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which applies the rounding operation to the input column.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/round.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which applies the rounding operation to the input column.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    func_dict = {\n        \"floor\": F.floor,\n        \"ceil\": F.ceil,\n        \"round\": F.round,\n    }\n\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n    output_col = single_input_single_output_scalar_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: func_dict[self.getRoundType()](x),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/round/#src.kamae.spark.transformers.round.RoundTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the round transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs a rounding operation.</p> Source code in <code>src/kamae/spark/transformers/round.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the round transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs a rounding operation.\n    \"\"\"\n    return RoundLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        round_type=self.getRoundType(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/round_to_decimal/","title":"round_to_decimal","text":""},{"location":"reference/src/kamae/spark/transformers/round_to_decimal/#src.kamae.spark.transformers.round_to_decimal.RoundToDecimalParams","title":"RoundToDecimalParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing decimals parameter needed for rounding transform layers.</p>"},{"location":"reference/src/kamae/spark/transformers/round_to_decimal/#src.kamae.spark.transformers.round_to_decimal.RoundToDecimalParams.getDecimals","title":"getDecimals","text":"<pre><code>getDecimals()\n</code></pre> <p>Gets the decimals parameter.</p> <p>:returns: Number of decimals to round to.</p> Source code in <code>src/kamae/spark/transformers/round_to_decimal.py</code> <pre><code>def getDecimals(self) -&gt; int:\n    \"\"\"\n    Gets the decimals parameter.\n\n    :returns: Number of decimals to round to.\n    \"\"\"\n    return self.getOrDefault(self.decimals)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/round_to_decimal/#src.kamae.spark.transformers.round_to_decimal.RoundToDecimalParams.setDecimals","title":"setDecimals","text":"<pre><code>setDecimals(value)\n</code></pre> <p>Sets the decimals parameter.</p> <p>:param value: Number of decimals to round to. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/round_to_decimal.py</code> <pre><code>def setDecimals(self, value: int) -&gt; \"RoundToDecimalParams\":\n    \"\"\"\n    Sets the decimals parameter.\n\n    :param value: Number of decimals to round to.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if value &lt; 0:\n        raise ValueError(\"decimals must be a non-negative integer\")\n    return self._set(decimals=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/round_to_decimal/#src.kamae.spark.transformers.round_to_decimal.RoundToDecimalTransformer","title":"RoundToDecimalTransformer","text":"<pre><code>RoundToDecimalTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    decimals=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>RoundToDecimalParams</code></p> <p>Round Spark Transformer for use in Spark pipelines. This transformer rounds the input column to the nearest decimal using the specified number of decimals.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param decimals: Number of decimals to round to. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/round_to_decimal.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    decimals: Optional[int] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes an RoundTransformer transformer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param decimals: Number of decimals to round to.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/round_to_decimal/#src.kamae.spark.transformers.round_to_decimal.RoundToDecimalTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/round_to_decimal/#src.kamae.spark.transformers.round_to_decimal.RoundToDecimalTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which applies the rounding operation to the input column.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/round_to_decimal.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which applies the rounding operation to the input column.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n    output_col = single_input_single_output_scalar_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: F.round(x, scale=self.getDecimals()),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/round_to_decimal/#src.kamae.spark.transformers.round_to_decimal.RoundToDecimalTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the round transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs a rounding operation.</p> Source code in <code>src/kamae/spark/transformers/round_to_decimal.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the round transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs a rounding operation.\n    \"\"\"\n    return RoundToDecimalLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        decimals=self.getDecimals(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/shared_one_hot_encode/","title":"shared_one_hot_encode","text":""},{"location":"reference/src/kamae/spark/transformers/shared_one_hot_encode/#src.kamae.spark.transformers.shared_one_hot_encode.SharedOneHotEncodeTransformer","title":"SharedOneHotEncodeTransformer","text":"<pre><code>SharedOneHotEncodeTransformer(\n    inputCols=None,\n    outputCols=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    labelsArray=None,\n    stringOrderType=\"frequencyDesc\",\n    maskToken=None,\n    numOOVIndices=1,\n    dropUnseen=False,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>MultiInputMultiOutputParams</code>, <code>StringIndexParams</code>, <code>DropUnseenParams</code></p> <p>SharedOneHotEncodeTransformer Spark Transformer for use in Spark pipelines. This transformer is used to one-hot encode multiple input columns using the string labels collected by the SharedOneHotEncodeEstimator.</p> <p>NOTE: If your data contains null characters: https://en.wikipedia.org/wiki/Null_character This transformer could fail since the hashing algorithm uses cannot accept null characters. If you have null characters in your data, you should remove them.</p> <p>:param inputCols: List of input column names. :param outputCols: List of output column name. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param labelsArray: List of string labels to use for one-hot encoding. :param: stringOrderType: How to order the string indices. Options are 'frequencyAsc', 'frequencyDesc', 'alphabeticalAsc', 'alphabeticalDesc'. Defaults to 'frequencyDesc'. :param maskToken: Token to use for masking. If set, the token will be indexed as 0. :param numOOVIndices: Number of out of vocabulary indices to use. The out of vocabulary indices are used to represent unseen labels and are placed at the beginning of the one-hot encoding. Defaults to 1. :param dropUnseen: Whether to drop unseen label indices. If set to True, the transformer will not add an extra dimension for unseen labels in the one-hot encoding. Defaults to False. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/shared_one_hot_encode.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCols: Optional[List[str]] = None,\n    outputCols: Optional[List[str]] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    labelsArray: Optional[List[str]] = None,\n    stringOrderType: str = \"frequencyDesc\",\n    maskToken: Optional[str] = None,\n    numOOVIndices: int = 1,\n    dropUnseen: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initializes the SharedOneHotEncodeTransformer transformer.\n\n    :param inputCols: List of input column names.\n    :param outputCols: List of output column name.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param labelsArray: List of string labels to use for one-hot encoding.\n    :param: stringOrderType: How to order the string indices.\n    Options are 'frequencyAsc', 'frequencyDesc', 'alphabeticalAsc',\n    'alphabeticalDesc'. Defaults to 'frequencyDesc'.\n    :param maskToken: Token to use for masking.\n    If set, the token will be indexed as 0.\n    :param numOOVIndices: Number of out of vocabulary indices to use. The\n    out of vocabulary indices are used to represent unseen labels and are\n    placed at the beginning of the one-hot encoding. Defaults to 1.\n    :param dropUnseen: Whether to drop unseen label indices. If set to True,\n    the transformer will not add an extra dimension for unseen labels in the\n    one-hot encoding. Defaults to False.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(\n        stringOrderType=\"frequencyDesc\",\n        numOOVIndices=1,\n        dropUnseen=False,\n        maskToken=None,\n    )\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/shared_one_hot_encode/#src.kamae.spark.transformers.shared_one_hot_encode.SharedOneHotEncodeTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/shared_one_hot_encode/#src.kamae.spark.transformers.shared_one_hot_encode.SharedOneHotEncodeTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset using the string index labels assigning array of one-hot encoded values to the output columns.</p> <p>:param dataset: Pyspark dataframe to transform.</p> <p>:returns: Pyspark dataframe with the input column one-hot encoded,  named as the output column.</p> Source code in <code>src/kamae/spark/transformers/shared_one_hot_encode.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset using the string index labels assigning array of\n    one-hot encoded values to the output columns.\n\n    :param dataset: Pyspark dataframe to transform.\n\n    :returns: Pyspark dataframe with the input column one-hot encoded,\n     named as the output column.\n    \"\"\"\n    labels = self.getLabelsArray()\n    ohe_num_oov_indices = self.getNumOOVIndices()\n    mask_token = self.getMaskToken()\n    drop_unseen = self.getDropUnseen()\n\n    # Assumption made that all the input columns have the same datatype/nesting.\n    input_datatypes = [\n        self.get_column_datatype(dataset=dataset, column_name=c)\n        for c in self.getInputCols()\n    ]\n\n    output_cols = []\n    for i, column in enumerate(self.getInputCols()):\n        output_col = single_input_single_output_scalar_udf_transform(\n            input_col=F.col(column),\n            input_col_datatype=input_datatypes[i],\n            func=lambda x: one_hot_encoding_udf(\n                label=x,\n                labels=labels,\n                num_oov_indices=ohe_num_oov_indices,\n                mask_token=mask_token,\n                drop_unseen=drop_unseen,\n            ),\n            udf_return_element_datatype=ArrayType(FloatType()),\n        )\n        output_cols.append(output_col.alias(self.getOutputCols()[i]))\n\n    select_cols = [F.col(c) for c in dataset.columns] + output_cols\n\n    return dataset.select(*select_cols)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/shared_one_hot_encode/#src.kamae.spark.transformers.shared_one_hot_encode.SharedOneHotEncodeTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the list of tensorflow layers for the shared onehot encoder transformer. We need to use a list as each layer could operate on differing input shapes.</p> <p>:returns: List of Tensorflow keras layer with name equal to the layerName parameter and the input column name, that performs the indexing.</p> Source code in <code>src/kamae/spark/transformers/shared_one_hot_encode.py</code> <pre><code>def get_tf_layer(self) -&gt; List[tf.keras.layers.Layer]:\n    \"\"\"\n    Gets the list of tensorflow layers for the shared onehot encoder transformer.\n    We need to use a list as each layer could operate on differing input shapes.\n\n    :returns: List of Tensorflow keras layer with name equal to the layerName\n    parameter and the input column name, that performs the indexing.\n    \"\"\"\n    return [\n        OneHotEncodeLayer(\n            name=f\"{self.getLayerName()}_{input_name}\",\n            input_dtype=self.getInputTFDtype(),\n            output_dtype=self.getOutputTFDtype(),\n            vocabulary=self.getLabelsArray(),\n            num_oov_indices=self.getNumOOVIndices(),\n            mask_token=self.getMaskToken(),\n            drop_unseen=self.getDropUnseen(),\n        )\n        for input_name in self.getInputCols()\n    ]\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/shared_string_index/","title":"shared_string_index","text":""},{"location":"reference/src/kamae/spark/transformers/shared_string_index/#src.kamae.spark.transformers.shared_string_index.SharedStringIndexTransformer","title":"SharedStringIndexTransformer","text":"<pre><code>SharedStringIndexTransformer(\n    inputCols=None,\n    outputCols=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    labelsArray=None,\n    stringOrderType=None,\n    maskToken=None,\n    numOOVIndices=1,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>StringIndexParams</code>, <code>MultiInputMultiOutputParams</code></p> <p>SharedStringIndexTransformer Spark Transformer for use in Spark pipelines. This transformer is used to index/transform feature columns using the string labels collected by the SharedStringIndexEstimator.</p> <p>NOTE: If your data contains null characters: https://en.wikipedia.org/wiki/Null_character This transformer could fail since the hashing algorithm uses cannot accept null characters. If you have null characters in your data, you should remove them.</p> <p>:param inputCols: Input column names. :param outputCols: Output column names. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column(s) to after transforming. Must be the same length as inputCols. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param: stringOrderType: How to order the string indices. Options are 'frequencyAsc', 'frequencyDesc', 'alphabeticalAsc', 'alphabeticalDesc'. :param maskToken: Token to use for masking. If set, the token will be indexed as 0. :param numOOVIndices: Number of out of vocabulary indices to use. Default is 1. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/shared_string_index.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCols: Optional[List[str]] = None,\n    outputCols: Optional[List[str]] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    labelsArray: Optional[List[str]] = None,\n    stringOrderType: Optional[str] = None,\n    maskToken: Optional[str] = None,\n    numOOVIndices: int = 1,\n) -&gt; None:\n    \"\"\"\n    Initializes the SharedStringIndexTransformer transformer.\n\n    :param inputCols: Input column names.\n    :param outputCols: Output column names.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column(s) to after\n    transforming. Must be the same length as inputCols.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param: stringOrderType: How to order the string indices.\n    Options are 'frequencyAsc', 'frequencyDesc', 'alphabeticalAsc',\n    'alphabeticalDesc'.\n    :param maskToken: Token to use for masking.\n    If set, the token will be indexed as 0.\n    :param numOOVIndices: Number of out of vocabulary indices to use.\n    Default is 1.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(\n        stringOrderType=\"frequencyDesc\", numOOVIndices=1, maskToken=None\n    )\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/shared_string_index/#src.kamae.spark.transformers.shared_string_index.SharedStringIndexTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/shared_string_index/#src.kamae.spark.transformers.shared_string_index.SharedStringIndexTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset using the string index labels assigning a unique integer to each string label.</p> <p>:param dataset: Pyspark dataframe to transform.</p> <p>:returns: Pyspark dataframe with the input columns indexed,  named as the output columns.</p> Source code in <code>src/kamae/spark/transformers/shared_string_index.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset using the string index labels assigning a unique\n    integer to each string label.\n\n    :param dataset: Pyspark dataframe to transform.\n\n    :returns: Pyspark dataframe with the input columns indexed,\n     named as the output columns.\n    \"\"\"\n    labels = self.getLabelsArray()\n    num_oov_indices = self.getNumOOVIndices()\n    mask_token = self.getMaskToken()\n\n    # Assumption made that all the input columns have the same datatype/nesting.\n    input_datatypes = [\n        self.get_column_datatype(dataset=dataset, column_name=c)\n        for c in self.getInputCols()\n    ]\n\n    output_cols = []\n    for i, column in enumerate(self.getInputCols()):\n        output_col = single_input_single_output_scalar_udf_transform(\n            input_col=F.col(column),\n            input_col_datatype=input_datatypes[i],\n            func=lambda x: indexer_udf(\n                label=x,\n                labels=labels,\n                num_oov_indices=num_oov_indices,\n                mask_token=mask_token,\n            ),\n            udf_return_element_datatype=IntegerType(),\n        )\n        output_cols.append(output_col.alias(self.getOutputCols()[i]))\n\n    select_cols = [F.col(c) for c in dataset.columns] + output_cols\n\n    return dataset.select(*select_cols)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/shared_string_index/#src.kamae.spark.transformers.shared_string_index.SharedStringIndexTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the list of tensorflow layers for the shared string indexer transformer. We need to use a list as each layer could operate on differing input shapes.</p> <p>:returns: List of Tensorflow keras layer with name equal to the layerName parameter and the input column name, that performs the indexing.</p> Source code in <code>src/kamae/spark/transformers/shared_string_index.py</code> <pre><code>def get_tf_layer(self) -&gt; List[tf.keras.layers.Layer]:\n    \"\"\"\n    Gets the list of tensorflow layers for the shared string indexer transformer.\n    We need to use a list as each layer could operate on differing input shapes.\n\n    :returns: List of Tensorflow keras layer with name equal to the layerName\n    parameter and the input column name, that performs the indexing.\n    \"\"\"\n    return [\n        StringIndexLayer(\n            name=f\"{self.getLayerName()}_{input_name}\",\n            input_dtype=self.getInputTFDtype(),\n            output_dtype=self.getOutputTFDtype(),\n            vocabulary=self.getLabelsArray(),\n            mask_token=self.getMaskToken(),\n            num_oov_indices=self.getNumOOVIndices(),\n        )\n        for input_name in self.getInputCols()\n    ]\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/standard_scale/","title":"standard_scale","text":""},{"location":"reference/src/kamae/spark/transformers/standard_scale/#src.kamae.spark.transformers.standard_scale.StandardScaleTransformer","title":"StandardScaleTransformer","text":"<pre><code>StandardScaleTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    mean=None,\n    stddev=None,\n    maskValue=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>StandardScaleParams</code>, <code>SingleInputSingleOutputParams</code></p> <p>Standard scaler transformer for use in Spark pipelines. This is used to standardize/transform the input column using the mean and standard deviation.</p> <p>WARNING: If the input is an array, we assume that the array has a constant shape across all rows.</p> <p>:param inputCol: Input column name to standardize. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. :param mean: List of mean values corresponding to the input column. :param stddev: List of standard deviation values corresponding to the input column. :param maskValue: Value which should be ignored in the standard scaling process. That is disregarded when calculating the mean and standard deviation and when the scaling is applied. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/standard_scale.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    mean: Optional[List[float]] = None,\n    stddev: Optional[List[float]] = None,\n    maskValue: Optional[float] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes a StandardScaleTransformer transformer.\n\n    :param inputCol: Input column name to standardize.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model.\n    :param mean: List of mean values corresponding to the input column.\n    :param stddev: List of standard deviation values corresponding to the\n    input column.\n    :param maskValue: Value which should be ignored in the standard scaling process.\n    That is disregarded when calculating the mean and standard deviation and\n    when the scaling is applied.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(maskValue=None)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/standard_scale/#src.kamae.spark.transformers.standard_scale.StandardScaleTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/standard_scale/#src.kamae.spark.transformers.standard_scale.StandardScaleTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset using the mean and standard deviation to standardize the input column. If a mask value is set, it is used to ignore elements in the dataset with that value, and they will remain unchanged in the standardization process.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Pyspark dataframe with the input column standardized,  named as the output column.</p> Source code in <code>src/kamae/spark/transformers/standard_scale.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset using the mean and standard deviation\n    to standardize the input column. If a mask value is set, it is used\n    to ignore elements in the dataset with that value, and they will remain\n    unchanged in the standardization process.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Pyspark dataframe with the input column standardized,\n     named as the output column.\n    \"\"\"\n    original_input_datatype = self.get_column_datatype(dataset, self.getInputCol())\n    if not isinstance(original_input_datatype, ArrayType):\n        input_col = F.array(F.col(self.getInputCol()))\n        input_datatype = ArrayType(original_input_datatype)\n    else:\n        input_col = F.col(self.getInputCol())\n        input_datatype = original_input_datatype\n\n    shift = F.array([F.lit(m) for m in self.getMean()])\n    scale = F.array([F.lit(1.0 / s if s != 0 else 0.0) for s in self.getStddev()])\n\n    output_col = single_input_single_output_array_transform(\n        input_col=input_col,\n        input_col_datatype=input_datatype,\n        func=lambda x: F.transform(\n            x,\n            lambda y, i: F.when(y == self.getMaskValue(), y).otherwise(\n                (y - F.lit(shift)[i]) * F.lit(scale)[i]\n            ),\n        ),\n    )\n\n    if not isinstance(original_input_datatype, ArrayType):\n        output_col = output_col.getItem(0)\n\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/standard_scale/#src.kamae.spark.transformers.standard_scale.StandardScaleTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the standard scaler transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter  that performs the standardization.</p> Source code in <code>src/kamae/spark/transformers/standard_scale.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the standard scaler transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter\n     that performs the standardization.\n    \"\"\"\n    np_mean = np.array(self.getMean())\n    np_variance = np.array(self.getStddev()) ** 2\n    mask_value = self.getMaskValue()\n    return StandardScaleLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        mean=np_mean,\n        variance=np_variance,\n        mask_value=mask_value,\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_affix/","title":"string_affix","text":""},{"location":"reference/src/kamae/spark/transformers/string_affix/#src.kamae.spark.transformers.string_affix.StringAffixParams","title":"StringAffixParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing parameters needed for string affixing. transforms.</p>"},{"location":"reference/src/kamae/spark/transformers/string_affix/#src.kamae.spark.transformers.string_affix.StringAffixParams.getPrefix","title":"getPrefix","text":"<pre><code>getPrefix()\n</code></pre> <p>Gets the prefix parameter.</p> <p>:returns: String value to use as a prefix when joining the strings.</p> Source code in <code>src/kamae/spark/transformers/string_affix.py</code> <pre><code>def getPrefix(self) -&gt; str:\n    \"\"\"\n    Gets the prefix parameter.\n\n    :returns: String value to use as a prefix when joining the strings.\n    \"\"\"\n    return self.getOrDefault(self.prefix)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_affix/#src.kamae.spark.transformers.string_affix.StringAffixParams.getSuffix","title":"getSuffix","text":"<pre><code>getSuffix()\n</code></pre> <p>Gets the suffix parameter.</p> <p>:returns: String value to use as a suffix when joining the strings.</p> Source code in <code>src/kamae/spark/transformers/string_affix.py</code> <pre><code>def getSuffix(self) -&gt; str:\n    \"\"\"\n    Gets the suffix parameter.\n\n    :returns: String value to use as a suffix when joining the strings.\n    \"\"\"\n    return self.getOrDefault(self.suffix)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_affix/#src.kamae.spark.transformers.string_affix.StringAffixParams.setPrefix","title":"setPrefix","text":"<pre><code>setPrefix(value)\n</code></pre> <p>Sets the prefix parameter.</p> <p>:param value: String value to use as a prefix when joining the strings. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/string_affix.py</code> <pre><code>def setPrefix(self, value: str) -&gt; \"StringAffixParams\":\n    \"\"\"\n    Sets the prefix parameter.\n\n    :param value: String value to use as a prefix when joining the strings.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(prefix=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_affix/#src.kamae.spark.transformers.string_affix.StringAffixParams.setSuffix","title":"setSuffix","text":"<pre><code>setSuffix(value)\n</code></pre> <p>Sets the suffix parameter.</p> <p>:param value: String value to use as a suffix when joining the strings. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/string_affix.py</code> <pre><code>def setSuffix(self, value: str) -&gt; \"StringAffixParams\":\n    \"\"\"\n    Sets the suffix parameter.\n\n    :param value: String value to use as a suffix when joining the strings.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(suffix=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_affix/#src.kamae.spark.transformers.string_affix.StringAffixTransformer","title":"StringAffixTransformer","text":"<pre><code>StringAffixTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    prefix=None,\n    suffix=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>StringAffixParams</code></p> <p>String Affix Spark Transformer for use in Spark pipelines. This transformer takes in a column and pre- and su- fixes it. Input columns must be of type string.</p> <p>:param outputCol: column to output the affixed string to. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param prefix: String to use as a prefix when joining the strings. :param suffix: String to use as a suffix when joining the strings.</p> Source code in <code>src/kamae/spark/transformers/string_affix.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    prefix: Optional[str] = None,\n    suffix: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes the string affix transformer.\n    :param inputCol: column to combine with prefix or suffix. Must be type string.\n    :param outputCol: column to output the affixed string to.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param prefix: String to use as a prefix when joining the strings.\n    :param suffix: String to use as a suffix when joining the strings.\n    \"\"\"\n    super().__init__()\n    self._setDefault(prefix=None, suffix=None)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_affix/#src.kamae.spark.transformers.string_affix.StringAffixTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/string_affix/#src.kamae.spark.transformers.string_affix.StringAffixTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, where the value is origin column combined with prefix and or suffix.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/string_affix.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    where the value is origin column combined with prefix and or suffix.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    self._validate_params()\n\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n\n    def add_prefix_suffix(\n        column: Column, prefix: Optional[str] = None, suffix: Optional[str] = None\n    ) -&gt; Column:\n        if prefix is not None and prefix != \"\":\n            column = F.concat(F.lit(prefix), column)\n        if suffix is not None and suffix != \"\":\n            column = F.concat(column, F.lit(suffix))\n        return column\n\n    output_col = single_input_single_output_scalar_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: add_prefix_suffix(x, self.getPrefix(), self.getSuffix()),\n    )\n\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_affix/#src.kamae.spark.transformers.string_affix.StringAffixTransformer._validate_params","title":"_validate_params","text":"<pre><code>_validate_params()\n</code></pre> <p>Validates the parameters passed to the transformer.</p> Source code in <code>src/kamae/spark/transformers/string_affix.py</code> <pre><code>def _validate_params(self) -&gt; None:\n    \"\"\"\n    Validates the parameters passed to the transformer.\n    \"\"\"\n    prefix = self.getPrefix()\n    suffix = self.getSuffix()\n    if (prefix is None or prefix == \"\") and (suffix is None or suffix == \"\"):\n        raise ValueError(\n            \"Either prefix or suffix must be set. Otherwise nothing to affix.\"\n        )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_affix/#src.kamae.spark.transformers.string_affix.StringAffixTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the string affix transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs prefixing and suffixing.</p> Source code in <code>src/kamae/spark/transformers/string_affix.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the string affix transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs prefixing and suffixing.\n    \"\"\"\n    return StringAffixLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        prefix=self.getPrefix(),\n        suffix=self.getSuffix(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_array_constant/","title":"string_array_constant","text":""},{"location":"reference/src/kamae/spark/transformers/string_array_constant/#src.kamae.spark.transformers.string_array_constant.StringArrayConstantTransformer","title":"StringArrayConstantTransformer","text":"<pre><code>StringArrayConstantTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    constantStringArray=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>ConstantStringArrayParams</code></p> <p>String Array Constant Spark Transformer for use in Spark pipelines. This transformer populates a column with a constant string array.</p> <p>:param inputCol: Input column used to copy shape from. Ignored for Spark, used for Tensorflow. :param outputCol: column to fill with the constant. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param constantStringArray: List of strings to use as a constant string array.</p> Source code in <code>src/kamae/spark/transformers/string_array_constant.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    constantStringArray: Optional[List[str]] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes the String Array Constant Transformer.\n\n    :param inputCol: Input column used to copy shape from. Ignored for Spark, used\n    for Tensorflow.\n    :param outputCol: column to fill with the constant.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param constantStringArray: List of strings to use as a constant string array.\n    \"\"\"\n    super().__init__()\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_array_constant/#src.kamae.spark.transformers.string_array_constant.StringArrayConstantTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/string_array_constant/#src.kamae.spark.transformers.string_array_constant.StringArrayConstantTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, populates it with the constant string array.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/string_array_constant.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    populates it with the constant string array.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n    output_col = single_input_single_output_scalar_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: F.lit(self.getConstantStringArray()).cast(\"array&lt;string&gt;\"),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_array_constant/#src.kamae.spark.transformers.string_array_constant.StringArrayConstantTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for generating the keras model that outputs the constant string array.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter</p> Source code in <code>src/kamae/spark/transformers/string_array_constant.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for generating the keras model that outputs\n    the constant string array.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter\n    \"\"\"\n    return StringArrayConstantLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        constant_string_array=self.getConstantStringArray(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_case/","title":"string_case","text":""},{"location":"reference/src/kamae/spark/transformers/string_case/#src.kamae.spark.transformers.string_case.StringCaseParams","title":"StringCaseParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing stringCaseType parameter needed for string casing transforms.</p>"},{"location":"reference/src/kamae/spark/transformers/string_case/#src.kamae.spark.transformers.string_case.StringCaseParams.getStringCaseType","title":"getStringCaseType","text":"<pre><code>getStringCaseType()\n</code></pre> <p>Gets the stringCaseType parameter.</p> <p>:returns: String value of how to change the case of the string.</p> Source code in <code>src/kamae/spark/transformers/string_case.py</code> <pre><code>def getStringCaseType(self) -&gt; str:\n    \"\"\"\n    Gets the stringCaseType parameter.\n\n    :returns: String value of how to change the case of the string.\n    \"\"\"\n    return self.getOrDefault(self.stringCaseType)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_case/#src.kamae.spark.transformers.string_case.StringCaseParams.setStringCaseType","title":"setStringCaseType","text":"<pre><code>setStringCaseType(value)\n</code></pre> <p>Sets the stringCaseType parameter to the given value. Must be one of: - 'upper' - 'lower'</p> <p>:param value: String to set the stringCaseType parameter to. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/string_case.py</code> <pre><code>def setStringCaseType(self, value: str) -&gt; \"StringCaseParams\":\n    \"\"\"\n    Sets the stringCaseType parameter to the given value.\n    Must be one of:\n    - 'upper'\n    - 'lower'\n\n    :param value: String to set the stringCaseType parameter to.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    possible_order_options = [\n        \"upper\",\n        \"lower\",\n    ]\n    if value not in possible_order_options:\n        raise ValueError(\n            f\"stringCaseType must be one of {', '.join(possible_order_options)}\"\n        )\n    return self._set(stringCaseType=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_case/#src.kamae.spark.transformers.string_case.StringCaseTransformer","title":"StringCaseTransformer","text":"<pre><code>StringCaseTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    stringCaseType=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>StringCaseParams</code></p> <p>StringCaseLayer Spark Transformer for use in Spark pipelines. This transformer applies an upper, lower or capitalise operation on the input column.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param stringCaseType: How to change the case of the string. Must be one of: - 'upper' - 'lower' Default is 'lower'. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/string_case.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    stringCaseType: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes an StringCaseTransformer transformer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param stringCaseType: How to change the case of the string. Must be one of:\n    - 'upper'\n    - 'lower'\n    Default is 'lower'.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(stringCaseType=\"lower\")\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_case/#src.kamae.spark.transformers.string_case.StringCaseTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/string_case/#src.kamae.spark.transformers.string_case.StringCaseTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which applies the given stringCaseType to the input column.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/string_case.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which applies the given stringCaseType to the input column.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    string_case_type = self.getStringCaseType()\n\n    def string_case(x: Column, case_type: str) -&gt; Column:\n        if case_type == \"upper\":\n            return F.upper(x)\n        elif case_type == \"lower\":\n            return F.lower(x)\n        else:\n            raise ValueError(\n                f\"\"\"stringCaseType must be one of 'upper' or 'lower'.\n                Got {case_type}\"\"\"\n            )\n\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n    output_col = single_input_single_output_scalar_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: string_case(x, string_case_type),\n    )\n\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_case/#src.kamae.spark.transformers.string_case.StringCaseTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the StringCaseLayer transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs the string casing operation.</p> Source code in <code>src/kamae/spark/transformers/string_case.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the StringCaseLayer transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs the string casing operation.\n    \"\"\"\n    return StringCaseLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        string_case_type=self.getStringCaseType(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_concatenate/","title":"string_concatenate","text":""},{"location":"reference/src/kamae/spark/transformers/string_concatenate/#src.kamae.spark.transformers.string_concatenate.StringConcatenateParams","title":"StringConcatenateParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing separator parameter needed for string concatenation transforms.</p>"},{"location":"reference/src/kamae/spark/transformers/string_concatenate/#src.kamae.spark.transformers.string_concatenate.StringConcatenateParams.getSeparator","title":"getSeparator","text":"<pre><code>getSeparator()\n</code></pre> <p>Gets the separator parameter.</p> <p>:returns: String value to use as a separator when joining the strings.</p> Source code in <code>src/kamae/spark/transformers/string_concatenate.py</code> <pre><code>def getSeparator(self) -&gt; str:\n    \"\"\"\n    Gets the separator parameter.\n\n    :returns: String value to use as a separator when joining the strings.\n    \"\"\"\n    return self.getOrDefault(self.separator)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_concatenate/#src.kamae.spark.transformers.string_concatenate.StringConcatenateParams.setSeparator","title":"setSeparator","text":"<pre><code>setSeparator(value)\n</code></pre> <p>Sets the separator parameter.</p> <p>:param value: String value to use as a separator when joining the strings. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/string_concatenate.py</code> <pre><code>def setSeparator(self, value: str) -&gt; \"StringConcatenateParams\":\n    \"\"\"\n    Sets the separator parameter.\n\n    :param value: String value to use as a separator when joining the strings.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(separator=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_concatenate/#src.kamae.spark.transformers.string_concatenate.StringConcatenateTransformer","title":"StringConcatenateTransformer","text":"<pre><code>StringConcatenateTransformer(\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    separator=\"_\",\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>MultiInputSingleOutputParams</code>, <code>StringConcatenateParams</code></p> <p>String Concatenate Spark Transformer for use in Spark pipelines. This transformer takes in multiple columns and concatenates them together into a single column using a separator. Input columns must be of type string.</p> <p>:param outputCol: column to output the concatenated string to. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param separator: String to use as a separator when joining the strings. If not provided, underscore <code>_</code> is used.</p> Source code in <code>src/kamae/spark/transformers/string_concatenate.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    separator: str = \"_\",\n) -&gt; None:\n    \"\"\"\n    Initializes the string concatenate transformer.\n    :param inputCols: columns to concatenate together. Must be of type string.\n    :param outputCol: column to output the concatenated string to.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param separator: String to use as a separator when joining the strings.\n    If not provided, underscore `_` is used.\n    \"\"\"\n    super().__init__()\n    kwargs = self._input_kwargs\n    self._setDefault(separator=\"_\")\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_concatenate/#src.kamae.spark.transformers.string_concatenate.StringConcatenateTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/string_concatenate/#src.kamae.spark.transformers.string_concatenate.StringConcatenateTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, where the value is the result of concatenating the values of the input columns.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/string_concatenate.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    where the value is the result of concatenating the values of the input columns.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    separator = self.getSeparator()\n\n    input_col_names = self.getInputCols()\n    input_cols = [F.col(c) for c in input_col_names]\n\n    input_datatypes = [\n        self.get_column_datatype(dataset=dataset, column_name=input_col_name)\n        for input_col_name in input_col_names\n    ]\n\n    output_col = multi_input_single_output_scalar_transform(\n        input_cols=input_cols,\n        input_col_datatypes=input_datatypes,\n        input_col_names=input_col_names,\n        func=lambda x: F.concat_ws(\n            separator, *[x[input_col_name] for input_col_name in input_col_names]\n        ),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_concatenate/#src.kamae.spark.transformers.string_concatenate.StringConcatenateTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the concatenate transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs a concatenation.</p> Source code in <code>src/kamae/spark/transformers/string_concatenate.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the concatenate transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs a concatenation.\n    \"\"\"\n    return StringConcatenateLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        separator=self.getSeparator(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_contains/","title":"string_contains","text":""},{"location":"reference/src/kamae/spark/transformers/string_contains/#src.kamae.spark.transformers.string_contains.StringContainsTransformer","title":"StringContainsTransformer","text":"<pre><code>StringContainsTransformer(\n    inputCol=None,\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    stringConstant=None,\n    negation=False,\n    layerName=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>MultiInputSingleOutputParams</code>, <code>NegationParams</code>, <code>StringConstantParams</code></p> <p>String contains Spark Transformer for use in Spark pipelines. This transformer performs a string contains operation on the input column. If the string constant is specified, we use it for the string contains on the single input. Otherwise, if multiple input columns are specified, we check if the first input column contains the second. Used for cases where you want to keep the input the same.</p> <p>:param inputCol: Input column name. :param inputCols: List of input column names. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param stringConstant: String constant to use in string contains operation. Only used in single input scenario. :param negation: Whether to negate the string contains operation. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/string_contains.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    stringConstant: Optional[str] = None,\n    negation: bool = False,\n    layerName: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes an StringContainsTransformer transformer.\n\n    :param inputCol: Input column name.\n    :param inputCols: List of input column names.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param stringConstant: String constant to use in string contains\n    operation.\n    Only used in single input scenario.\n    :param negation: Whether to negate the string contains operation.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(negation=False, stringConstant=None)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_contains/#src.kamae.spark.transformers.string_contains.StringContainsTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/string_contains/#src.kamae.spark.transformers.string_contains.StringContainsTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which contains the result of the string contains operation.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/string_contains.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which contains the result of the string contains operation.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_cols = self.get_multiple_input_cols(\n        constant_param_name=\"stringConstant\", input_cols_limit=2\n    )\n\n    # input_cols can contain either actual columns or lit(constants). In order to\n    # determine the datatype of the input columns, we select them from the dataset\n    # first.\n    input_col_names = dataset.select(input_cols).columns\n    input_col_datatypes = [\n        self.get_column_datatype(dataset=dataset.select(input_cols), column_name=c)\n        for c in input_col_names\n    ]\n\n    def string_contains(\n        x: Column, input_col_names: List[str], negation: bool\n    ) -&gt; Column:\n        col_expr = F.when(\n            x[input_col_names[1]] == F.lit(\"\"), x[input_col_names[0]] == F.lit(\"\")\n        ).otherwise(x[input_col_names[0]].contains(x[input_col_names[1]]))\n        return col_expr if not negation else ~col_expr\n\n    output_col = multi_input_single_output_scalar_transform(\n        input_cols=input_cols,\n        input_col_names=input_col_names,\n        input_col_datatypes=input_col_datatypes,\n        func=lambda x: string_contains(x, input_col_names, self.getNegation()),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_contains/#src.kamae.spark.transformers.string_contains.StringContainsTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the StringContainsLayer transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs a string contains operation.</p> Source code in <code>src/kamae/spark/transformers/string_contains.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the StringContainsLayer transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs a string contains operation.\n    \"\"\"\n    return StringContainsLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        negation=self.getNegation(),\n        string_constant=self.getStringConstant(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_contains/#src.kamae.spark.transformers.string_contains.StringContainsTransformer.setInputCols","title":"setInputCols","text":"<pre><code>setInputCols(value)\n</code></pre> <p>Overrides setting the input columns for the transformer. Throws an error if we do not have exactly two input columns.</p> <p>:param value: List of input columns. :returns: Class instance.</p> Source code in <code>src/kamae/spark/transformers/string_contains.py</code> <pre><code>def setInputCols(self, value: List[str]) -&gt; \"StringContainsTransformer\":\n    \"\"\"\n    Overrides setting the input columns for the transformer.\n    Throws an error if we do not have exactly two input columns.\n\n    :param value: List of input columns.\n    :returns: Class instance.\n    \"\"\"\n    if len(value) != 2:\n        raise ValueError(\n            \"\"\"When setting inputCols for StringContainsTransformer,\n            there must be exactly two input columns.\"\"\"\n        )\n    return self._set(inputCols=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_contains_list/","title":"string_contains_list","text":""},{"location":"reference/src/kamae/spark/transformers/string_contains_list/#src.kamae.spark.transformers.string_contains_list.StringContainsListTransformer","title":"StringContainsListTransformer","text":"<pre><code>StringContainsListTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    constantStringArray=None,\n    negation=False,\n    layerName=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>NegationParams</code>, <code>ConstantStringArrayParams</code></p> <p>String contains list Spark Transformer for use in Spark pipelines. This transformer performs a string contains operation on the input column over all constants in the passed constantStringArray.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param constantStringArray: String constant array to use in string contains list operation. :param negation: Whether to negate the string contains list operation. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/string_contains_list.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    constantStringArray: Optional[List[str]] = None,\n    negation: bool = False,\n    layerName: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Intializes a StringContainsListTransformer transformer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param constantStringArray: String constant array to use in string contains list\n    operation.\n    :param negation: Whether to negate the string contains list operation.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(negation=False)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_contains_list/#src.kamae.spark.transformers.string_contains_list.StringContainsListTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/string_contains_list/#src.kamae.spark.transformers.string_contains_list.StringContainsListTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which contains the result of the string contains operation.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/string_contains_list.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which contains the result of the string contains operation.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n\n    if not self.isDefined(\"constantStringArray\"):\n        raise ValueError(\"constantStringArray must be defined.\")\n\n    def string_contains_list(\n        x: Column, string_list: List[str], negation: bool\n    ) -&gt; Column:\n        contains_cols = [\n            x.contains(string_constant) for string_constant in string_list\n        ]\n        col_expr = reduce(lambda y, z: y | z, contains_cols)\n        return col_expr if not negation else ~col_expr\n\n    output_col = single_input_single_output_scalar_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: string_contains_list(\n            x=x,\n            string_list=self.getConstantStringArray(),\n            negation=self.getNegation(),\n        ),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_contains_list/#src.kamae.spark.transformers.string_contains_list.StringContainsListTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the StringContainsLayer transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs a string contains operation.</p> Source code in <code>src/kamae/spark/transformers/string_contains_list.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the StringContainsLayer transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs a string contains operation.\n    \"\"\"\n\n    if not self.isDefined(\"constantStringArray\"):\n        raise ValueError(\"constantStringArray must be defined.\")\n\n    return StringContainsListLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        negation=self.getNegation(),\n        string_constant_list=self.getConstantStringArray(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_equals_if_statement/","title":"string_equals_if_statement","text":""},{"location":"reference/src/kamae/spark/transformers/string_equals_if_statement/#src.kamae.spark.transformers.string_equals_if_statement.StringEqualsIfStatementParams","title":"StringEqualsIfStatementParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing parameters needed for StringEqualsIfStatementTransformer transform layers.</p>"},{"location":"reference/src/kamae/spark/transformers/string_equals_if_statement/#src.kamae.spark.transformers.string_equals_if_statement.StringEqualsIfStatementParams.getResultIfFalse","title":"getResultIfFalse","text":"<pre><code>getResultIfFalse()\n</code></pre> <p>Gets the resultIfFalse parameter.</p> <p>:returns: Float value to return if condition is false.</p> Source code in <code>src/kamae/spark/transformers/string_equals_if_statement.py</code> <pre><code>def getResultIfFalse(self) -&gt; str:\n    \"\"\"\n    Gets the resultIfFalse parameter.\n\n    :returns: Float value to return if condition is false.\n    \"\"\"\n    return self.getOrDefault(self.resultIfFalse)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_equals_if_statement/#src.kamae.spark.transformers.string_equals_if_statement.StringEqualsIfStatementParams.getResultIfTrue","title":"getResultIfTrue","text":"<pre><code>getResultIfTrue()\n</code></pre> <p>Gets the resultIfTrue parameter.</p> <p>:returns: Float value to return if condition is true.</p> Source code in <code>src/kamae/spark/transformers/string_equals_if_statement.py</code> <pre><code>def getResultIfTrue(self) -&gt; str:\n    \"\"\"\n    Gets the resultIfTrue parameter.\n\n    :returns: Float value to return if condition is true.\n    \"\"\"\n    return self.getOrDefault(self.resultIfTrue)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_equals_if_statement/#src.kamae.spark.transformers.string_equals_if_statement.StringEqualsIfStatementParams.getValueToCompare","title":"getValueToCompare","text":"<pre><code>getValueToCompare()\n</code></pre> <p>Gets the valueToCompare parameter.</p> <p>:returns: Float value to compare to input column.</p> Source code in <code>src/kamae/spark/transformers/string_equals_if_statement.py</code> <pre><code>def getValueToCompare(self) -&gt; str:\n    \"\"\"\n    Gets the valueToCompare parameter.\n\n    :returns: Float value to compare to input column.\n    \"\"\"\n    return self.getOrDefault(self.valueToCompare)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_equals_if_statement/#src.kamae.spark.transformers.string_equals_if_statement.StringEqualsIfStatementParams.setResultIfFalse","title":"setResultIfFalse","text":"<pre><code>setResultIfFalse(value)\n</code></pre> <p>Sets the resultIfFalse parameter.</p> <p>:param value: Float value to return if condition is false. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/string_equals_if_statement.py</code> <pre><code>def setResultIfFalse(self, value: str) -&gt; \"StringEqualsIfStatementParams\":\n    \"\"\"\n    Sets the resultIfFalse parameter.\n\n    :param value: Float value to return if condition is false.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(resultIfFalse=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_equals_if_statement/#src.kamae.spark.transformers.string_equals_if_statement.StringEqualsIfStatementParams.setResultIfTrue","title":"setResultIfTrue","text":"<pre><code>setResultIfTrue(value)\n</code></pre> <p>Sets the resultIfTrue parameter.</p> <p>:param value: Float value to return if condition is true. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/string_equals_if_statement.py</code> <pre><code>def setResultIfTrue(self, value: str) -&gt; \"StringEqualsIfStatementParams\":\n    \"\"\"\n    Sets the resultIfTrue parameter.\n\n    :param value: Float value to return if condition is true.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(resultIfTrue=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_equals_if_statement/#src.kamae.spark.transformers.string_equals_if_statement.StringEqualsIfStatementParams.setValueToCompare","title":"setValueToCompare","text":"<pre><code>setValueToCompare(value)\n</code></pre> <p>Sets the valueToCompare parameter.</p> <p>:param value: Float value to compare to input column. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/string_equals_if_statement.py</code> <pre><code>def setValueToCompare(self, value: str) -&gt; \"StringEqualsIfStatementParams\":\n    \"\"\"\n    Sets the valueToCompare parameter.\n\n    :param value: Float value to compare to input column.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(valueToCompare=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_equals_if_statement/#src.kamae.spark.transformers.string_equals_if_statement.StringEqualsIfStatementTransformer","title":"StringEqualsIfStatementTransformer","text":"<pre><code>StringEqualsIfStatementTransformer(\n    inputCol=None,\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    valueToCompare=None,\n    resultIfTrue=None,\n    resultIfFalse=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>MultiInputSingleOutputParams</code>, <code>StringEqualsIfStatementParams</code></p> <p>StringEqualIfStatement Spark Transformer for use in Spark pipelines. This transformer computes an if equal statement between a set of constants and columns.</p> <p>:param inputCol: Input column name. Only used if inputCols is not specified. If specified, then all other aspects of the if statement are constant. :param inputCols: Input column names. List of input columns to in the case where the if statement is not constant. Must be specified in the order [valueToCompare, resultIfTrue, resultIfFalse]. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param valueToCompare: Optional str value to compare to input column. If not specified, then assumed to be the first input column. :param resultIfTrue: Optional str value to return if condition is true. If not specified, then assumed to be the second input column. :param resultIfFalse: Optional str value to return if condition is false. If not specified, then assumed to be the third input column. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/string_equals_if_statement.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    valueToCompare: Optional[str] = None,\n    resultIfTrue: Optional[str] = None,\n    resultIfFalse: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes a StringEqualsIfStatementTransformer transformer.\n\n    :param inputCol: Input column name. Only used if inputCols is not specified.\n    If specified, then all other aspects of the if statement are constant.\n    :param inputCols: Input column names. List of input columns to in the case\n    where the if statement is not constant. Must be specified in the order\n    [valueToCompare, resultIfTrue, resultIfFalse].\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param valueToCompare: Optional str value to compare to input column.\n    If not specified, then assumed to be the first input column.\n    :param resultIfTrue: Optional str value to return if condition is true.\n    If not specified, then assumed to be the second input column.\n    :param resultIfFalse: Optional str value to return if condition is false.\n    If not specified, then assumed to be the third input column.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(\n        valueToCompare=None,\n        resultIfTrue=None,\n        resultIfFalse=None,\n    )\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_equals_if_statement/#src.kamae.spark.transformers.string_equals_if_statement.StringEqualsIfStatementTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/string_equals_if_statement/#src.kamae.spark.transformers.string_equals_if_statement.StringEqualsIfStatementTransformer._construct_input_cols","title":"_construct_input_cols","text":"<pre><code>_construct_input_cols()\n</code></pre> <p>Constructs the input columns for the transformer. This is fairly complex since the user can set any of the following: - valueToCompare - resultIfTrue - resultIfFalse</p> <p>If all of these are set, then the user should have provided a single <code>inputCol</code>. Otherwise, if any of these are not set, then the user should have provided <code>inputCols</code> containing the missing values. The <code>inputCols</code> should be in the order [valueToCompare, resultIfTrue, resultIfFalse]. But if the user has specified, for example, <code>resultIfTrue</code> as a constant then the <code>inputCols</code> should be in the order [valueToCompare, resultIfFalse].</p> <p>:returns: Tuple of 4 pyspark columns.</p> Source code in <code>src/kamae/spark/transformers/string_equals_if_statement.py</code> <pre><code>def _construct_input_cols(self) -&gt; List[Column]:\n    \"\"\"\n    Constructs the input columns for the transformer. This is fairly complex\n    since the user can set any of the following:\n    - valueToCompare\n    - resultIfTrue\n    - resultIfFalse\n\n    If all of these are set, then the user should have provided a single `inputCol`.\n    Otherwise, if any of these are not set, then the user should have provided\n    `inputCols` containing the missing values. The `inputCols` should be in the\n    order [valueToCompare, resultIfTrue, resultIfFalse]. But if the user has\n    specified, for example, `resultIfTrue` as a constant then the\n    `inputCols` should be in the order [valueToCompare, resultIfFalse].\n\n    :returns: Tuple of 4 pyspark columns.\n    \"\"\"\n    optional_constant_cols = [\"valueToCompare\", \"resultIfTrue\", \"resultIfFalse\"]\n    optional_constants_defined = {\n        const: self.getOrDefault(const) is not None\n        for const in optional_constant_cols\n    }\n\n    if self.isDefined(\"inputCols\"):\n        # If the user has set inputCols, then some or all of the optional constants\n        # are defined as input column variables\n        input_cols = self.getInputCols()\n        if len(input_cols) + sum(optional_constants_defined.values()) != 4:\n            raise ValueError(\n                f\"\"\"Total number of input columns and constants must be 4,\n                but got {len(input_cols)} input columns and\n                {sum(optional_constants_defined.values())} constants.\"\"\"\n            )\n        # The first input column is always the value to compare\n        input_col_list = [\n            F.col(input_cols[0]),\n        ]\n        input_col_counter = 1\n        for const_col in optional_constant_cols:\n            # Loop through the optional constant names.\n            # If the constant is not defined then it must be an input column.\n            # Otherwise, it is a literal value.\n            if not optional_constants_defined[const_col]:\n                input_col_list.append(F.col(input_cols[input_col_counter]))\n                input_col_counter += 1\n            else:\n                input_col_list.append(F.lit(self.getOrDefault(const_col)))\n        return [\n            input_col_list[0].alias(self.uid + \"_inputCol\"),\n            input_col_list[1].alias(self.uid + \"_valueToCompare\"),\n            input_col_list[2].alias(self.uid + \"_resultIfTrue\"),\n            input_col_list[3].alias(self.uid + \"_resultIfFalse\"),\n        ]\n    elif self.isDefined(\"inputCol\"):\n        # If the user has set inputCol, then all the optional constants\n        # must be defined.\n        if not all(\n            [\n                self.getOrDefault(const) is not None\n                for const in optional_constant_cols\n            ]\n        ):\n            raise ValueError(\n                f\"\"\"Must specify all of {optional_constant_cols}\"\n                if inputCol is specified.\"\"\"\n            )\n        return [\n            F.col(self.getInputCol()).alias(self.uid + \"_inputCol\"),\n            F.lit(self.getValueToCompare()).alias(self.uid + \"_valueToCompare\"),\n            F.lit(self.getResultIfTrue()).alias(self.uid + \"_resultIfTrue\"),\n            F.lit(self.getResultIfFalse()).alias(self.uid + \"_resultIfFalse\"),\n        ]\n    else:\n        raise ValueError(\"Must specify either inputCol or inputCols.\")\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_equals_if_statement/#src.kamae.spark.transformers.string_equals_if_statement.StringEqualsIfStatementTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which is the output of an if statement with condition:</p> <p>IF <code>inputCol</code> == <code>valueToCompare</code> THEN <code>resultIfTrue</code> ELSE <code>resultIfFalse</code></p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/string_equals_if_statement.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which is the output of an if statement with condition:\n\n    IF `inputCol` == `valueToCompare`\n    THEN `resultIfTrue`\n    ELSE `resultIfFalse`\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_cols = self._construct_input_cols()\n    # input_cols can contain either actual columns or lit(constants). In order to\n    # determine the datatype of the input columns, we select them from the dataset\n    # first.\n    input_col_names = dataset.select(input_cols).columns\n    input_col_datatypes = [\n        self.get_column_datatype(dataset=dataset.select(input_cols), column_name=c)\n        for c in input_col_names\n    ]\n\n    output_col = multi_input_single_output_scalar_transform(\n        input_cols=input_cols,\n        input_col_names=input_col_names,\n        input_col_datatypes=input_col_datatypes,\n        func=lambda x: F.when(\n            x[input_col_names[0]] == x[input_col_names[1]], x[input_col_names[2]]\n        ).otherwise(x[input_col_names[3]]),\n    )\n\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_equals_if_statement/#src.kamae.spark.transformers.string_equals_if_statement.StringEqualsIfStatementTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the string if equal statement transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs the string if equals statement.</p> Source code in <code>src/kamae/spark/transformers/string_equals_if_statement.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the string if equal statement transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs the string if equals statement.\n    \"\"\"\n    return StringEqualsIfStatementLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        value_to_compare=self.getValueToCompare(),\n        result_if_true=self.getResultIfTrue(),\n        result_if_false=self.getResultIfFalse(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_equals_if_statement/#src.kamae.spark.transformers.string_equals_if_statement.StringEqualsIfStatementTransformer.setInputCols","title":"setInputCols","text":"<pre><code>setInputCols(value)\n</code></pre> <p>Sets the inputCols parameter, throwing an error if the total length of the inputCols and the constants are more than 4.</p> <p>:param value: List of input column names. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/string_equals_if_statement.py</code> <pre><code>def setInputCols(self, value: List[str]) -&gt; \"StringEqualsIfStatementTransformer\":\n    \"\"\"\n    Sets the inputCols parameter, throwing an error if the total length of the\n    inputCols and the constants are more than 4.\n\n    :param value: List of input column names.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    optional_constant_cols = [\"valueToCompare\", \"resultIfTrue\", \"resultIfFalse\"]\n    num_defined_constants = len(\n        [c for c in optional_constant_cols if self.getOrDefault(c) is not None]\n    )\n    if len(value) + num_defined_constants &gt; 4:\n        raise ValueError(\n            f\"\"\"Total number of input columns and constants cannot be more than 4,\n            but got {len(value)} input columns and\n            {num_defined_constants} constants.\"\"\"\n        )\n\n    return self._set(inputCols=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_index/","title":"string_index","text":""},{"location":"reference/src/kamae/spark/transformers/string_index/#src.kamae.spark.transformers.string_index.StringIndexTransformer","title":"StringIndexTransformer","text":"<pre><code>StringIndexTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    labelsArray=None,\n    stringOrderType=None,\n    maskToken=None,\n    numOOVIndices=1,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>StringIndexParams</code>, <code>SingleInputSingleOutputParams</code></p> <p>StringIndexTransformer Spark Transformer for use in Spark pipelines. This transformer is used to index/transform feature columns using the string labels collected by the StringIndexEstimator.</p> <p>NOTE: If your data contains null characters: https://en.wikipedia.org/wiki/Null_character This transformer could fail since the hashing algorithm uses cannot accept null characters. If you have null characters in your data, you should remove them.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param: stringOrderType: How to order the string indices. Options are 'frequencyAsc', 'frequencyDesc', 'alphabeticalAsc', 'alphabeticalDesc'. :param maskToken: Token to use for masking. If set, the token will be indexed as 0. :param numOOVIndices: Number of out of vocabulary indices to use. Default is 1. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/string_index.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    labelsArray: Optional[List[str]] = None,\n    stringOrderType: Optional[str] = None,\n    maskToken: Optional[str] = None,\n    numOOVIndices: int = 1,\n) -&gt; None:\n    \"\"\"\n    Initializes the StringIndexTransformer transformer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param: stringOrderType: How to order the string indices.\n    Options are 'frequencyAsc', 'frequencyDesc', 'alphabeticalAsc',\n    'alphabeticalDesc'.\n    :param maskToken: Token to use for masking.\n    If set, the token will be indexed as 0.\n    :param numOOVIndices: Number of out of vocabulary indices to use.\n    Default is 1.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(\n        stringOrderType=\"frequencyDesc\", numOOVIndices=1, maskToken=None\n    )\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_index/#src.kamae.spark.transformers.string_index.StringIndexTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/string_index/#src.kamae.spark.transformers.string_index.StringIndexTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset using the string index labels assigning a unique integer to each string label.</p> <p>:param dataset: Pyspark dataframe to transform.</p> <p>:returns: Pyspark dataframe with the input column indexed,  named as the output column.</p> Source code in <code>src/kamae/spark/transformers/string_index.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset using the string index labels assigning a unique\n    integer to each string label.\n\n    :param dataset: Pyspark dataframe to transform.\n\n    :returns: Pyspark dataframe with the input column indexed,\n     named as the output column.\n    \"\"\"\n    labels = self.getLabelsArray()\n    num_oov_indices = self.getNumOOVIndices()\n    mask_token = self.getMaskToken()\n\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n    output_col = single_input_single_output_scalar_udf_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: indexer_udf(\n            label=x,\n            labels=labels,\n            num_oov_indices=num_oov_indices,\n            mask_token=mask_token,\n        ),\n        udf_return_element_datatype=IntegerType(),\n    )\n\n    return dataset.withColumn(\n        self.getOutputCol(),\n        output_col,\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_index/#src.kamae.spark.transformers.string_index.StringIndexTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the string indexer transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that performs the indexing.</p> Source code in <code>src/kamae/spark/transformers/string_index.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the string indexer transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter\n    that performs the indexing.\n    \"\"\"\n    return StringIndexLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        vocabulary=self.getLabelsArray(),\n        mask_token=self.getMaskToken(),\n        num_oov_indices=self.getNumOOVIndices(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_isin_list/","title":"string_isin_list","text":""},{"location":"reference/src/kamae/spark/transformers/string_isin_list/#src.kamae.spark.transformers.string_isin_list.StringIsInListTransformer","title":"StringIsInListTransformer","text":"<pre><code>StringIsInListTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    constantStringArray=None,\n    negation=False,\n    layerName=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>NegationParams</code>, <code>ConstantStringArrayParams</code></p> <p>String is in list Spark Transformer for use in Spark pipelines. This transformer performs a string equality operation on the input column over all constants in the passed constantStringArray.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param constantStringArray: String constant array to use in string isin list operation. :param negation: Whether to negate the string isin list operation. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/string_isin_list.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    constantStringArray: Optional[List[str]] = None,\n    negation: bool = False,\n    layerName: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Intializes a StringIsInListTransformer transformer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param constantStringArray: String constant array to use in string isin list\n    operation.\n    :param negation: Whether to negate the string isin list operation.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(negation=False)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_isin_list/#src.kamae.spark.transformers.string_isin_list.StringIsInListTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/string_isin_list/#src.kamae.spark.transformers.string_isin_list.StringIsInListTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which contains the result of the string isin operation.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/string_isin_list.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which contains the result of the string isin operation.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n\n    if not self.isDefined(\"constantStringArray\"):\n        raise ValueError(\"constantStringArray must be defined.\")\n\n    def string_isin_list(\n        x: Column, string_list: List[str], negation: bool\n    ) -&gt; Column:\n        col_expr = x.isin(string_list)\n        return col_expr if not negation else ~col_expr\n\n    output_col = single_input_single_output_scalar_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: string_isin_list(\n            x=x,\n            string_list=self.getConstantStringArray(),\n            negation=self.getNegation(),\n        ),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_isin_list/#src.kamae.spark.transformers.string_isin_list.StringIsInListTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the StringIsInListLayer transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that performs a string isin operation.</p> Source code in <code>src/kamae/spark/transformers/string_isin_list.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the StringIsInListLayer transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n    performs a string isin operation.\n    \"\"\"\n\n    if not self.isDefined(\"constantStringArray\"):\n        raise ValueError(\"constantStringArray must be defined.\")\n\n    return StringIsInListLayer(\n        name=self.getLayerName(),\n        negation=self.getNegation(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        string_constant_list=self.getConstantStringArray(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_list_to_string/","title":"string_list_to_string","text":""},{"location":"reference/src/kamae/spark/transformers/string_list_to_string/#src.kamae.spark.transformers.string_list_to_string.StringListToStringParams","title":"StringListToStringParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing separator parameter needed for string list to string transforms.</p>"},{"location":"reference/src/kamae/spark/transformers/string_list_to_string/#src.kamae.spark.transformers.string_list_to_string.StringListToStringParams.getSeparator","title":"getSeparator","text":"<pre><code>getSeparator()\n</code></pre> <p>Gets the separator to use when joining the string list.</p> <p>:returns: Separator to use when joining the string list.</p> Source code in <code>src/kamae/spark/transformers/string_list_to_string.py</code> <pre><code>def getSeparator(self) -&gt; str:\n    \"\"\"\n    Gets the separator to use when joining the string list.\n\n    :returns: Separator to use when joining the string list.\n    \"\"\"\n    return self.getOrDefault(self.separator)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_list_to_string/#src.kamae.spark.transformers.string_list_to_string.StringListToStringParams.setSeparator","title":"setSeparator","text":"<pre><code>setSeparator(value)\n</code></pre> <p>Sets the separator to use when joining the string list.</p> <p>:param value: Separator to use when joining the string list. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/string_list_to_string.py</code> <pre><code>def setSeparator(self, value: str) -&gt; \"StringListToStringParams\":\n    \"\"\"\n    Sets the separator to use when joining the string list.\n\n    :param value: Separator to use when joining the string list.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(separator=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_list_to_string/#src.kamae.spark.transformers.string_list_to_string.StringListToStringTransformer","title":"StringListToStringTransformer","text":"<pre><code>StringListToStringTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    separator=\"\",\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>StringListToStringParams</code></p> <p>StringListToStringLayer Spark Transformer for use in Spark pipelines. This transformer takes a column of string lists and joins them into a single string.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param separator: Separator to use when joining the string list. Default is the empty string. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/string_list_to_string.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    separator: str = \"\",\n) -&gt; None:\n    \"\"\"\n    Initializes an StringListToStringTransformer transformer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param separator: Separator to use when joining the string list.\n    Default is the empty string.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(separator=\"\")\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_list_to_string/#src.kamae.spark.transformers.string_list_to_string.StringListToStringTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/string_list_to_string/#src.kamae.spark.transformers.string_list_to_string.StringListToStringTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which applies the given stringCaseType to the input column.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/string_list_to_string.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which applies the given stringCaseType to the input column.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    separator = self.getSeparator()\n\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n    if not isinstance(input_datatype, ArrayType):\n        raise TypeError(\n            f\"\"\"Input column {self.getInputCol()} must be of type ArrayType,\n            not {input_datatype}.\"\"\"\n        )\n    output_col = single_input_single_output_array_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: F.concat_ws(separator, x),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_list_to_string/#src.kamae.spark.transformers.string_list_to_string.StringListToStringTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the StringListToStringLayer transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that joins the string list.</p> Source code in <code>src/kamae/spark/transformers/string_list_to_string.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the StringListToStringLayer transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n    joins the string list.\n    \"\"\"\n    return StringListToStringLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        separator=self.getSeparator(),\n        axis=-1,\n        keepdims=True,\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_map/","title":"string_map","text":""},{"location":"reference/src/kamae/spark/transformers/string_map/#src.kamae.spark.transformers.string_map.StringMapTransformer","title":"StringMapTransformer","text":"<pre><code>StringMapTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    stringMatchValues=None,\n    stringReplaceValues=None,\n    defaultReplaceValue=None,\n    layerName=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>StringMapTransformerParams</code></p> <p>String Map Spark Transformer for use in Spark Pipelines. This transformer replaces a list of strings with the respective mapping value.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param stringMatchValues: List of string match constants. :param stringReplaceValues: List of string replace constants. :param defaultReplaceValue: Default value to replace the unmatched strings with. If None, the original string is kept unchanged. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/string_map.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    stringMatchValues: Optional[List[str]] = None,\n    stringReplaceValues: Optional[List[str]] = None,\n    defaultReplaceValue: Optional[str] = None,\n    layerName: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes an StringMapTransformer transformer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param stringMatchValues: List of string match constants.\n    :param stringReplaceValues: List of string replace constants.\n    :param defaultReplaceValue: Default value to replace the unmatched strings with.\n    If None, the original string is kept unchanged.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(\n        stringMatchValues=None,\n        stringReplaceValues=None,\n        defaultReplaceValue=None,\n    )\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_map/#src.kamae.spark.transformers.string_map.StringMapTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/string_map/#src.kamae.spark.transformers.string_map.StringMapTransformer._string_map","title":"_string_map","text":"<pre><code>_string_map(column)\n</code></pre> <p>Helper function to create a string map expression.</p> <p>:param column: Column to apply the string map operation to. :returns: Column with string map operation applied.</p> Source code in <code>src/kamae/spark/transformers/string_map.py</code> <pre><code>def _string_map(self, column: Column) -&gt; Column:\n    \"\"\"\n    Helper function to create a string map expression.\n\n    :param column: Column to apply the string map operation to.\n    :returns: Column with string map operation applied.\n    \"\"\"\n    col_expr: Column = None\n    for match_value, replace_value in zip(\n        self.getStringMatchValues(), self.getStringReplaceValues()\n    ):\n        if col_expr is None:\n            col_expr = F.when(column == F.lit(match_value), replace_value)\n        else:\n            col_expr = col_expr.when(column == F.lit(match_value), replace_value)\n    if self.getDefaultReplaceValue() is not None:\n        col_expr = col_expr.otherwise(self.getDefaultReplaceValue())\n    else:\n        col_expr = col_expr.otherwise(column)\n    return col_expr\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_map/#src.kamae.spark.transformers.string_map.StringMapTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which contains the result of the string map operation.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/string_map.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which contains the result of the string map operation.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    if self.getStringMatchValues() is None or self.getStringReplaceValues() is None:\n        raise ValueError(\n            \"stringMatchValues and stringReplaceValues cannot be None.\"\n        )\n    if len(self.getStringMatchValues()) != len(self.getStringReplaceValues()):\n        raise ValueError(\n            \"Length of stringMatchValues and stringReplaceValues must be equal.\"\n        )\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n    output_col = single_input_single_output_scalar_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: self._string_map(x),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_map/#src.kamae.spark.transformers.string_map.StringMapTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the StringMapLayer transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that performs a string replace operation.</p> Source code in <code>src/kamae/spark/transformers/string_map.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the StringMapLayer transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n    performs a string replace operation.\n    \"\"\"\n    return StringMapLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        string_match_values=self.getStringMatchValues(),\n        string_replace_values=self.getStringReplaceValues(),\n        default_replace_value=self.getDefaultReplaceValue(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_map/#src.kamae.spark.transformers.string_map.StringMapTransformerParams","title":"StringMapTransformerParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing StringMatchValues and StringReplaceValues needed for string map layers.</p>"},{"location":"reference/src/kamae/spark/transformers/string_map/#src.kamae.spark.transformers.string_map.StringMapTransformerParams.getDefaultReplaceValue","title":"getDefaultReplaceValue","text":"<pre><code>getDefaultReplaceValue()\n</code></pre> <p>Gets the defaultReplaceValue parameter.</p> <p>:returns: Default value to replace the unmatched strings with.</p> Source code in <code>src/kamae/spark/transformers/string_map.py</code> <pre><code>def getDefaultReplaceValue(self) -&gt; str:\n    \"\"\"\n    Gets the defaultReplaceValue parameter.\n\n    :returns: Default value to replace the unmatched strings with.\n    \"\"\"\n    return self.getOrDefault(self.defaultReplaceValue)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_map/#src.kamae.spark.transformers.string_map.StringMapTransformerParams.getStringMatchValues","title":"getStringMatchValues","text":"<pre><code>getStringMatchValues()\n</code></pre> <p>Gets the stringMatchValues parameter.</p> <p>:returns: List of string match constants.</p> Source code in <code>src/kamae/spark/transformers/string_map.py</code> <pre><code>def getStringMatchValues(self) -&gt; List[str]:\n    \"\"\"\n    Gets the stringMatchValues parameter.\n\n    :returns: List of string match constants.\n    \"\"\"\n    return self.getOrDefault(self.stringMatchValues)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_map/#src.kamae.spark.transformers.string_map.StringMapTransformerParams.getStringReplaceValues","title":"getStringReplaceValues","text":"<pre><code>getStringReplaceValues()\n</code></pre> <p>Gets the stringReplaceValues parameter.</p> <p>:returns: List of string replace constants.</p> Source code in <code>src/kamae/spark/transformers/string_map.py</code> <pre><code>def getStringReplaceValues(self) -&gt; List[str]:\n    \"\"\"\n    Gets the stringReplaceValues parameter.\n\n    :returns: List of string replace constants.\n    \"\"\"\n    return self.getOrDefault(self.stringReplaceValues)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_map/#src.kamae.spark.transformers.string_map.StringMapTransformerParams.setDefaultReplaceValue","title":"setDefaultReplaceValue","text":"<pre><code>setDefaultReplaceValue(value)\n</code></pre> <p>Sets the defaultReplaceValue parameter.</p> <p>:param value: Default value to replace the unmatched strings with. If None, the original string is kept unchanged. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/string_map.py</code> <pre><code>def setDefaultReplaceValue(self, value: str) -&gt; \"StringMapTransformerParams\":\n    \"\"\"\n    Sets the defaultReplaceValue parameter.\n\n    :param value: Default value to replace the unmatched strings with.\n    If None, the original string is kept unchanged.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(defaultReplaceValue=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_map/#src.kamae.spark.transformers.string_map.StringMapTransformerParams.setStringMatchValues","title":"setStringMatchValues","text":"<pre><code>setStringMatchValues(value)\n</code></pre> <p>Sets the stringMatchValues parameter.</p> <p>:param value: List of string match constants. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/string_map.py</code> <pre><code>def setStringMatchValues(self, value: List[str]) -&gt; \"StringMapTransformerParams\":\n    \"\"\"\n    Sets the stringMatchValues parameter.\n\n    :param value: List of string match constants.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if value is None or len(value) == 0:\n        raise ValueError(\"stringMatchValues cannot be empty.\")\n    return self._set(stringMatchValues=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_map/#src.kamae.spark.transformers.string_map.StringMapTransformerParams.setStringReplaceValues","title":"setStringReplaceValues","text":"<pre><code>setStringReplaceValues(value)\n</code></pre> <p>Sets the stringReplaceValues parameter.</p> <p>:param value: List of string replace constants. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/string_map.py</code> <pre><code>def setStringReplaceValues(self, value: List[str]) -&gt; \"StringMapTransformerParams\":\n    \"\"\"\n    Sets the stringReplaceValues parameter.\n\n    :param value: List of string replace constants.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if value is None or len(value) == 0:\n        raise ValueError(\"stringReplaceValues cannot be empty.\")\n    return self._set(stringReplaceValues=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_replace/","title":"string_replace","text":""},{"location":"reference/src/kamae/spark/transformers/string_replace/#src.kamae.spark.transformers.string_replace.StringReplaceParams","title":"StringReplaceParams","text":"<p>               Bases: <code>StringRegexParams</code></p> <p>Mixin class containing StringMatchConstant, StringReplaceConstant needed for string replace layers.</p>"},{"location":"reference/src/kamae/spark/transformers/string_replace/#src.kamae.spark.transformers.string_replace.StringReplaceParams.getStringMatchConstant","title":"getStringMatchConstant","text":"<pre><code>getStringMatchConstant()\n</code></pre> <p>Gets the stringConstant parameter.</p> <p>:returns: String replacement constant value to use in string replace layer.</p> Source code in <code>src/kamae/spark/transformers/string_replace.py</code> <pre><code>def getStringMatchConstant(self) -&gt; str:\n    \"\"\"\n    Gets the stringConstant parameter.\n\n    :returns: String replacement constant value to use in string replace layer.\n    \"\"\"\n    return self.getOrDefault(self.stringMatchConstant)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_replace/#src.kamae.spark.transformers.string_replace.StringReplaceParams.getStringReplaceConstant","title":"getStringReplaceConstant","text":"<pre><code>getStringReplaceConstant()\n</code></pre> <p>Gets the stringReplaceConstant parameter.</p> <p>:returns: String replacement constant value to use in string replace layer.</p> Source code in <code>src/kamae/spark/transformers/string_replace.py</code> <pre><code>def getStringReplaceConstant(self) -&gt; str:\n    \"\"\"\n    Gets the stringReplaceConstant parameter.\n\n    :returns: String replacement constant value to use in string replace layer.\n    \"\"\"\n    return self.getOrDefault(self.stringReplaceConstant)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_replace/#src.kamae.spark.transformers.string_replace.StringReplaceParams.setStringMatchConstant","title":"setStringMatchConstant","text":"<pre><code>setStringMatchConstant(value)\n</code></pre> <p>Sets the stringConstant parameter.</p> <p>:param value: String constant value to use in different string transformers. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/string_replace.py</code> <pre><code>def setStringMatchConstant(self, value: str) -&gt; \"StringReplaceParams\":\n    \"\"\"\n    Sets the stringConstant parameter.\n\n    :param value: String constant value to use in different string transformers.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(stringMatchConstant=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_replace/#src.kamae.spark.transformers.string_replace.StringReplaceParams.setStringReplaceConstant","title":"setStringReplaceConstant","text":"<pre><code>setStringReplaceConstant(value)\n</code></pre> <p>Sets the stringReplaceConstant parameter.</p> Source code in <code>src/kamae/spark/transformers/string_replace.py</code> <pre><code>def setStringReplaceConstant(self, value: str) -&gt; \"StringReplaceParams\":\n    \"\"\"\n    Sets the stringReplaceConstant parameter.\n    \"\"\"\n    return self._set(stringReplaceConstant=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_replace/#src.kamae.spark.transformers.string_replace.StringReplaceTransformer","title":"StringReplaceTransformer","text":"<pre><code>StringReplaceTransformer(\n    inputCol=None,\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    stringMatchConstant=None,\n    stringReplaceConstant=None,\n    regex=False,\n    layerName=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>MultiInputSingleOutputParams</code>, <code>StringReplaceParams</code></p> <p>String replace Spark Transformer for use in Spark Pipelines. This transformer performs a string replace operation on the input column.</p> <p>The transformer takes up to 3 input columns. The first input column is always required and is the column we operate on. A match constant/column is provided to match against and the replace constant/column is provided to replace the matched substrings with.</p> <p>KNOWN ISSUE: when replacing with a string that contains a backslash, the backslash must be double escaped (\\) in order to be added properly. This is consistent in both spark and tensorflow components.</p> <p>:param inputCol: Input column name. :param inputCols: List of up to 3 input column names. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param stringMatchConstant: String constant to match for. operation. :param stringReplaceConstant: String constant to replace with. :param regex: Whether to allow regex-matching in the string matching. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/string_replace.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    stringMatchConstant: Optional[str] = None,\n    stringReplaceConstant: Optional[str] = None,\n    regex: bool = False,\n    layerName: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes an StringReplaceTransformer transformer.\n\n    :param inputCol: Input column name.\n    :param inputCols: List of up to 3 input column names.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param stringMatchConstant: String constant to match for.\n    operation.\n    :param stringReplaceConstant: String constant to replace with.\n    :param regex: Whether to allow regex-matching in the string matching.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(\n        regex=False, stringMatchConstant=None, stringReplaceConstant=None\n    )\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_replace/#src.kamae.spark.transformers.string_replace.StringReplaceTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/string_replace/#src.kamae.spark.transformers.string_replace.StringReplaceTransformer._construct_input_cols","title":"_construct_input_cols","text":"<pre><code>_construct_input_cols()\n</code></pre> <p>Constructs the input columns for the transformer.</p> <p>:returns: List of pyspark columns.</p> Source code in <code>src/kamae/spark/transformers/string_replace.py</code> <pre><code>def _construct_input_cols(self) -&gt; List[Column]:\n    \"\"\"\n    Constructs the input columns for the transformer.\n\n    :returns: List of pyspark columns.\n    \"\"\"\n    if self.isDefined(\"inputCol\"):\n        input_cols = [F.col(self.getInputCol())]\n    elif self.isDefined(\"inputCols\"):\n        input_cols = [F.col(c) for c in self.getInputCols()]\n    else:\n        raise ValueError(\"Must specify either inputCol or inputCols.\")\n\n    if self.getStringReplaceConstant() is not None:\n        input_cols.insert(\n            len(input_cols),\n            F.lit(self.getStringReplaceConstant()).alias(\n                self.uid + \"_stringReplaceConstant\"\n            ),\n        )\n    if self.getStringMatchConstant() is not None:\n        input_cols.insert(\n            1,\n            F.lit(self.getStringMatchConstant()).alias(\n                self.uid + \"_stringMatchConstant\"\n            ),\n        )\n\n    if len(input_cols) &gt; 3:\n        raise ValueError(\n            \"\"\"When setting inputCols for StringReplaceTransformer,\n        there must be 1-3 columns. In the case 3 columns are specified, string\n        constants should not be specified.\"\"\"\n        )\n    elif len(input_cols) &lt; 3:\n        raise ValueError(\n            \"\"\"Less than 3 input columns were provided,\n        but no string constants were passed as arguments.\"\"\"\n        )\n\n    return input_cols\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_replace/#src.kamae.spark.transformers.string_replace.StringReplaceTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which contains the result of the string replace operation.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/string_replace.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which contains the result of the string replace operation.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_cols = self._construct_input_cols()\n    # input_cols can contain either actual columns or lit(constants). In order to\n    # determine the datatype of the input columns, we select them from the dataset\n    # first.\n    input_col_names = dataset.select(input_cols).columns\n    input_col_datatypes = [\n        self.get_column_datatype(dataset=dataset.select(input_cols), column_name=c)\n        for c in input_col_names\n    ]\n    regex = self.getRegex()\n\n    def string_replace(\n        x: Column, input_col_names: List[str], regex: bool\n    ) -&gt; Column:\n        col_expr = (\n            F.regexp_replace(\n                x[input_col_names[0]],\n                F.regexp_replace(\n                    x[input_col_names[1]], \"[^A-Za-z0-9]\", \"\\\\\\\\\" + \"$0\"\n                ),\n                x[input_col_names[2]],\n            )\n            if not regex\n            else F.regexp_replace(\n                x[input_col_names[0]],\n                F.when(x[input_col_names[1]] == F.lit(\"\"), F.lit(\"^$\")).otherwise(\n                    x[input_col_names[1]]\n                ),\n                x[input_col_names[2]],\n            )\n        )\n        return col_expr\n\n    output_col = multi_input_single_output_scalar_transform(\n        input_cols=input_cols,\n        input_col_names=input_col_names,\n        input_col_datatypes=input_col_datatypes,\n        func=lambda x: string_replace(x, input_col_names, regex),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_replace/#src.kamae.spark.transformers.string_replace.StringReplaceTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the StringReplaceLayer transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs a string replace operation.</p> Source code in <code>src/kamae/spark/transformers/string_replace.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the StringReplaceLayer transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs a string replace operation.\n    \"\"\"\n    return StringReplaceLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        regex=self.getRegex(),\n        string_match_constant=self.getStringMatchConstant(),\n        string_replace_constant=self.getStringReplaceConstant(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_replace/#src.kamae.spark.transformers.string_replace.StringReplaceTransformer.setInputCols","title":"setInputCols","text":"<pre><code>setInputCols(value)\n</code></pre> <p>Overrides setting the input columns for the transformer. Throws an error if we have more than 3 input columns.</p> <p>:param value: List of input columns. :returns: Class instance.</p> Source code in <code>src/kamae/spark/transformers/string_replace.py</code> <pre><code>def setInputCols(self, value: List[str]) -&gt; \"StringReplaceTransformer\":\n    \"\"\"\n    Overrides setting the input columns for the transformer.\n    Throws an error if we have more than 3 input columns.\n\n    :param value: List of input columns.\n    :returns: Class instance.\n    \"\"\"\n    if len(value) &gt; 3:\n        raise ValueError(\n            \"\"\"When setting inputCols for StringReplaceTransformer,\n            there must be 1-3 columns.\"\"\"\n        )\n\n    return self._set(inputCols=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_to_string_list/","title":"string_to_string_list","text":""},{"location":"reference/src/kamae/spark/transformers/string_to_string_list/#src.kamae.spark.transformers.string_to_string_list.StringToStringListParams","title":"StringToStringListParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing separator parameter needed for string to string list transforms.</p>"},{"location":"reference/src/kamae/spark/transformers/string_to_string_list/#src.kamae.spark.transformers.string_to_string_list.StringToStringListParams.getDefaultValue","title":"getDefaultValue","text":"<pre><code>getDefaultValue()\n</code></pre> <p>Gets the default value to use when the input is empty.</p> <p>:returns: Default value to use when the input is empty.</p> Source code in <code>src/kamae/spark/transformers/string_to_string_list.py</code> <pre><code>def getDefaultValue(self) -&gt; str:\n    \"\"\"\n    Gets the default value to use when the input is empty.\n\n    :returns: Default value to use when the input is empty.\n    \"\"\"\n    return self.getOrDefault(self.defaultValue)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_to_string_list/#src.kamae.spark.transformers.string_to_string_list.StringToStringListParams.getListLength","title":"getListLength","text":"<pre><code>getListLength()\n</code></pre> <p>Gets the length of the output list.</p> <p>:returns: Length of the output list.</p> Source code in <code>src/kamae/spark/transformers/string_to_string_list.py</code> <pre><code>def getListLength(self) -&gt; int:\n    \"\"\"\n    Gets the length of the output list.\n\n    :returns: Length of the output list.\n    \"\"\"\n    return self.getOrDefault(self.listLength)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_to_string_list/#src.kamae.spark.transformers.string_to_string_list.StringToStringListParams.getSeparator","title":"getSeparator","text":"<pre><code>getSeparator()\n</code></pre> <p>Gets the separator to use when joining the string list.</p> <p>:returns: Separator to use when joining the string list.</p> Source code in <code>src/kamae/spark/transformers/string_to_string_list.py</code> <pre><code>def getSeparator(self) -&gt; str:\n    \"\"\"\n    Gets the separator to use when joining the string list.\n\n    :returns: Separator to use when joining the string list.\n    \"\"\"\n    return self.getOrDefault(self.separator)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_to_string_list/#src.kamae.spark.transformers.string_to_string_list.StringToStringListParams.setDefaultValue","title":"setDefaultValue","text":"<pre><code>setDefaultValue(value)\n</code></pre> <p>Sets the default value to use when the input is empty.</p> <p>:param value: Default value to use when the input is empty. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/string_to_string_list.py</code> <pre><code>def setDefaultValue(self, value: str) -&gt; \"StringToStringListParams\":\n    \"\"\"\n    Sets the default value to use when the input is empty.\n\n    :param value: Default value to use when the input is empty.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(defaultValue=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_to_string_list/#src.kamae.spark.transformers.string_to_string_list.StringToStringListParams.setListLength","title":"setListLength","text":"<pre><code>setListLength(value)\n</code></pre> <p>Sets the length of the list.</p> <p>:param value: Length of the output list. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/string_to_string_list.py</code> <pre><code>def setListLength(self, value: int) -&gt; \"StringToStringListParams\":\n    \"\"\"\n    Sets the length of the list.\n\n    :param value: Length of the output list.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    if value &lt; 1:\n        raise ValueError(\"listLength must be greater than 0.\")\n    return self._set(listLength=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_to_string_list/#src.kamae.spark.transformers.string_to_string_list.StringToStringListParams.setSeparator","title":"setSeparator","text":"<pre><code>setSeparator(value)\n</code></pre> <p>Sets the separator to use when joining the string list.</p> <p>:param value: Separator to use when joining the string list. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/string_to_string_list.py</code> <pre><code>def setSeparator(self, value: str) -&gt; \"StringToStringListParams\":\n    \"\"\"\n    Sets the separator to use when joining the string list.\n\n    :param value: Separator to use when joining the string list.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(separator=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_to_string_list/#src.kamae.spark.transformers.string_to_string_list.StringToStringListTransformer","title":"StringToStringListTransformer","text":"<pre><code>StringToStringListTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    separator=\",\",\n    listLength=1,\n    defaultValue=\"\",\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>StringToStringListParams</code></p> <p>StringToStringListLayer Spark Transformer for use in Spark pipelines. This transformer takes a column of string lists and joins them into a single string.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param separator: Separator to use when joining the string list. Defaults to \",\". :param listLength: Length of the output list. Default is 1. :param defaultValue: Default value to use when the input is empty. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/string_to_string_list.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    separator: str = \",\",\n    listLength: int = 1,\n    defaultValue: str = \"\",\n) -&gt; None:\n    \"\"\"\n    Initializes an StringToStringListTransformer transformer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param separator: Separator to use when joining the string list.\n    Defaults to \",\".\n    :param listLength: Length of the output list. Default is 1.\n    :param defaultValue: Default value to use when the input is empty.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(separator=\",\")\n    self._setDefault(listLength=1)\n    self._setDefault(defaultValue=\"\")\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_to_string_list/#src.kamae.spark.transformers.string_to_string_list.StringToStringListTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/string_to_string_list/#src.kamae.spark.transformers.string_to_string_list.StringToStringListTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which is an array of strings created by splitting the input column by the separator.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/string_to_string_list.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which is an array of strings created by splitting the input column by the\n    separator.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    separator = re.escape(self.getSeparator())\n\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n\n    def string_to_string_list(x: Column, separator: str) -&gt; Column:\n        split_col = F.split(x, pattern=separator)\n        # Replace empty strings with default value\n        split_array_col = F.transform(\n            split_col,\n            lambda x: F.when(x == F.lit(\"\"), self.getDefaultValue()).otherwise(x),\n        )\n        # Pad/truncate array to size\n        padded_split_array_col = F.concat(\n            F.slice(split_array_col, 1, self.getListLength()),\n            F.array_repeat(\n                F.lit(self.getDefaultValue()),\n                self.getListLength() - F.size(split_array_col),\n            ),\n        )\n        return padded_split_array_col\n\n    output_col = single_input_single_output_scalar_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: string_to_string_list(x=x, separator=separator),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/string_to_string_list/#src.kamae.spark.transformers.string_to_string_list.StringToStringListTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the StringToStringListLayer transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that splits the string into a list of strings.</p> Source code in <code>src/kamae/spark/transformers/string_to_string_list.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the StringToStringListLayer transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n    splits the string into a list of strings.\n    \"\"\"\n    return StringToStringListLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        separator=self.getSeparator(),\n        default_value=self.getDefaultValue(),\n        list_length=self.getListLength(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/sub_string_delim_at_index/","title":"sub_string_delim_at_index","text":""},{"location":"reference/src/kamae/spark/transformers/sub_string_delim_at_index/#src.kamae.spark.transformers.sub_string_delim_at_index.SubStringDelimAtIndexParams","title":"SubStringDelimAtIndexParams","text":"<p>               Bases: <code>Params</code></p> <p>Mixin class containing delimiter &amp; index parameter needed for sub string transforms.</p>"},{"location":"reference/src/kamae/spark/transformers/sub_string_delim_at_index/#src.kamae.spark.transformers.sub_string_delim_at_index.SubStringDelimAtIndexParams.getDefaultValue","title":"getDefaultValue","text":"<pre><code>getDefaultValue()\n</code></pre> <p>Gets the defaultValue parameter.</p> <p>:returns: String value use as default if index is out of bounds.</p> Source code in <code>src/kamae/spark/transformers/sub_string_delim_at_index.py</code> <pre><code>def getDefaultValue(self) -&gt; str:\n    \"\"\"\n    Gets the defaultValue parameter.\n\n    :returns: String value use as default if index is out of bounds.\n    \"\"\"\n    return self.getOrDefault(self.defaultValue)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/sub_string_delim_at_index/#src.kamae.spark.transformers.sub_string_delim_at_index.SubStringDelimAtIndexParams.getDelimiter","title":"getDelimiter","text":"<pre><code>getDelimiter()\n</code></pre> <p>Gets the delimiter parameter.</p> <p>:returns: String value to split substring on.</p> Source code in <code>src/kamae/spark/transformers/sub_string_delim_at_index.py</code> <pre><code>def getDelimiter(self) -&gt; str:\n    \"\"\"\n    Gets the delimiter parameter.\n\n    :returns: String value to split substring on.\n    \"\"\"\n    return self.getOrDefault(self.delimiter)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/sub_string_delim_at_index/#src.kamae.spark.transformers.sub_string_delim_at_index.SubStringDelimAtIndexParams.getIndex","title":"getIndex","text":"<pre><code>getIndex()\n</code></pre> <p>Gets the index parameter.</p> <p>:returns: Integer value of index of substring to return.</p> Source code in <code>src/kamae/spark/transformers/sub_string_delim_at_index.py</code> <pre><code>def getIndex(self) -&gt; int:\n    \"\"\"\n    Gets the index parameter.\n\n    :returns: Integer value of index of substring to return.\n    \"\"\"\n    return self.getOrDefault(self.index)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/sub_string_delim_at_index/#src.kamae.spark.transformers.sub_string_delim_at_index.SubStringDelimAtIndexParams.setDefaultValue","title":"setDefaultValue","text":"<pre><code>setDefaultValue(value)\n</code></pre> <p>Sets the defaultValue parameter.</p> <p>:param value: String value use as default if index is out of bounds. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/sub_string_delim_at_index.py</code> <pre><code>def setDefaultValue(self, value: str) -&gt; \"SubStringDelimAtIndexParams\":\n    \"\"\"\n    Sets the defaultValue parameter.\n\n    :param value: String value use as default if index is out of bounds.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(defaultValue=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/sub_string_delim_at_index/#src.kamae.spark.transformers.sub_string_delim_at_index.SubStringDelimAtIndexParams.setDelimiter","title":"setDelimiter","text":"<pre><code>setDelimiter(value)\n</code></pre> <p>Sets the delimiter parameter.</p> <p>:param value: String value to split substring on. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/sub_string_delim_at_index.py</code> <pre><code>def setDelimiter(self, value: str) -&gt; \"SubStringDelimAtIndexParams\":\n    \"\"\"\n    Sets the delimiter parameter.\n\n    :param value: String value to split substring on.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(delimiter=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/sub_string_delim_at_index/#src.kamae.spark.transformers.sub_string_delim_at_index.SubStringDelimAtIndexParams.setIndex","title":"setIndex","text":"<pre><code>setIndex(value)\n</code></pre> <p>Sets the delimiter parameter.</p> <p>:param value: Index of substring to return. :returns: Instance of class mixed in.</p> Source code in <code>src/kamae/spark/transformers/sub_string_delim_at_index.py</code> <pre><code>def setIndex(self, value: int) -&gt; \"SubStringDelimAtIndexParams\":\n    \"\"\"\n    Sets the delimiter parameter.\n\n    :param value: Index of substring to return.\n    :returns: Instance of class mixed in.\n    \"\"\"\n    return self._set(index=value)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/sub_string_delim_at_index/#src.kamae.spark.transformers.sub_string_delim_at_index.SubStringDelimAtIndexTransformer","title":"SubStringDelimAtIndexTransformer","text":"<pre><code>SubStringDelimAtIndexTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    delimiter=None,\n    index=None,\n    defaultValue=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>SubStringDelimAtIndexParams</code></p> <p>Sub string at delimiter Spark Transformer for use in Spark pipelines. This transformer splits a string at a delimiter and returns the substring at the specified index. If the delimiter is the empty string, the string is split by characters. If the index is negative, start counting from the end of the string. If the index is out of bounds, the default value is returned.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param delimiter: Value to use to split the string into substrings. Default is \"_\". :param index: Once the string is split using delimiter, which index to return. If the index is negative, start counting from the end of the string. Default is 0. :param defaultValue: If the index is out of bounds after string split,  what value to return. Default is empty string. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/sub_string_delim_at_index.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    delimiter: Optional[str] = None,\n    index: Optional[int] = None,\n    defaultValue: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes an SubStringDelimAtIndexTransformer transformer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param delimiter: Value to use to split the string into substrings.\n    Default is \"_\".\n    :param index: Once the string is split using delimiter, which index to return.\n    If the index is negative, start counting from the end of the string.\n    Default is 0.\n    :param defaultValue: If the index is out of bounds after string split,\n     what value to return. Default is empty string.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(delimiter=\"_\")\n    self._setDefault(defaultValue=\"\")\n    self._setDefault(index=0)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/sub_string_delim_at_index/#src.kamae.spark.transformers.sub_string_delim_at_index.SubStringDelimAtIndexTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/sub_string_delim_at_index/#src.kamae.spark.transformers.sub_string_delim_at_index.SubStringDelimAtIndexTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which splits the input column at the delimiter and returns the substring at the specified index. If the index is out of bounds, the default value is returned.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/sub_string_delim_at_index.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which splits the input column at the delimiter and returns the substring\n    at the specified index. If the index is out of bounds, the default value\n    is returned.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    delimiter = re.escape(self.getDelimiter())\n    index = self.getIndex()\n    default_value = self.getDefaultValue()\n\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n    # Since element_at is a 1-based index , we need to add 1 to the index if it\n    # is non-negative.\n    one_based_index = index + 1 if index &gt;= 0 else index\n    output_col = single_input_single_output_scalar_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: F.coalesce(\n            F.element_at(F.split(x, delimiter), one_based_index),\n            F.lit(default_value),\n        ),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/sub_string_delim_at_index/#src.kamae.spark.transformers.sub_string_delim_at_index.SubStringDelimAtIndexTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for SubStringDelimAtIndexTransformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs sub string at delimiter.</p> Source code in <code>src/kamae/spark/transformers/sub_string_delim_at_index.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for SubStringDelimAtIndexTransformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs sub string at delimiter.\n    \"\"\"\n    return SubStringDelimAtIndexLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        delimiter=self.getDelimiter(),\n        index=self.getIndex(),\n        default_value=self.getDefaultValue(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/subtract/","title":"subtract","text":""},{"location":"reference/src/kamae/spark/transformers/subtract/#src.kamae.spark.transformers.subtract.SubtractTransformer","title":"SubtractTransformer","text":"<pre><code>SubtractTransformer(\n    inputCol=None,\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    mathFloatConstant=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>MultiInputSingleOutputParams</code>, <code>MathFloatConstantParams</code></p> <p>SubtractLayer Spark Transformer for use in Spark pipelines. This transformer subtracts a column by a constant or another column.</p> <p>:param inputCol: Input column name. Only used if inputCols is not specified. If specified, we divide this column by the mathFloatConstant. :param inputCols: Input column names. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param mathFloatConstant: Optional constant to divide by. If not provided, then two input columns are required. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/subtract.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    mathFloatConstant: Optional[float] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes an SubtractTransformer transformer.\n\n    :param inputCol: Input column name. Only used if inputCols is not specified.\n    If specified, we divide this column by the mathFloatConstant.\n    :param inputCols: Input column names.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param mathFloatConstant: Optional constant to divide by. If not provided,\n    then two input columns are required.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(mathFloatConstant=None)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/subtract/#src.kamae.spark.transformers.subtract.SubtractTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/subtract/#src.kamae.spark.transformers.subtract.SubtractTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which is the same as the column with name <code>inputCol</code>.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/subtract.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which is the same as the column with name `inputCol`.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_cols = self.get_multiple_input_cols(\n        constant_param_name=\"mathFloatConstant\",\n    )\n    # input_cols can contain either actual columns or lit(constants). In order to\n    # determine the datatype of the input columns, we select them from the dataset\n    # first.\n    input_col_names = dataset.select(input_cols).columns\n    input_col_datatypes = [\n        self.get_column_datatype(dataset=dataset.select(input_cols), column_name=c)\n        for c in input_col_names\n    ]\n    output_col = multi_input_single_output_scalar_transform(\n        input_cols=input_cols,\n        input_col_names=input_col_names,\n        input_col_datatypes=input_col_datatypes,\n        func=lambda x: reduce(sub, [x[c] for c in input_col_names]),\n    )\n\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/subtract/#src.kamae.spark.transformers.subtract.SubtractTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the divide transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs a divide operation.</p> Source code in <code>src/kamae/spark/transformers/subtract.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the divide transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs a divide operation.\n    \"\"\"\n    return SubtractLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        subtrahend=self.getMathFloatConstant(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/sum/","title":"sum","text":""},{"location":"reference/src/kamae/spark/transformers/sum/#src.kamae.spark.transformers.sum.SumTransformer","title":"SumTransformer","text":"<pre><code>SumTransformer(\n    inputCol=None,\n    inputCols=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    layerName=None,\n    mathFloatConstant=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>MultiInputSingleOutputParams</code>, <code>MathFloatConstantParams</code></p> <p>SumLayer Spark Transformer for use in Spark pipelines. This transformer sums a column with a constant or another column.</p> <p>:param inputCol: Input column name. Only used if inputCols is not specified. If specified, we sum this column by the mathFloatConstant. :param inputCols: Input column names. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column(s) to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param layerName: Name of the layer. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :param mathFloatConstant: Optional constant to sum. If not provided, then two input columns are required. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/sum.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    inputCols: Optional[List[str]] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    layerName: Optional[str] = None,\n    mathFloatConstant: Optional[float] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes an SumTransformer transformer.\n\n    :param inputCol: Input column name. Only used if inputCols is not specified.\n    If specified, we sum this column by the mathFloatConstant.\n    :param inputCols: Input column names.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column(s) to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param layerName: Name of the layer. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :param mathFloatConstant: Optional constant to sum. If not provided,\n    then two input columns are required.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(mathFloatConstant=None)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/sum/#src.kamae.spark.transformers.sum.SumTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/sum/#src.kamae.spark.transformers.sum.SumTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input dataset. Creates a new column with name <code>outputCol</code>, which is the same as the column with name <code>inputCol</code>.</p> <p>:param dataset: Pyspark dataframe to transform. :returns: Transformed pyspark dataframe.</p> Source code in <code>src/kamae/spark/transformers/sum.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input dataset. Creates a new column with name `outputCol`,\n    which is the same as the column with name `inputCol`.\n\n    :param dataset: Pyspark dataframe to transform.\n    :returns: Transformed pyspark dataframe.\n    \"\"\"\n    input_cols = self.get_multiple_input_cols(\n        constant_param_name=\"mathFloatConstant\",\n    )\n    # input_cols can contain either actual columns or lit(constants). In order to\n    # determine the datatype of the input columns, we select them from the dataset\n    # first.\n    input_col_names = dataset.select(input_cols).columns\n    input_col_datatypes = [\n        self.get_column_datatype(dataset=dataset.select(input_cols), column_name=c)\n        for c in input_col_names\n    ]\n    output_col = multi_input_single_output_scalar_transform(\n        input_cols=input_cols,\n        input_col_names=input_col_names,\n        input_col_datatypes=input_col_datatypes,\n        func=lambda x: reduce(add, [x[c] for c in input_col_names]),\n    )\n\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/sum/#src.kamae.spark.transformers.sum.SumTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer for the sum transformer.</p> <p>:returns: Tensorflow keras layer with name equal to the layerName parameter that  performs a sum operation.</p> Source code in <code>src/kamae/spark/transformers/sum.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer for the sum transformer.\n\n    :returns: Tensorflow keras layer with name equal to the layerName parameter that\n     performs a sum operation.\n    \"\"\"\n    return SumLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        addend=self.getMathFloatConstant(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/unix_timestamp_to_date_time/","title":"unix_timestamp_to_date_time","text":""},{"location":"reference/src/kamae/spark/transformers/unix_timestamp_to_date_time/#src.kamae.spark.transformers.unix_timestamp_to_date_time.UnixTimestampToDateTimeTransformer","title":"UnixTimestampToDateTimeTransformer","text":"<pre><code>UnixTimestampToDateTimeTransformer(\n    inputCol=None,\n    outputCol=None,\n    inputDtype=None,\n    outputDtype=None,\n    unit=\"s\",\n    includeTime=True,\n    layerName=None,\n)\n</code></pre> <p>               Bases: <code>BaseTransformer</code>, <code>SingleInputSingleOutputParams</code>, <code>UnixTimestampParams</code>, <code>DateTimeParams</code></p> <p>Transformer that converts a unix timestamp to a datetime.</p> <p>The unix timestamp can be in milliseconds or seconds, set by the <code>unit</code> parameter. If the <code>includeTime</code> parameter is set to True, the output will be in yyyy-MM-dd HHss.SSS format. If set to False, the output will be in yyyy-MM-dd format.</p> <p>:param inputCol: Input column name. :param outputCol: Output column name. :param inputDtype: Input data type to cast input column to before transforming. :param outputDtype: Output data type to cast the output column to after transforming. :param unit: Unit of the timestamp. Can be <code>milliseconds</code> (shorthand <code>ms</code>)  or <code>seconds</code> (shorthand <code>s</code>). Default is <code>s</code> (seconds). :param includeTime: Whether to include the time in the output. Default is True. :param layerName: Layer name. Used as the name of the tensorflow layer in the keras model. If not set, we use the uid of the Spark transformer. :returns: None - class instantiated.</p> Source code in <code>src/kamae/spark/transformers/unix_timestamp_to_date_time.py</code> <pre><code>@keyword_only\ndef __init__(\n    self,\n    inputCol: Optional[str] = None,\n    outputCol: Optional[str] = None,\n    inputDtype: Optional[str] = None,\n    outputDtype: Optional[str] = None,\n    unit: str = \"s\",\n    includeTime: bool = True,\n    layerName: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialises the UnixTimestampToDateTimeTransformer.\n\n    :param inputCol: Input column name.\n    :param outputCol: Output column name.\n    :param inputDtype: Input data type to cast input column to before\n    transforming.\n    :param outputDtype: Output data type to cast the output column to after\n    transforming.\n    :param unit: Unit of the timestamp. Can be `milliseconds` (shorthand `ms`)\n     or `seconds` (shorthand `s`). Default is `s` (seconds).\n    :param includeTime: Whether to include the time in the output. Default is True.\n    :param layerName: Layer name. Used as the name of the tensorflow layer\n    in the keras model. If not set, we use the uid of the Spark transformer.\n    :returns: None - class instantiated.\n    \"\"\"\n    super().__init__()\n    self._setDefault(unit=\"s\", includeTime=True)\n    kwargs = self._input_kwargs\n    self.setParams(**kwargs)\n    # TODO: Remove this when we only support PySpark 3.5+. It is only used to get\n    #  the timezone set by the user for datetime operations. In 3.5+ we can use the\n    #  current_timezone() function. Also is there a better way to access this than\n    #  inside a class attribute? Setting it at the top of the file causes issues\n    #  in tests as we import all transformers when the package is loaded.\n    self.spark = SparkSession.builder.getOrCreate()\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/unix_timestamp_to_date_time/#src.kamae.spark.transformers.unix_timestamp_to_date_time.UnixTimestampToDateTimeTransformer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/spark/transformers/unix_timestamp_to_date_time/#src.kamae.spark.transformers.unix_timestamp_to_date_time.UnixTimestampToDateTimeTransformer._transform","title":"_transform","text":"<pre><code>_transform(dataset)\n</code></pre> <p>Transforms the input integer timestamp to the date string with format yyyy-MM-dd HHss.SSS.</p> <p>:param dataset: Input dataframe. :returns: Transformed dataframe.</p> Source code in <code>src/kamae/spark/transformers/unix_timestamp_to_date_time.py</code> <pre><code>def _transform(self, dataset: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the input integer timestamp to the date string with format\n    yyyy-MM-dd HH:mm:ss.SSS.\n\n    :param dataset: Input dataframe.\n    :returns: Transformed dataframe.\n    \"\"\"\n\n    input_datatype = self.get_column_datatype(\n        dataset=dataset, column_name=self.getInputCol()\n    )\n\n    def unix_timestamp_to_datetime(\n        unix_timestamp: Column, include_time: bool\n    ) -&gt; Column:\n        \"\"\"\n        Returns the date in yyyy-MM-dd HH:mm:ss.SSS format from a Unix timestamp\n        in seconds.\n\n        :param unix_timestamp: Unix timestamp in seconds.\n        :param include_time: Whether to include the time in the output.\n        :returns: Column of the date in yyyy-MM-dd HH:mm:ss.SSS format if\n        include_time is True, otherwise in yyyy-MM-dd format.\n        \"\"\"\n        # from_unixtime throws away milliseconds, so we have to calculate them\n        # separately\n        milliseconds_3dp = (\n            (unix_timestamp - F.floor(unix_timestamp)) * 1000.0\n        ).cast(\"int\")\n        local_datetime_str_wo_millis = F.from_unixtime(\n            unix_timestamp, format=\"yyyy-MM-dd HH:mm:ss\"\n        )\n        local_datetime_str_w_millis = F.concat(\n            local_datetime_str_wo_millis,\n            F.lit(\".\"),\n            F.lpad(milliseconds_3dp.cast(\"string\"), 3, \"0\"),\n        )\n        local_datetime = F.to_timestamp(\n            local_datetime_str_w_millis, format=\"yyyy-MM-dd HH:mm:ss.SSS\"\n        )\n        utc_datetime = F.to_utc_timestamp(\n            local_datetime, self.spark.conf.get(\"spark.sql.session.timeZone\")\n        )\n        date_fmt = \"yyyy-MM-dd HH:mm:ss.SSS\" if include_time else \"yyyy-MM-dd\"\n        return F.date_format(utc_datetime, date_fmt)\n\n    output_col = single_input_single_output_scalar_transform(\n        input_col=F.col(self.getInputCol()),\n        input_col_datatype=input_datatype,\n        func=lambda x: unix_timestamp_to_datetime(x, self.getIncludeTime())\n        if self.getUnit() == \"s\"\n        else unix_timestamp_to_datetime(x / F.lit(1000), self.getIncludeTime()),\n    )\n    return dataset.withColumn(self.getOutputCol(), output_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/transformers/unix_timestamp_to_date_time/#src.kamae.spark.transformers.unix_timestamp_to_date_time.UnixTimestampToDateTimeTransformer.get_tf_layer","title":"get_tf_layer","text":"<pre><code>get_tf_layer()\n</code></pre> <p>Gets the tensorflow layer that performs the unix timestamp to date transform.</p> <p>:returns: Tensorflow layer that performs the unix timestamp to date transform.</p> Source code in <code>src/kamae/spark/transformers/unix_timestamp_to_date_time.py</code> <pre><code>def get_tf_layer(self) -&gt; tf.keras.layers.Layer:\n    \"\"\"\n    Gets the tensorflow layer that performs the unix timestamp to date transform.\n\n    :returns: Tensorflow layer that performs the unix timestamp to date transform.\n    \"\"\"\n    return UnixTimestampToDateTimeLayer(\n        name=self.getLayerName(),\n        input_dtype=self.getInputTFDtype(),\n        output_dtype=self.getOutputTFDtype(),\n        unit=self.getUnit(),\n        include_time=self.getIncludeTime(),\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/","title":"utils","text":""},{"location":"reference/src/kamae/spark/utils/array_utils/","title":"array_utils","text":""},{"location":"reference/src/kamae/spark/utils/array_utils/#src.kamae.spark.utils.array_utils.broadcast_scalar_column_to_array","title":"broadcast_scalar_column_to_array","text":"<pre><code>broadcast_scalar_column_to_array(\n    scalar_column, array_column, array_column_datatype\n)\n</code></pre> <p>Broadcasts a scalar column to a (possibly nested) array column. Repeats the scalar value to match the shape of the given array column.</p> <p>:param scalar_column: Spark column to broadcast. :param array_column: Spark array column to broadcast to. :param array_column_datatype: DataType of the array column. Used to understand how nested the array column is. :returns: Spark array column with the scalar column broadcasted to it.</p> Source code in <code>src/kamae/spark/utils/array_utils.py</code> <pre><code>def broadcast_scalar_column_to_array(\n    scalar_column: Column, array_column: Column, array_column_datatype: DataType\n) -&gt; Column:\n    \"\"\"\n    Broadcasts a scalar column to a (possibly nested) array column. Repeats the scalar\n    value to match the shape of the given array column.\n\n    :param scalar_column: Spark column to broadcast.\n    :param array_column: Spark array column to broadcast to.\n    :param array_column_datatype: DataType of the array column. Used to understand how\n    nested the array column is.\n    :returns: Spark array column with the scalar column broadcasted to it.\n    \"\"\"\n    nested_level = get_array_nesting_level(array_column_datatype)\n    nested_transform_func = nested_transform(\n        func=lambda x: scalar_column, nest_level=nested_level\n    )\n    return nested_transform_func(array_column)\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/array_utils/#src.kamae.spark.utils.array_utils.broadcast_scalar_column_to_array_with_inner_singleton_array","title":"broadcast_scalar_column_to_array_with_inner_singleton_array","text":"<pre><code>broadcast_scalar_column_to_array_with_inner_singleton_array(\n    scalar_column, array_column, array_column_datatype\n)\n</code></pre> <p>Similar to broadcast_scalar_column_to_array, but does not repeat the scalar value in the final innermost array.</p> <p>Broadcasts a scalar column to a (possibly nested) array column. Repeats the scalar value to match the shape of the first N-1 of an N-dim array. The final innermost array is left as a singleton array. Used for the array concatenate operation where we do not want to repeat the value in the final array.</p> <p>Example: scalar_column = 1 array_column = [[1, 2], [3, 4]] array_column_datatype = ArrayType(ArrayType(IntegerType()))</p> <p>Output: [[1], [1]]</p> <p>Using <code>broadcast_scalar_column_to_array</code> with the same example would return</p> <p>Output: [[1, 1], [1, 1]]</p> <p>:param scalar_column: Spark column to broadcast. :param array_column: Spark array column to broadcast to. :param array_column_datatype: DataType of the array column. Used to understand how nested the array column is. :returns: Spark array column with the scalar column broadcasted to it, and with a singleton array in the innermost position.</p> Source code in <code>src/kamae/spark/utils/array_utils.py</code> <pre><code>def broadcast_scalar_column_to_array_with_inner_singleton_array(\n    scalar_column: Column, array_column: Column, array_column_datatype: DataType\n) -&gt; Column:\n    \"\"\"\n    Similar to broadcast_scalar_column_to_array, but does not repeat the scalar value in\n    the final innermost array.\n\n    Broadcasts a scalar column to a (possibly nested) array column. Repeats the scalar\n    value to match the shape of the first N-1 of an N-dim array. The final innermost\n    array is left as a singleton array. Used for the array concatenate operation where\n    we do not want to repeat the value in the final array.\n\n    Example:\n    scalar_column = 1\n    array_column = [[1, 2], [3, 4]]\n    array_column_datatype = ArrayType(ArrayType(IntegerType()))\n\n    Output: [[1], [1]]\n\n    Using `broadcast_scalar_column_to_array` with the same example would return\n\n    Output: [[1, 1], [1, 1]]\n\n    :param scalar_column: Spark column to broadcast.\n    :param array_column: Spark array column to broadcast to.\n    :param array_column_datatype: DataType of the array column. Used to understand how\n    nested the array column is.\n    :returns: Spark array column with the scalar column broadcasted to it, and with a\n    singleton array in the innermost position.\n    \"\"\"\n    nested_level = get_array_nesting_level(array_column_datatype)\n    nested_transform_func = nested_transform(\n        func=lambda x: F.array(scalar_column), nest_level=nested_level - 1\n    )\n    return nested_transform_func(array_column)\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/array_utils/#src.kamae.spark.utils.array_utils.build_udf_return_type","title":"build_udf_return_type","text":"<pre><code>build_udf_return_type(element_dtype, nest_level)\n</code></pre> <p>Builds the return type of a UDF that applies a function to each element of a nested array. This is used to specify the return type of the UDF.</p> <p>:param element_dtype: DataType of the elements of the nested array. :param nest_level: Nesting level of the array column. :returns: DataType of the return type of the UDF.</p> Source code in <code>src/kamae/spark/utils/array_utils.py</code> <pre><code>def build_udf_return_type(element_dtype: DataType, nest_level: int) -&gt; DataType:\n    \"\"\"\n    Builds the return type of a UDF that applies a function to each element of a nested\n    array. This is used to specify the return type of the UDF.\n\n    :param element_dtype: DataType of the elements of the nested array.\n    :param nest_level: Nesting level of the array column.\n    :returns: DataType of the return type of the UDF.\n    \"\"\"\n    if nest_level == 0:\n        return element_dtype\n    elif nest_level &lt; 0:\n        raise ValueError(\"nest_level must be greater than or equal to 0.\")\n    for _ in range(nest_level):\n        element_dtype = ArrayType(element_dtype)\n    return element_dtype\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/array_utils/#src.kamae.spark.utils.array_utils.flatten_nested_arrays","title":"flatten_nested_arrays","text":"<pre><code>flatten_nested_arrays(column, column_data_type)\n</code></pre> <p>Flattens a nested array to a single array.</p> <p>:param column: Spark array column to be flattened :param column_data_type: Datatype of the Spark array column. :returns: Flattened array column.</p> Source code in <code>src/kamae/spark/utils/array_utils.py</code> <pre><code>def flatten_nested_arrays(column: Column, column_data_type: DataType) -&gt; Column:\n    \"\"\"\n    Flattens a nested array to a single array.\n\n    :param column: Spark array column to be flattened\n    :param column_data_type: Datatype of the Spark array column.\n    :returns: Flattened array column.\n    \"\"\"\n    nested_level = get_array_nesting_level(column_dtype=column_data_type)\n    flattened_column = column\n    for _ in range(1, nested_level):\n        flattened_column = F.flatten(flattened_column)\n\n    return flattened_column\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/array_utils/#src.kamae.spark.utils.array_utils.get_array_nesting_level","title":"get_array_nesting_level","text":"<pre><code>get_array_nesting_level(column_dtype, start_level=0)\n</code></pre> <p>Calls get_array_nesting_level_and_element_dtype to determine the nesting level of a Spark array column.</p> <p>:param column_dtype: DataType of the column to check. :param start_level: Counter of the number of times it had to recurse to get the element type of the array column. Default is 0. :returns: The nesting level of the array column.</p> Source code in <code>src/kamae/spark/utils/array_utils.py</code> <pre><code>def get_array_nesting_level(column_dtype: DataType, start_level: int = 0) -&gt; int:\n    \"\"\"\n    Calls get_array_nesting_level_and_element_dtype to determine the nesting level of a\n    Spark array column.\n\n    :param column_dtype: DataType of the column to check.\n    :param start_level: Counter of the number of times it had to recurse to get the\n    element type of the array column. Default is 0.\n    :returns: The nesting level of the array column.\n    \"\"\"\n    return get_array_nesting_level_and_element_dtype(column_dtype, start_level)[0]\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/array_utils/#src.kamae.spark.utils.array_utils.get_array_nesting_level_and_element_dtype","title":"get_array_nesting_level_and_element_dtype","text":"<pre><code>get_array_nesting_level_and_element_dtype(\n    column_dtype, start_level=0\n)\n</code></pre> <p>Determines the nesting level of a Spark array column. Also returns the underlying type of the column. Recursively attempts to get the element type of the array column until it is no longer an array. Keeps a counter of the number of times it had to recurse to get the element type. This counter is the nesting level of the array column. Returns this counter along with the last found element type.</p> <p>:param column_dtype: DataType of the column to check. :param start_level: Counter of the number of times it had to recurse to get the element type of the array column. Default is 0. :returns: Tuple of the nesting level of the array column and the element type of the array column.</p> Source code in <code>src/kamae/spark/utils/array_utils.py</code> <pre><code>def get_array_nesting_level_and_element_dtype(\n    column_dtype: DataType, start_level: int = 0\n) -&gt; Tuple[int, DataType]:\n    \"\"\"\n    Determines the nesting level of a Spark array column. Also returns the underlying\n    type of the column. Recursively attempts to get the element type of the array column\n    until it is no longer an array. Keeps a counter of the number of times it had to\n    recurse to get the element type. This counter is the nesting level of the array\n    column. Returns this counter along with the last found element type.\n\n    :param column_dtype: DataType of the column to check.\n    :param start_level: Counter of the number of times it had to recurse to get the\n    element type of the array column. Default is 0.\n    :returns: Tuple of the nesting level of the array column and the element type of the\n    array column.\n    \"\"\"\n    try:\n        child_dtype = column_dtype.elementType\n    except AttributeError:\n        return start_level, column_dtype\n    else:\n        return get_array_nesting_level_and_element_dtype(\n            column_dtype=child_dtype, start_level=start_level + 1\n        )\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/array_utils/#src.kamae.spark.utils.array_utils.get_element_type","title":"get_element_type","text":"<pre><code>get_element_type(dtype)\n</code></pre> <p>Gets the element type of given datatype. If the datatype is not an array, it returns the datatype itself.</p> <p>:param dtype: The datatype to get the element type of. :returns: The element type of the datatype.</p> Source code in <code>src/kamae/spark/utils/array_utils.py</code> <pre><code>def get_element_type(dtype: DataType) -&gt; DataType:\n    \"\"\"\n    Gets the element type of given datatype. If the datatype is not an array,\n    it returns the datatype itself.\n\n    :param dtype: The datatype to get the element type of.\n    :returns: The element type of the datatype.\n    \"\"\"\n    return get_array_nesting_level_and_element_dtype(dtype)[1]\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/array_utils/#src.kamae.spark.utils.array_utils.nested_arrays_zip","title":"nested_arrays_zip","text":"<pre><code>nested_arrays_zip(columns, nest_level, column_names=None)\n</code></pre> <p>Zips multiple columns of (possibly nested) array type.</p> <p>:param columns: List of Spark array columns to be zipped. :param nest_level: Nesting level of the array columns. :param column_names: Optional list of column names to use for the zipped array. If provided, the final array of structs will use these as the names of the columns. If not set, the columns will be named \"input_0\", \"input_1\", etc. If set, must be the same length as the number of columns. :returns: Zipped array column.</p> Source code in <code>src/kamae/spark/utils/array_utils.py</code> <pre><code>def nested_arrays_zip(\n    columns: List[Column], nest_level: int, column_names: Optional[List[str]] = None\n) -&gt; Column:\n    \"\"\"\n    Zips multiple columns of (possibly nested) array type.\n\n    :param columns: List of Spark array columns to be zipped.\n    :param nest_level: Nesting level of the array columns.\n    :param column_names: Optional list of column names to use for the zipped array. If\n    provided, the final array of structs will use these as the names of the columns.\n    If not set, the columns will be named \"input_0\", \"input_1\", etc.\n    If set, must be the same length as the number of columns.\n    :returns: Zipped array column.\n    \"\"\"\n    if column_names is not None:\n        if len(column_names) != len(columns):\n            raise ValueError(\n                f\"\"\"column_names must be the same length as the number of columns.\n                Received {len(column_names)} column names for {len(columns)} columns.\"\"\"\n            )\n    else:\n        column_names = [f\"input_{i}\" for i in range(len(columns))]\n\n    # First collect everything into a struct, so we can operate on a single column\n    arrays_zipped = F.struct(*[c.alias(n) for n, c in zip(column_names, columns)])\n    # For each successive nesting level below this, zip the arrays together, each time\n    # pushing the struct down one level.\n    for n_lvl in range(0, nest_level):\n        arrays_zipped = nested_transform(\n            func=lambda x: F.arrays_zip(*[x[n].alias(n) for n in column_names]),\n            nest_level=n_lvl,\n        )(arrays_zipped)\n\n    return arrays_zipped\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/array_utils/#src.kamae.spark.utils.array_utils.nested_lambda","title":"nested_lambda","text":"<pre><code>nested_lambda(func, nest_level)\n</code></pre> <p>Similar to nested_transform, however in this case creates a lambda function that applies a function to each element of a nested array. Used within UDFs to apply a function to each element of a nested array.</p> <p>:param func: Function to apply to each element of the nested array column. :param nest_level: Nesting level of the array column. :returns: Nested lambda function.</p> Source code in <code>src/kamae/spark/utils/array_utils.py</code> <pre><code>def nested_lambda(func: Callable, nest_level: int) -&gt; Callable:\n    \"\"\"\n    Similar to nested_transform, however in this case creates a lambda function that\n    applies a function to each element of a nested array. Used within UDFs to apply a\n    function to each element of a nested array.\n\n    :param func: Function to apply to each element of the nested array column.\n    :param nest_level: Nesting level of the array column.\n    :returns: Nested lambda function.\n    \"\"\"\n\n    def apply_func_to_list(x: List[Any], function: Callable[[Any], Any]) -&gt; List[Any]:\n        return [function(y) for y in x]\n\n    if nest_level == 0:\n        return func\n    elif nest_level &lt; 0:\n        raise ValueError(\"nest_level must be greater than or equal to 0.\")\n    return lambda x: apply_func_to_list(\n        x, nested_lambda(func, nest_level=nest_level - 1)\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/array_utils/#src.kamae.spark.utils.array_utils.nested_transform","title":"nested_transform","text":"<pre><code>nested_transform(func, nest_level)\n</code></pre> <p>Creates a nested version of <code>pyspark.sql.functions.transform</code> that can be used to apply a given <code>func</code> elementwise on a nested array column.</p> <p>:param func: Function to apply to each element of the nested array column. :param nest_level: Nesting level of the array column. :returns: Nested transform function.</p> Source code in <code>src/kamae/spark/utils/array_utils.py</code> <pre><code>def nested_transform(\n    func: Callable[[Column], Column], nest_level: int\n) -&gt; Callable[[Column], Column]:\n    \"\"\"\n    Creates a nested version of `pyspark.sql.functions.transform` that can be used to\n    apply a given `func` elementwise on a nested array column.\n\n    :param func: Function to apply to each element of the nested array column.\n    :param nest_level: Nesting level of the array column.\n    :returns: Nested transform function.\n    \"\"\"\n    if nest_level &lt;= 0:\n        # If the nesting level is 0 or less, we have run out of arrays and are now at\n        # the scalar level.\n        return func\n    return lambda x: F.transform(x, nested_transform(func, nest_level=nest_level - 1))\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/indexer_utils/","title":"indexer_utils","text":""},{"location":"reference/src/kamae/spark/utils/indexer_utils/#src.kamae.spark.utils.indexer_utils.collect_labels_array","title":"collect_labels_array","text":"<pre><code>collect_labels_array(\n    dataset,\n    column,\n    column_datatype,\n    string_order_type=\"frequencyDesc\",\n    mask_token=None,\n    max_num_labels=None,\n)\n</code></pre> <p>Collects the string labels from a given column in a given dataset.</p> <p>:param dataset: Spark input dataset. :param column: Spark input column. :param column_datatype: Spark input column datatype. :param string_order_type: String order type, one of 'frequencyAsc', 'frequencyDesc', 'alphabeticalAsc', 'alphabeticalDesc' :param mask_token: Optional value to mask when indexing. :param max_num_labels: Optional value to limit the number of labels.</p> <p>:returns: List of string labels.</p> Source code in <code>src/kamae/spark/utils/indexer_utils.py</code> <pre><code>def collect_labels_array(\n    dataset: DataFrame,\n    column: Column,\n    column_datatype: DataType,\n    string_order_type: str = \"frequencyDesc\",\n    mask_token: Optional[str] = None,\n    max_num_labels: Optional[int] = None,\n) -&gt; List[str]:\n    \"\"\"\n    Collects the string labels from a given column in a given dataset.\n\n    :param dataset: Spark input dataset.\n    :param column: Spark input column.\n    :param column_datatype: Spark input column datatype.\n    :param string_order_type: String order type, one of\n    'frequencyAsc', 'frequencyDesc', 'alphabeticalAsc', 'alphabeticalDesc'\n    :param mask_token: Optional value to mask when indexing.\n    :param max_num_labels: Optional value to limit the number of labels.\n\n    :returns: List of string labels.\n    \"\"\"\n    possible_order_options = [\n        \"frequencyAsc\",\n        \"frequencyDesc\",\n        \"alphabeticalAsc\",\n        \"alphabeticalDesc\",\n    ]\n    if string_order_type not in possible_order_options:\n        raise ValueError(\n            f\"string_order_type must be one of {', '.join(possible_order_options)}\"\n        )\n\n    if (max_num_labels is not None) and (not isinstance(max_num_labels, int)):\n        raise ValueError(\"max_labels_count must be an integer\")\n\n    input_col_an_array = isinstance(column_datatype, ArrayType)\n\n    if input_col_an_array:\n        # Flatten the array to a single array\n        flattened_array_col = flatten_nested_arrays(\n            column=column, column_data_type=column_datatype\n        )\n        # If the input column is an array, we need to flatten it twice\n        # before we can collect the string labels\n        rdd_vals = (\n            dataset.select(flattened_array_col.cast(\"array&lt;string&gt;\"))\n            .rdd.map(lambda x: x.asDict().values())\n            .flatMap(lambda x: x)\n            .flatMap(lambda x: x)\n        )\n    else:\n        # Otherwise, since it is scalar we can just collect the string labels\n        rdd_vals = dataset.select(column.cast(\"string\")).rdd.map(lambda x: x[0])\n\n    # If the mask token is defined, we remove it from the labels.\n    # This is because otherwise Tensorflow can throw an error when\n    # the mask token is not in the correct position of the labels.\n    rdd_vals = (\n        rdd_vals.filter(lambda x: x != mask_token)\n        if mask_token is not None\n        else rdd_vals\n    )\n    sort_ascending = string_order_type in [\"alphabeticalAsc\", \"frequencyAsc\"]\n    if string_order_type in [\"frequencyAsc\", \"frequencyDesc\"]:\n        label_vals = (\n            rdd_vals.filter(lambda x: x is not None)\n            .map(lambda x: (x, 1))\n            .reduceByKey(lambda x, y: x + y)\n            .sortBy(lambda x: x[1], ascending=sort_ascending)\n            .map(lambda x: x[0])\n        )\n    else:\n        label_vals = (\n            rdd_vals.filter(lambda x: x is not None)\n            .distinct()\n            .sortBy(lambda x: x, ascending=sort_ascending)\n        )\n\n    if max_num_labels is not None:\n        return label_vals.take(max_num_labels)\n    else:\n        return label_vals.collect()\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/indexer_utils/#src.kamae.spark.utils.indexer_utils.collect_labels_array_from_multiple_columns","title":"collect_labels_array_from_multiple_columns","text":"<pre><code>collect_labels_array_from_multiple_columns(\n    dataset,\n    columns,\n    column_datatypes,\n    string_order_type=\"frequencyDesc\",\n    mask_token=None,\n    max_num_labels=None,\n)\n</code></pre> <p>Collects the string labels across multiple columns in a given dataset.</p> <p>:param dataset: Spark input dataset :param columns: List of Spark input columns. :param column_datatypes: List of Spark input column datatypes. :param string_order_type: String order type, one of 'frequencyAsc', 'frequencyDesc', 'alphabeticalAsc', 'alphabeticalDesc' :param mask_token: Optional value to mask when indexing. :param max_num_labels: Optional value to limit the number of labels. :returns: List of string labels.</p> Source code in <code>src/kamae/spark/utils/indexer_utils.py</code> <pre><code>def collect_labels_array_from_multiple_columns(\n    dataset: DataFrame,\n    columns: List[Column],\n    column_datatypes: List[DataType],\n    string_order_type: str = \"frequencyDesc\",\n    mask_token: Optional[str] = None,\n    max_num_labels: Optional[int] = None,\n) -&gt; List[str]:\n    \"\"\"\n    Collects the string labels across multiple columns in a given dataset.\n\n    :param dataset: Spark input dataset\n    :param columns: List of Spark input columns.\n    :param column_datatypes: List of Spark input column datatypes.\n    :param string_order_type: String order type, one of\n    'frequencyAsc', 'frequencyDesc', 'alphabeticalAsc', 'alphabeticalDesc'\n    :param mask_token: Optional value to mask when indexing.\n    :param max_num_labels: Optional value to limit the number of labels.\n    :returns: List of string labels.\n    \"\"\"\n    flattened_arrays = []\n    for column, column_datatype in zip(columns, column_datatypes):\n        column_is_array = isinstance(column_datatype, ArrayType)\n        if column_is_array:\n            # Flatten any arrays and add to the list\n            flattened_arrays.append(\n                flatten_nested_arrays(column=column, column_data_type=column_datatype)\n            )\n        else:\n            flattened_arrays.append(F.array(column))\n\n    concatenated_array = F.concat(*flattened_arrays).cast(\"array&lt;string&gt;\")\n\n    return collect_labels_array(\n        dataset=dataset,\n        column=concatenated_array,\n        column_datatype=ArrayType(StringType()),\n        string_order_type=string_order_type,\n        mask_token=mask_token,\n        max_num_labels=max_num_labels,\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/indexer_utils/#src.kamae.spark.utils.indexer_utils.safe_hash64","title":"safe_hash64","text":"<pre><code>safe_hash64(label)\n</code></pre> <p>Attempt to hash a string label and raise an exception if it contains a null character.</p> <p>:param label: String to hash. :raises ValueError: If the label contains a null character. :returns: Hashed integer value.</p> Source code in <code>src/kamae/spark/utils/indexer_utils.py</code> <pre><code>def safe_hash64(label: str) -&gt; int:\n    \"\"\"\n    Attempt to hash a string label and raise an exception if it contains a null\n    character.\n\n    :param label: String to hash.\n    :raises ValueError: If the label contains a null character.\n    :returns: Hashed integer value.\n    \"\"\"\n    try:\n        return hash64(label)\n    except ValueError as e:\n        if str(e) == \"embedded null character\":\n            raise ValueError(\n                f\"\"\"Label {label} contains a null character.\n                These cause issues with hashing. You should remove these from your data.\n                https://en.wikipedia.org/wiki/Null_character\n                \"\"\"\n            ).with_traceback(e.__traceback__)\n        else:\n            raise e\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/list_utils/","title":"list_utils","text":""},{"location":"reference/src/kamae/spark/utils/list_utils/#src.kamae.spark.utils.list_utils.check_listwise_columns","title":"check_listwise_columns","text":"<pre><code>check_listwise_columns(\n    dataset,\n    query_col_name,\n    value_col_name,\n    sort_col_name=None,\n)\n</code></pre> <p>Check if the query and value columns are array columns in the dataset. If so throw an error since these do not support arrays currently.</p> <p>:param dataset: DataFrame containing the query and value columns. :param query_col_name: Name of the query column. :param value_col_name: Name of the value column.</p> <p>:raises ValueError: If the query or value column is an array column. :returns: None</p> Source code in <code>src/kamae/spark/utils/list_utils.py</code> <pre><code>def check_listwise_columns(\n    dataset: DataFrame,\n    query_col_name: str,\n    value_col_name: str,\n    sort_col_name: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Check if the query and value columns are array columns in the dataset.\n    If so throw an error since these do not support arrays currently.\n\n    :param dataset: DataFrame containing the query and value columns.\n    :param query_col_name: Name of the query column.\n    :param value_col_name: Name of the value column.\n\n    :raises ValueError: If the query or value column is an array column.\n    :returns: None\n    \"\"\"\n    query_col_datatype = dataset.schema[query_col_name].dataType\n    value_col_datatype = dataset.schema[value_col_name].dataType\n\n    if isinstance(query_col_datatype, ArrayType):\n        raise ValueError(\"Query column cannot be an array column\")\n\n    if isinstance(value_col_datatype, ArrayType):\n        raise ValueError(\"Value column cannot be an array column\")\n\n    if sort_col_name is not None:\n        sort_col_datatype = dataset.schema[sort_col_name].dataType\n        if isinstance(sort_col_datatype, ArrayType):\n            raise ValueError(\"Sort column cannot be an array column\")\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/list_utils/#src.kamae.spark.utils.list_utils.get_listwise_condition_and_window","title":"get_listwise_condition_and_window","text":"<pre><code>get_listwise_condition_and_window(\n    query_col,\n    value_col,\n    sort_col=None,\n    sort_order=\"asc\",\n    sort_top_n=None,\n    min_filter_value=None,\n)\n</code></pre> <p>Get the condition and window operations for listwise statistics calculation.</p> <p>:param query_col: Column containing the query id. :param value_col: Column containing the value to calculate statistics on. :param sort_col: Column to sort the values by. Default is None. :param sort_order: Order to sort the values by. Default is \"asc\". :param sort_top_n: Number of top values to consider for statistics calculation. Default is None. :param min_filter_value: Minimum value to consider for statistics calculation. Default is None. :returns: Tuple of the condition and window operations.</p> Source code in <code>src/kamae/spark/utils/list_utils.py</code> <pre><code>def get_listwise_condition_and_window(\n    query_col: Column,\n    value_col: Column,\n    sort_col: Optional[Column] = None,\n    sort_order: str = \"asc\",\n    sort_top_n: Optional[int] = None,\n    min_filter_value: Optional[float] = None,\n) -&gt; (Column, WindowSpec):\n    \"\"\"\n    Get the condition and window operations for listwise statistics calculation.\n\n    :param query_col: Column containing the query id.\n    :param value_col: Column containing the value to calculate statistics on.\n    :param sort_col: Column to sort the values by. Default is None.\n    :param sort_order: Order to sort the values by. Default is \"asc\".\n    :param sort_top_n: Number of top values to consider for statistics calculation.\n    Default is None.\n    :param min_filter_value: Minimum value to consider for statistics calculation.\n    Default is None.\n    :returns: Tuple of the condition and window operations.\n    \"\"\"\n    condition_col = None\n    window_spec = Window.partitionBy(query_col)\n\n    # Define statistics calculation condition based on topN and sortOrder\n    if sort_col is not None:\n        if sort_order == \"asc\":\n            sort_col = sort_col.asc()\n        elif sort_order == \"desc\":\n            sort_col = sort_col.desc()\n        else:\n            ValueError(f\"Invalid sortOrder: {sort_order}\")\n        rank_fun = F.row_number().over(window_spec.orderBy(sort_col))\n        if sort_top_n is not None:\n            condition_col = rank_fun &lt;= sort_top_n\n        else:\n            ValueError(\"sortTopN must be set if sortCol is set\")\n    else:\n        condition_col = F.lit(True)\n\n    # Define statistics calculation condition based on min filter value\n    if min_filter_value is not None:\n        condition_col = condition_col &amp; (value_col &gt;= F.lit(min_filter_value))\n\n    return condition_col, window_spec\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/scaler_utils/","title":"scaler_utils","text":""},{"location":"reference/src/kamae/spark/utils/scaler_utils/#src.kamae.spark.utils.scaler_utils.construct_nested_elements_for_scaling","title":"construct_nested_elements_for_scaling","text":"<pre><code>construct_nested_elements_for_scaling(\n    column, column_datatype, array_dim\n)\n</code></pre> <p>Scaling nested elements in Spark is difficult and requires us to extract the <code>ith</code> element from the innermost array to compute the moments on. This function creates multiple columns, one for each dimension of the inner array and flattens them into a single array. It then explodes these out so that the mean and variance can be computed on the flattened array.</p> <p>Only intended to be used for the StandardScaleEstimator, ConditionalStandardScaleEstimator &amp; MinMaxScaleEstimator.</p> <p>:param column: The input column to extract the element from. :param column_datatype: The datatype of the input column. :param array_dim: The dimension of the innermost array. :returns: A column containing a struct of (possibly exploded) elements.</p> Source code in <code>src/kamae/spark/utils/scaler_utils.py</code> <pre><code>def construct_nested_elements_for_scaling(\n    column: Column,\n    column_datatype: DataType,\n    array_dim: int,\n) -&gt; Column:\n    \"\"\"\n    Scaling nested elements in Spark is difficult and requires us to extract the `ith`\n    element from the innermost array to compute the moments on.\n    This function creates multiple columns, one for each dimension of the inner array\n    and flattens them into a single array. It then explodes these out so that the mean\n    and variance can be computed on the flattened array.\n\n    Only intended to be used for the StandardScaleEstimator,\n    ConditionalStandardScaleEstimator &amp; MinMaxScaleEstimator.\n\n    :param column: The input column to extract the element from.\n    :param column_datatype: The datatype of the input column.\n    :param array_dim: The dimension of the innermost array.\n    :returns: A column containing a struct of (possibly exploded) elements.\n    \"\"\"\n    nested_lvl, element_dtype = get_array_nesting_level_and_element_dtype(\n        column_datatype\n    )\n    elements = [\n        single_input_single_output_array_transform(\n            input_col=column,\n            input_col_datatype=column_datatype,\n            func=lambda x: F.element_at(x, idx),\n        ).alias(f\"element_{idx}\")\n        for idx in range(1, array_dim + 1)\n    ]\n    if nested_lvl &gt; 1:\n        # If the element is a nested array, we need to flatten it.\n        flat_elements = [\n            flatten_nested_arrays(\n                column=e,\n                column_data_type=build_udf_return_type(\n                    element_dtype=element_dtype, nest_level=nested_lvl - 1\n                ),\n            )\n            for e in elements\n        ]\n        # We then zip the flattened elements together and call explode.\n        # Explode is needed in order to use the spark mean and stddev functions\n        # over the arrays.\n        return F.explode(F.arrays_zip(*flat_elements)).alias(\"element_struct\")\n\n    else:\n        # If the element is scalar then we wrap it in a struct to mimic\n        # the same structure as the nested arrays.\n        return F.struct(*elements).alias(\"element_struct\")\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/transform_utils/","title":"transform_utils","text":""},{"location":"reference/src/kamae/spark/utils/transform_utils/#src.kamae.spark.utils.transform_utils._multi_input_single_output_transform","title":"_multi_input_single_output_transform","text":"<pre><code>_multi_input_single_output_transform(\n    input_cols,\n    input_col_datatypes,\n    input_col_names,\n    func,\n    scalar,\n)\n</code></pre> <p>Applies either a scalar or array Spark function to multiple input columns and returns a single output column.</p> <p>NOTE: This function expects a <code>func</code> that takes a single column returning a single column. Under the hood we will zip the input columns into a single column and apply <code>func</code> to the zipped column. Therefore <code>func</code> should be cognizant of the names of the elements in the zipped array column.</p> <p>Example adding two input columns together with names <code>input_0</code> and <code>input_1</code>: func = lambda x: x[\"input_0\"] + x[\"input_1\"]</p> <p>Ideally this function should not be used directly, the user should choose one of the below scalar vs array functions for their usecase. As every transform is either scalar or array, not both. This function is used to reduce code duplication between the scalar and array functions.</p> <p>:param input_cols: List of input columns. :param input_col_datatypes: List of input column datatypes. :param input_col_names: List of input column names. If any of the columns are arrays, the names of the elements in the zipped array column will be the same as these names. :param func: Function to apply to the zipped input column. :param scalar: Whether the function to be applied is a scalar function or an array function. :returns: Output column.</p> Source code in <code>src/kamae/spark/utils/transform_utils.py</code> <pre><code>def _multi_input_single_output_transform(\n    input_cols: List[Column],\n    input_col_datatypes: List[DataType],\n    input_col_names: List[str],\n    func: Callable[[Column], Column],\n    scalar: bool,\n) -&gt; Column:\n    \"\"\"\n    Applies either a scalar or array Spark function to multiple input columns and\n    returns a single output column.\n\n    NOTE: This function expects a `func` that takes a single column returning a single\n    column. Under the hood we will zip the input columns into a single column and apply\n    `func` to the zipped column. Therefore `func` should be cognizant of the names of\n    the elements in the zipped array column.\n\n    Example adding two input columns together with names `input_0` and `input_1`:\n    func = lambda x: x[\"input_0\"] + x[\"input_1\"]\n\n    Ideally this function should not be used directly, the user\n    should choose one of the below scalar vs array functions for their usecase. As every\n    transform is either scalar or array, not both. This function is used to reduce code\n    duplication between the scalar and array functions.\n\n    :param input_cols: List of input columns.\n    :param input_col_datatypes: List of input column datatypes.\n    :param input_col_names: List of input column names. If any of the columns are\n    arrays, the names of the elements in the zipped array column will be the same as\n    these names.\n    :param func: Function to apply to the zipped input column.\n    :param scalar: Whether the function to be applied is a scalar function or an\n    array function.\n    :returns: Output column.\n    \"\"\"\n    inputs_are_scalar = [\n        not isinstance(datatype, ArrayType) for datatype in input_col_datatypes\n    ]\n    inputs_are_array = [\n        isinstance(datatype, ArrayType) for datatype in input_col_datatypes\n    ]\n\n    if not scalar and any(inputs_are_scalar):\n        raise ValueError(\n            f\"\"\"Expected all input columns to be of type ArrayType,\n            received {input_col_datatypes} instead.\"\"\"\n        )\n\n    if not (all(inputs_are_array) or all(inputs_are_scalar)):\n        # If the inputs are not either all scalar or all arrays, then we have a mix of\n        # scalars and arrays. In this case we need to broadcast the scalars to the size\n        # of the arrays.\n        scalar_columns = [\n            (idx, col_w_datatype[0])\n            for idx, col_w_datatype in enumerate(zip(input_cols, input_col_datatypes))\n            if not isinstance(col_w_datatype[1], ArrayType)\n        ]\n        array_columns_w_types = [\n            (idx, col_w_datatype[0], col_w_datatype[1])\n            for idx, col_w_datatype in enumerate(zip(input_cols, input_col_datatypes))\n            if isinstance(col_w_datatype[1], ArrayType)\n        ]\n        # Broadcast the scalar to the size of the arrays. Use the first array column\n        # to determine the size of the broadcasted scalar. Assumes all arrays are\n        # of the same size.\n        broadcasted_scalars = [\n            (\n                idx,\n                broadcast_scalar_column_to_array(\n                    scalar_column=scalar_column,\n                    array_column=array_columns_w_types[0][1],\n                    array_column_datatype=array_columns_w_types[0][2],\n                ),\n            )\n            for idx, scalar_column in scalar_columns\n        ]\n        # Resort the array columns and the broadcasted scalars, so they match the order\n        # of the input columns.\n        columns_w_idx = [\n            (idx, column) for idx, column, _ in array_columns_w_types\n        ] + broadcasted_scalars\n        sorted_columns = [\n            column for idx, column in sorted(columns_w_idx, key=lambda x: x[0])\n        ]\n        # Check all arrays have same nesting level\n        nested_levels = [\n            get_array_nesting_level(column_dtype)\n            for _, _, column_dtype in array_columns_w_types\n        ]\n        if not all(nested_level == nested_levels[0] for nested_level in nested_levels):\n            raise ValueError(\n                f\"\"\"Expected all input columns to have the same array nesting level,\n                received {nested_levels} instead.\"\"\"\n            )\n        nested_level = nested_levels[0]\n        zipped_array_column = nested_arrays_zip(\n            columns=sorted_columns,\n            nest_level=nested_level,\n            column_names=input_col_names,\n        )\n    else:\n        # Otherwise, we have all scalars or all arrays and so can just zip together\n        # the input columns.\n        nested_level = get_array_nesting_level(column_dtype=input_col_datatypes[0])\n        zipped_array_column = nested_arrays_zip(\n            columns=input_cols, nest_level=nested_level, column_names=input_col_names\n        )\n\n    if not scalar:\n        # If the function is not scalar then it operates on the full array, therefore\n        # we reduce the nested level by 1.\n        nested_level -= 1\n\n    nested_func = nested_transform(func=func, nest_level=nested_level)\n    return nested_func(zipped_array_column)\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/transform_utils/#src.kamae.spark.utils.transform_utils._single_input_single_output_transform","title":"_single_input_single_output_transform","text":"<pre><code>_single_input_single_output_transform(\n    input_col, input_col_datatype, func, scalar\n)\n</code></pre> <p>Applies either a scalar or array Spark function to a single input column and returns a single output column. Ideally this function should not be used directly, the user should choose one of the below scalar vs array functions for their usecase. As every transform is either scalar or array, not both. This function is used to reduce code duplication between the scalar and array functions.</p> <p>:param input_col: Input column. :param input_col_datatype: Input column datatype. :param func: Function to apply to the input column. :param scalar: Whether the function to be applied is a scalar function or an array function. :returns: Output column.</p> Source code in <code>src/kamae/spark/utils/transform_utils.py</code> <pre><code>def _single_input_single_output_transform(\n    input_col: Column,\n    input_col_datatype: DataType,\n    func: Callable[[Column], Column],\n    scalar: bool,\n) -&gt; Column:\n    \"\"\"\n    Applies either a scalar or array Spark function to a single input column and returns\n    a single output column. Ideally this function should not be used directly, the user\n    should choose one of the below scalar vs array functions for their usecase. As every\n    transform is either scalar or array, not both. This function is used to reduce code\n    duplication between the scalar and array functions.\n\n    :param input_col: Input column.\n    :param input_col_datatype: Input column datatype.\n    :param func: Function to apply to the input column.\n    :param scalar: Whether the function to be applied is a scalar function or an\n    array function.\n    :returns: Output column.\n    \"\"\"\n    if not scalar and not isinstance(input_col_datatype, ArrayType):\n        raise ValueError(\n            f\"\"\"Input column was expected to be an array but received datatype\n            {input_col_datatype}\n            \"\"\"\n        )\n    nested_level = get_array_nesting_level(input_col_datatype)\n    if not scalar:\n        # If the function is not scalar then it operates on the full array, therefore\n        # we reduce the nested level by 1.\n        nested_level -= 1\n    nested_transform_func = nested_transform(func, nested_level)\n    return nested_transform_func(input_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/transform_utils/#src.kamae.spark.utils.transform_utils._single_input_single_output_udf_transform","title":"_single_input_single_output_udf_transform","text":"<pre><code>_single_input_single_output_udf_transform(\n    input_col,\n    input_col_datatype,\n    func,\n    udf_return_element_datatype,\n    scalar,\n)\n</code></pre> <p>Applies a Python function (e.g. a lambda function that we will wrap into a UDF) to a single input column and returns a single output column. Ideally this function should not be used directly, the user should choose one of the below scalar vs array functions for their usecase. As every transform is either scalar or array, not both. This function is used to reduce code duplication between the scalar and array functions.</p> <p>:param input_col: Input column. :param input_col_datatype: Input column datatype. :param func: Function to apply to the input column. :param udf_return_element_datatype: Datatype of the UDF return type. Should be the raw underlying type. :param scalar: Whether the function to be applied is a scalar function or an array function. :returns: Output column.</p> Source code in <code>src/kamae/spark/utils/transform_utils.py</code> <pre><code>def _single_input_single_output_udf_transform(\n    input_col: Column,\n    input_col_datatype: DataType,\n    func: Callable,\n    udf_return_element_datatype: DataType,\n    scalar: bool,\n) -&gt; Column:\n    \"\"\"\n    Applies a Python function (e.g. a lambda function that we will wrap into a\n    UDF) to a single input column and returns a single output column. Ideally this\n    function should not be used directly, the user should choose one of the below scalar\n    vs array functions for their usecase. As every transform is either scalar or array,\n    not both. This function is used to reduce code duplication between the scalar and\n    array functions.\n\n    :param input_col: Input column.\n    :param input_col_datatype: Input column datatype.\n    :param func: Function to apply to the input column.\n    :param udf_return_element_datatype: Datatype of the UDF return type. Should be the\n    raw underlying type.\n    :param scalar: Whether the function to be applied is a scalar function or an\n    array function.\n    :returns: Output column.\n    \"\"\"\n    if not scalar and not isinstance(input_col_datatype, ArrayType):\n        raise ValueError(\n            f\"\"\"Input column was expected to be an array but received datatype\n            {input_col_datatype}\n            \"\"\"\n        )\n    nested_level = get_array_nesting_level(input_col_datatype)\n    udf_return_type = build_udf_return_type(\n        element_dtype=udf_return_element_datatype, nest_level=nested_level\n    )\n    if not scalar:\n        # If the function is not scalar then it operates on the full array, therefore\n        # we reduce the nested level by 1.\n        nested_level -= 1\n\n    nested_lambda_func = nested_lambda(\n        func=func,\n        nest_level=nested_level,\n    )\n    udf_func = F.udf(nested_lambda_func, udf_return_type)\n    return udf_func(input_col)\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/transform_utils/#src.kamae.spark.utils.transform_utils.multi_input_single_output_array_transform","title":"multi_input_single_output_array_transform","text":"<pre><code>multi_input_single_output_array_transform(\n    input_cols, input_col_datatypes, input_col_names, func\n)\n</code></pre> <p>Applies an array Spark function (e.g. a Spark standard library function that operates on an array directly, as opposed to elementwise on arrays) to multiple input columns and returns a single output column. Caters for the case, where the input column is a nested array, in which case the function is applied to the innermost array.</p> <p>NOTE: Function zips the multple inputs into a single column and so <code>func</code> should reference the input names of the new zipped struct column.</p> <p>Example: <code>func = lambda x: reduce(add, [x[c] for c in input_col_names])</code></p> <p>:param input_cols: List of input columns. :param input_col_datatypes: List of input column datatypes. :param input_col_names: List of input column names. If any of the columns are arrays, the names of the elements in the zipped array column will be the same as these names. :param func: Function to apply to the input columns in the case they are all scalar. arrays. Main difference here is that the function must be cognizant of the names of the elements in the zipped array column. :returns: Output column.</p> Source code in <code>src/kamae/spark/utils/transform_utils.py</code> <pre><code>def multi_input_single_output_array_transform(\n    input_cols: List[Column],\n    input_col_datatypes: List[DataType],\n    input_col_names: List[str],\n    func: Callable[[Column], Column],\n) -&gt; Column:\n    \"\"\"\n    Applies an array Spark function (e.g. a Spark standard library function that\n    operates on an array directly, as opposed to elementwise on arrays) to multiple\n    input columns and returns a single output column. Caters for the case, where the\n    input column is a nested array, in which case the function is applied to the\n    innermost array.\n\n    NOTE: Function zips the multple inputs into a single column and so `func` should\n    reference the input names of the new zipped struct column.\n\n    Example:\n    `func = lambda x: reduce(add, [x[c] for c in input_col_names])`\n\n    :param input_cols: List of input columns.\n    :param input_col_datatypes: List of input column datatypes.\n    :param input_col_names: List of input column names. If any of the columns are\n    arrays, the names of the elements in the zipped array column will be the same as\n    these names.\n    :param func: Function to apply to the input columns in the case they are all scalar.\n    arrays. Main difference here is that the function must be cognizant of the names of\n    the elements in the zipped array column.\n    :returns: Output column.\n    \"\"\"\n    return _multi_input_single_output_transform(\n        input_cols=input_cols,\n        input_col_datatypes=input_col_datatypes,\n        input_col_names=input_col_names,\n        func=func,\n        scalar=False,\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/transform_utils/#src.kamae.spark.utils.transform_utils.multi_input_single_output_scalar_transform","title":"multi_input_single_output_scalar_transform","text":"<pre><code>multi_input_single_output_scalar_transform(\n    input_cols, input_col_datatypes, input_col_names, func\n)\n</code></pre> <p>Applies a scalar Spark function (e.g. a Spark standard library function that can be applied elementwise to arrays) to multiple input columns and returns a single output column. Caters for the case, where the input columns are:</p> <ol> <li>All scalars.</li> <li>All (possibly nested) arrays.</li> <li>A mix of scalars and (possibly nested) arrays.</li> </ol> <p>Zips the arrays into a single (potentially nested array) column. Applies <code>func</code> to the zipped column. <code>func</code> must be cognizant of the names of the elements in the zipped array column. If the inputs are a mix of scalars and arrays, broadcast the scalar to the size of the arrays before zipping the input columns.</p> <p>Example of <code>func</code> for a transformer that sums a list of input columns:</p> <p><code>func = lambda x: reduce(add, [x[c] for c in input_col_names])</code></p> <p>:param input_cols: List of input columns. :param input_col_datatypes: List of input column datatypes. :param input_col_names: List of input column names. If any of the columns are arrays, the names of the elements in the zipped array column will be the same as these names. :param func: Function to apply to the input columns in the case they are all scalar. :returns: Output column.</p> Source code in <code>src/kamae/spark/utils/transform_utils.py</code> <pre><code>def multi_input_single_output_scalar_transform(\n    input_cols: List[Column],\n    input_col_datatypes: List[DataType],\n    input_col_names: List[str],\n    func: Callable[[Column], Column],\n) -&gt; Column:\n    \"\"\"\n    Applies a scalar Spark function (e.g. a Spark standard library function that can be\n    applied elementwise to arrays) to multiple input columns and returns a single output\n    column. Caters for the case, where the input columns are:\n\n    1. All scalars.\n    2. All (possibly nested) arrays.\n    3. A mix of scalars and (possibly nested) arrays.\n\n    Zips the arrays into a single (potentially nested array) column.\n    Applies `func` to the zipped column. `func` must be cognizant of the names of the\n    elements in the zipped array column. If the inputs are a mix of scalars and arrays,\n    broadcast the scalar to the size of the arrays before zipping the input columns.\n\n    Example of `func` for a transformer that sums a list of input columns:\n\n    `func = lambda x: reduce(add, [x[c] for c in input_col_names])`\n\n    :param input_cols: List of input columns.\n    :param input_col_datatypes: List of input column datatypes.\n    :param input_col_names: List of input column names. If any of the columns are\n    arrays, the names of the elements in the zipped array column will be the same as\n    these names.\n    :param func: Function to apply to the input columns in the case they are all scalar.\n    :returns: Output column.\n    \"\"\"\n    return _multi_input_single_output_transform(\n        input_cols=input_cols,\n        input_col_datatypes=input_col_datatypes,\n        input_col_names=input_col_names,\n        func=func,\n        scalar=True,\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/transform_utils/#src.kamae.spark.utils.transform_utils.single_input_single_output_array_transform","title":"single_input_single_output_array_transform","text":"<pre><code>single_input_single_output_array_transform(\n    input_col, input_col_datatype, func\n)\n</code></pre> <p>Applies an array Spark function (e.g. a Spark standard library function that operates on an array directly, as opposed to elementwise on arrays) to a single input column and returns a single output column. Caters for the case, where the input column is a nested array, in which case the function is applied to the innermost array.</p> <p>:param input_col: Input column. :param input_col_datatype: Input column datatype. :param func: Function to apply to the input column. :returns: Output column.</p> Source code in <code>src/kamae/spark/utils/transform_utils.py</code> <pre><code>def single_input_single_output_array_transform(\n    input_col: Column, input_col_datatype: DataType, func: Callable[[Column], Column]\n) -&gt; Column:\n    \"\"\"\n    Applies an array Spark function (e.g. a Spark standard library function that\n    operates on an array directly, as opposed to elementwise on arrays) to a single\n    input column and returns a single output column. Caters for the case, where the\n    input column is a nested array, in which case the function is applied to the\n    innermost array.\n\n    :param input_col: Input column.\n    :param input_col_datatype: Input column datatype.\n    :param func: Function to apply to the input column.\n    :returns: Output column.\n    \"\"\"\n    return _single_input_single_output_transform(\n        input_col, input_col_datatype, func, scalar=False\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/transform_utils/#src.kamae.spark.utils.transform_utils.single_input_single_output_array_udf_transform","title":"single_input_single_output_array_udf_transform","text":"<pre><code>single_input_single_output_array_udf_transform(\n    input_col,\n    input_col_datatype,\n    func,\n    udf_return_element_datatype,\n)\n</code></pre> <p>Applies an array Python function (e.g. a lambda function that operates on an array directly, as opposed to elementwise on arrays which is wrapped into a UDF) to a single input column and returns a single output column. Caters for the case, where the input column is a nested array, in which case the function is applied to the innermost array.</p> <p><code>func</code> should be provided as a python function, and not wrapped in the Spark UDF wrapper.</p> <p>:param input_col: Input column. :param input_col_datatype: Input column datatype. :param func: Function to apply to the input column. :param udf_return_element_datatype: Datatype of the UDF return type. Should be the raw underlying type. :returns: Output column.</p> Source code in <code>src/kamae/spark/utils/transform_utils.py</code> <pre><code>def single_input_single_output_array_udf_transform(\n    input_col: Column,\n    input_col_datatype: DataType,\n    func: Callable,\n    udf_return_element_datatype: DataType,\n) -&gt; Column:\n    \"\"\"\n    Applies an array Python function (e.g. a lambda function that operates on an array\n    directly, as opposed to elementwise on arrays which is wrapped into a UDF) to a\n    single input column and returns a single output column. Caters for the case, where\n    the input column is a nested array, in which case the function is applied to the\n    innermost array.\n\n    `func` should be provided as a python function, and not wrapped in the Spark UDF\n    wrapper.\n\n    :param input_col: Input column.\n    :param input_col_datatype: Input column datatype.\n    :param func: Function to apply to the input column.\n    :param udf_return_element_datatype: Datatype of the UDF return type. Should be the\n    raw underlying type.\n    :returns: Output column.\n    \"\"\"\n    return _single_input_single_output_udf_transform(\n        input_col, input_col_datatype, func, udf_return_element_datatype, scalar=False\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/transform_utils/#src.kamae.spark.utils.transform_utils.single_input_single_output_scalar_transform","title":"single_input_single_output_scalar_transform","text":"<pre><code>single_input_single_output_scalar_transform(\n    input_col, input_col_datatype, func\n)\n</code></pre> <p>Applies a scalar Spark function (e.g. a Spark standard library function that can be applied elementwise to arrays) to a single input column and returns a single output column. Caters for the case, where the input column is:</p> <ol> <li>A scalar.</li> <li>A (possibly nested) array.</li> </ol> <p>If the input is a scalar, apply <code>func</code> directly to the input column. If the input is an array, apply <code>func</code> elementwise to the input column.</p> <p>:param input_col: Input column. :param input_col_datatype: Input column datatype. :param func: Function to apply to the input column. :returns: Output column.</p> Source code in <code>src/kamae/spark/utils/transform_utils.py</code> <pre><code>def single_input_single_output_scalar_transform(\n    input_col: Column, input_col_datatype: DataType, func: Callable[[Column], Column]\n) -&gt; Column:\n    \"\"\"\n    Applies a scalar Spark function (e.g. a Spark standard library function that can be\n    applied elementwise to arrays) to a single input column and returns a single output\n    column. Caters for the case, where the input column is:\n\n    1. A scalar.\n    2. A (possibly nested) array.\n\n    If the input is a scalar, apply `func` directly to the input column.\n    If the input is an array, apply `func` elementwise to the input column.\n\n    :param input_col: Input column.\n    :param input_col_datatype: Input column datatype.\n    :param func: Function to apply to the input column.\n    :returns: Output column.\n    \"\"\"\n    return _single_input_single_output_transform(\n        input_col, input_col_datatype, func, scalar=True\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/transform_utils/#src.kamae.spark.utils.transform_utils.single_input_single_output_scalar_udf_transform","title":"single_input_single_output_scalar_udf_transform","text":"<pre><code>single_input_single_output_scalar_udf_transform(\n    input_col,\n    input_col_datatype,\n    func,\n    udf_return_element_datatype,\n)\n</code></pre> <p>Applies a scalar Python function (e.g. a lambda function that we will wrap into a UDF) to a single input column and returns a single output column. Caters for the case, where the input column is:</p> <ol> <li>A scalar.</li> <li>A (possibly nested) array.</li> </ol> <p>If the input is a scalar, apply <code>func</code> directly to the input column. If the input is an array, apply <code>func</code> elementwise to the input column.</p> <p><code>func</code> should be provided as a python function, and not wrapped in the Spark UDF wrapper.</p> <p>:param input_col: Input column. :param input_col_datatype: Input column datatype. :param func: Function to apply to the input column. :param udf_return_element_datatype: Datatype of the UDF return type. Should be the raw underlying type. :returns: Output column.</p> Source code in <code>src/kamae/spark/utils/transform_utils.py</code> <pre><code>def single_input_single_output_scalar_udf_transform(\n    input_col: Column,\n    input_col_datatype: DataType,\n    func: Callable,\n    udf_return_element_datatype: DataType,\n) -&gt; Column:\n    \"\"\"\n    Applies a scalar Python function (e.g. a lambda function that we will wrap into a\n    UDF) to a single input column and returns a single output column. Caters for the\n    case, where the input column is:\n\n    1. A scalar.\n    2. A (possibly nested) array.\n\n    If the input is a scalar, apply `func` directly to the input column.\n    If the input is an array, apply `func` elementwise to the input column.\n\n    `func` should be provided as a python function, and not wrapped in the Spark UDF\n    wrapper.\n\n    :param input_col: Input column.\n    :param input_col_datatype: Input column datatype.\n    :param func: Function to apply to the input column.\n    :param udf_return_element_datatype: Datatype of the UDF return type. Should be the\n    raw underlying type.\n    :returns: Output column.\n    \"\"\"\n    return _single_input_single_output_udf_transform(\n        input_col, input_col_datatype, func, udf_return_element_datatype, scalar=True\n    )\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/user_defined_functions/","title":"user_defined_functions","text":""},{"location":"reference/src/kamae/spark/utils/user_defined_functions/#src.kamae.spark.utils.user_defined_functions.hash_udf","title":"hash_udf","text":"<pre><code>hash_udf(label, num_bins, mask_value=None)\n</code></pre> <p>User defined Spark function (UDF) to hash a string to an integer value.</p> <p>This uses FarmHash64. We have a python binding of the Google library https://github.com/google/farmhash within the pyfarmhash package.</p> <p>Therefore, this matches the hash function used in the tensorflow layer.</p> <p>:param label: String to hash. :param num_bins: Number of bins to hash to. :param mask_value: Mask value to use for hash indexing. :returns: Hashed integer value.</p> Source code in <code>src/kamae/spark/utils/user_defined_functions.py</code> <pre><code>def hash_udf(\n    label: str, num_bins: int, mask_value: Optional[str] = None\n) -&gt; Union[int, None]:\n    \"\"\"\n    User defined Spark function (UDF) to hash a string to an integer value.\n\n    This uses FarmHash64. We have a python binding of the Google library\n    https://github.com/google/farmhash within the pyfarmhash package.\n\n    Therefore, this matches the hash function used in the tensorflow layer.\n\n    :param label: String to hash.\n    :param num_bins: Number of bins to hash to.\n    :param mask_value: Mask value to use for hash indexing.\n    :returns: Hashed integer value.\n    \"\"\"\n    if label is None:\n        return None\n    if label == mask_value:\n        return 0\n\n    hash_val = safe_hash64(label)\n    if mask_value is not None:\n        # If masking value is set, then the zero index is reserved for it.\n        # Therefore, we reduce the num_bins by 1 and add 1 to the binned value.\n        return (hash_val % (num_bins - 1)) + 1\n    else:\n        return hash_val % num_bins\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/user_defined_functions/#src.kamae.spark.utils.user_defined_functions.indexer_udf","title":"indexer_udf","text":"<pre><code>indexer_udf(\n    label, labels, mask_token=None, num_oov_indices=1\n)\n</code></pre> <p>User defined Spark function (UDF) to index a label based on the labels array mask token, and number of out of vocabulary indices.</p> <p>If the label is the mask token, we return 0. If the label is not in the labels array and num_oov_indices is 0, we raise an error. Otherwise, we calculate the offset for the index based on the mask token and num_oov_indices. Lastly, if the number of out of vocabulary indices is more than 1 we hash the label to an out of vocabulary index.</p> <p>Raises an error if the provided label is hashed and contains a null character.</p> <p>:param label: Current label to index. :param labels: Array of labels to index. :param mask_token: Mask token to use for the 0 index. :param num_oov_indices: Number of out of vocabulary indices. :returns: Indexed integer value.</p> Source code in <code>src/kamae/spark/utils/user_defined_functions.py</code> <pre><code>def indexer_udf(\n    label: str,\n    labels: List[str],\n    mask_token: Optional[str] = None,\n    num_oov_indices: int = 1,\n) -&gt; int:\n    \"\"\"\n    User defined Spark function (UDF) to index a label based on the labels array\n    mask token, and number of out of vocabulary indices.\n\n    If the label is the mask token, we return 0. If the label is not in the labels\n    array and num_oov_indices is 0, we raise an error. Otherwise, we calculate the\n    offset for the index based on the mask token and num_oov_indices. Lastly, if\n    the number of out of vocabulary indices is more than 1 we hash the label to an\n    out of vocabulary index.\n\n    Raises an error if the provided label is hashed and contains a null character.\n\n    :param label: Current label to index.\n    :param labels: Array of labels to index.\n    :param mask_token: Mask token to use for the 0 index.\n    :param num_oov_indices: Number of out of vocabulary indices.\n    :returns: Indexed integer value.\n    \"\"\"\n    if label is None:\n        if num_oov_indices &gt; 0:\n            return 0\n        else:\n            raise ValueError(\n                \"\"\"Found null label but numOOVIndices is 0.\n                Consider setting numOOVIndices to 1 or more to cater for null labels.\"\"\"\n            )\n\n    if mask_token is not None and label == mask_token:\n        # If the label is the mask token,\n        # we want to return 0, since 0 is reserved for masked tokens\n        return 0\n\n    # Calculate the offset for the index\n    start_index = 1 if mask_token is not None else 0\n    offset = num_oov_indices + start_index\n\n    # Attempt to get the index.\n    try:\n        return labels.index(label) + offset\n    except ValueError:\n        if num_oov_indices == 0:\n            # If we have no out of vocabulary indices and the label is not in the\n            # labels array, raise an error.\n            raise ValueError(\n                f\"\"\"Label {label} not found in labels array and\n                numOOVIndices is 0. Consider setting numOOVIndices to 1 or more.\"\"\"\n            )\n        # If the label is not in the labels array and not equal to the mask token,\n        # then we need to hash it to an out of vocabulary index.\n        hashed_int = safe_hash64(label)\n        return (hashed_int % num_oov_indices) + start_index\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/user_defined_functions/#src.kamae.spark.utils.user_defined_functions.min_hash_udf","title":"min_hash_udf","text":"<pre><code>min_hash_udf(labels, num_permutations, mask_value=None)\n</code></pre> <p>User defined Spark function (UDF) to encode a list of strings as a min hash array.</p> <p>:param labels: List of strings to encode. :param num_permutations: Number of permutations to use. Output will be a list of size num_permutations. :param mask_value: Mask value to use for hash indexing. If set, the mask value will be ignored when computing the min hash. :returns: List of integers representing the min hash array.</p> Source code in <code>src/kamae/spark/utils/user_defined_functions.py</code> <pre><code>def min_hash_udf(\n    labels: List[str], num_permutations: int, mask_value: Optional[str] = None\n) -&gt; List[int]:\n    \"\"\"\n    User defined Spark function (UDF) to encode a list of strings as a min hash array.\n\n    :param labels: List of strings to encode.\n    :param num_permutations: Number of permutations to use. Output will be a list of\n    size num_permutations.\n    :param mask_value: Mask value to use for hash indexing. If set, the mask value\n    will be ignored when computing the min hash.\n    :returns: List of integers representing the min hash array.\n    \"\"\"\n    min_hash_array = []\n    if not labels:\n        # Ensure at least one label\n        labels.append(\"\")\n    for i in range(num_permutations):\n        # Set the number of bins to the maximum integer value. We just want to hash\n        # the input without binning it, so we use the maximum integer value.\n        # This matches the behavior of the TensorFlow layer.\n        if mask_value is not None:\n            hashed_vals = [\n                tf.int32.max\n                if label == mask_value\n                else hash_udf(label=f\"{label}{i}\", num_bins=tf.int32.max)\n                for label in labels\n            ]\n        else:\n            hashed_vals = [\n                hash_udf(label=f\"{label}{i}\", num_bins=tf.int32.max) for label in labels\n            ]\n        min_hash_val = min(hashed_vals)\n        min_hash_bit = min_hash_val &amp; 1\n        min_hash_array.append(min_hash_bit)\n\n    return min_hash_array\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/user_defined_functions/#src.kamae.spark.utils.user_defined_functions.one_hot_encoding_udf","title":"one_hot_encoding_udf","text":"<pre><code>one_hot_encoding_udf(\n    label,\n    labels,\n    mask_token=None,\n    num_oov_indices=1,\n    drop_unseen=False,\n)\n</code></pre> <p>User defined Spark function (UDF) to create a one-hot encoding of a label based on the labels array, mask token, number of out of vocabulary indices, and whether to drop unseen labels.</p> <p>:param label: Current label to index. :param labels: Array of labels to index. :param mask_token: Mask token to use for the 0 index. :param num_oov_indices: Number of out of vocabulary indices. :param drop_unseen: Whether to drop unseen label indices. :returns: List of floats representing the one-hot encoding.</p> Source code in <code>src/kamae/spark/utils/user_defined_functions.py</code> <pre><code>def one_hot_encoding_udf(\n    label: str,\n    labels: List[str],\n    mask_token: Optional[str] = None,\n    num_oov_indices: int = 1,\n    drop_unseen: bool = False,\n) -&gt; List[float]:\n    \"\"\"\n    User defined Spark function (UDF) to create a one-hot encoding of a label based\n    on the labels array, mask token, number of out of vocabulary indices, and whether\n    to drop unseen labels.\n\n    :param label: Current label to index.\n    :param labels: Array of labels to index.\n    :param mask_token: Mask token to use for the 0 index.\n    :param num_oov_indices: Number of out of vocabulary indices.\n    :param drop_unseen: Whether to drop unseen label indices.\n    :returns: List of floats representing the one-hot encoding.\n    \"\"\"\n    index = indexer_udf(\n        label=label,\n        labels=labels,\n        mask_token=mask_token,\n        num_oov_indices=num_oov_indices,\n    )\n    mask_offset = 1 if mask_token is not None else 0\n    if drop_unseen:\n        encoding = [0.0] * len(labels)\n        shifted_index = index - (num_oov_indices + mask_offset)\n        if shifted_index &gt;= 0:\n            encoding[shifted_index] = 1.0\n    else:\n        encoding = [0.0] * (len(labels) + num_oov_indices + mask_offset)\n        encoding[index] = 1.0\n    return encoding\n</code></pre>"},{"location":"reference/src/kamae/spark/utils/user_defined_functions/#src.kamae.spark.utils.user_defined_functions.ordinal_array_encode_udf","title":"ordinal_array_encode_udf","text":"<pre><code>ordinal_array_encode_udf(value, pad_value=None)\n</code></pre> <p>User defined Spark function (UDF) to encode a list of strings as an ordinal array. Example: value = ['a', 'b', 'c'] ordinal_array = [0, 1, 2]</p> <p>:param value: List of strings to encode. :param pad_value: Value to use for padding. Padded values get an index of -1. :returns: List of integers representing the ordinal array.</p> Source code in <code>src/kamae/spark/utils/user_defined_functions.py</code> <pre><code>def ordinal_array_encode_udf(\n    value: List[str], pad_value: Optional[str] = None\n) -&gt; List[int]:\n    \"\"\"\n    User defined Spark function (UDF) to encode a list of strings as an ordinal array.\n    Example:\n    value = ['a', 'b', 'c']\n    ordinal_array = [0, 1, 2]\n\n    :param value: List of strings to encode.\n    :param pad_value: Value to use for padding. Padded values get an index of -1.\n    :returns: List of integers representing the ordinal array.\n    \"\"\"\n    string_index_mapping = {pad_value: -1}\n    ordinal_array = []\n    for string in value:\n        if string not in string_index_mapping:\n            string_index_mapping[string] = len(string_index_mapping) - 1\n        ordinal_array.append(string_index_mapping[string])\n    return ordinal_array\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/","title":"tensorflow","text":""},{"location":"reference/src/kamae/tensorflow/layers/","title":"layers","text":""},{"location":"reference/src/kamae/tensorflow/layers/absolute_value/","title":"absolute_value","text":""},{"location":"reference/src/kamae/tensorflow/layers/absolute_value/#src.kamae.tensorflow.layers.absolute_value.AbsoluteValueLayer","title":"AbsoluteValueLayer","text":"<pre><code>AbsoluteValueLayer(\n    name=None, input_dtype=None, output_dtype=None, **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs the abs(x) operation on a given input tensor</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/absolute_value.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the AbsoluteValueLayer layer\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/absolute_value/#src.kamae.tensorflow.layers.absolute_value.AbsoluteValueLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/absolute_value/#src.kamae.tensorflow.layers.absolute_value.AbsoluteValueLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the abs(x) operation on a given input tensor.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param inputs: Tensor to perform the abs(x) operation on. :returns: The absolute value of the input tensor.</p> Source code in <code>src/kamae/tensorflow/layers/absolute_value.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the abs(x) operation on a given input tensor.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that the input\n    is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n\n    :param inputs: Tensor to perform the abs(x) operation on.\n    :returns: The absolute value of the input tensor.\n    \"\"\"\n    outputs = tf.math.abs(inputs)\n    return outputs\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/absolute_value/#src.kamae.tensorflow.layers.absolute_value.AbsoluteValueLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the AbsoluteValue layer. Used for saving and loading from a model.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/absolute_value.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the AbsoluteValue layer.\n    Used for saving and loading from a model.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/array_concatenate/","title":"array_concatenate","text":""},{"location":"reference/src/kamae/tensorflow/layers/array_concatenate/#src.kamae.tensorflow.layers.array_concatenate.ArrayConcatenateLayer","title":"ArrayConcatenateLayer","text":"<pre><code>ArrayConcatenateLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    axis=-1,\n    auto_broadcast=False,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs a concatenation of the input tensors.</p> <p>:param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param axis: Axis to concatenate on. Defaults to -1. :param auto_broadcast: If <code>True</code>, will broadcast the input tensors to the biggest rank before concatenating. Defaults to <code>False</code>.</p> Source code in <code>src/kamae/tensorflow/layers/array_concatenate.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    axis: int = -1,\n    auto_broadcast: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises the ArrayConcatenateLayer layer.\n\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param axis: Axis to concatenate on. Defaults to -1.\n    :param auto_broadcast: If `True`, will broadcast the input tensors to the\n    biggest rank before concatenating. Defaults to `False`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    if auto_broadcast and axis != -1:\n        raise ValueError(\"auto_broadcast is only supported for axis=-1\")\n    self.axis = axis\n    self.auto_broadcast = auto_broadcast\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/array_concatenate/#src.kamae.tensorflow.layers.array_concatenate.ArrayConcatenateLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer. Returns <code>None</code> as the compatible dtypes are not restricted.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/array_concatenate/#src.kamae.tensorflow.layers.array_concatenate.ArrayConcatenateLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Concatenates the input tensors along the specified axis. If auto_broadcast is set to True, the tensors are broadcasted to the same rank before concatenating.</p> <p>Decorated with <code>@enforce_multiple_tensor_input</code> to ensure that the input is an iterable of tensors. Raises an error if a single tensor is passed in.</p> <p>:param inputs: Iterable of tensors to concatenate. :returns: Concatenated tensor.</p> Source code in <code>src/kamae/tensorflow/layers/array_concatenate.py</code> <pre><code>@enforce_multiple_tensor_input\ndef _call(self, inputs: Iterable[Tensor], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Concatenates the input tensors along the specified axis.\n    If auto_broadcast is set to True, the tensors are broadcasted to the\n    same rank before concatenating.\n\n    Decorated with `@enforce_multiple_tensor_input` to ensure that the input\n    is an iterable of tensors. Raises an error if a single tensor is passed\n    in.\n\n    :param inputs: Iterable of tensors to concatenate.\n    :returns: Concatenated tensor.\n    \"\"\"\n    if self.auto_broadcast:\n        # Determine the maximum rank statically\n        max_rank = max([len(tensor.shape) for tensor in inputs])\n\n        # Reshape all tensors to the same rank, so to calculate later the max_shape\n        # WARNING: It assumes that order of inputs and reshaped_inputs is the same!\n        reshaped_inputs = reshape_to_equal_rank(inputs)\n\n        # Check the maximum static shape (i.e. with None being the biggest number)\n        # except the last one to concat. Here we use the static tensor.shape.\n        max_static_shape = []\n        for i in range(max_rank - 1):\n            shapes = [x.shape[i] for x in reshaped_inputs]\n            if None in shapes:\n                max_static_shape.append(None)\n            else:\n                max_static_shape.append(max(shapes))\n\n        # Determine the maximum dynamic shape for each dimension, except last one\n        # Since shapes can be dynamic (None), we need to use tf.shape\n        max_dynamic_shape = []\n        for i in range(max_rank - 1):\n            shapes = [tf.shape(x)[i] for x in reshaped_inputs]\n            max_dynamic_shape.append(tf.reduce_max(shapes))\n\n        # Broadcast tensors to the maximum dynamic shape if the static is different\n        # WARNING: It assumes that when the static shapes of two tensors are None\n        # at a given rank, the dynamic shapes are the same.\n        for idx, x in enumerate(reshaped_inputs):\n            x_static_shape = x.shape[:-1]\n            if x_static_shape != max_static_shape:\n                last_dim = x.shape[-1]\n                broadcast_shape = tf.concat([max_dynamic_shape, [last_dim]], axis=0)\n                broadcasted_x = tf.broadcast_to(x, broadcast_shape)\n                reshaped_inputs[idx] = broadcasted_x\n        inputs = reshaped_inputs\n\n    return tf.concat(inputs, axis=self.axis)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/array_concatenate/#src.kamae.tensorflow.layers.array_concatenate.ArrayConcatenateLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the VectorConcat layer. Used for saving and loading from a model.</p> <p>Specifically, adds the <code>axis</code> to the config.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/array_concatenate.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the VectorConcat layer.\n    Used for saving and loading from a model.\n\n    Specifically, adds the `axis` to the config.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"axis\": self.axis,\n            \"auto_broadcast\": self.auto_broadcast,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/array_crop/","title":"array_crop","text":""},{"location":"reference/src/kamae/tensorflow/layers/array_crop/#src.kamae.tensorflow.layers.array_crop.ArrayCropLayer","title":"ArrayCropLayer","text":"<pre><code>ArrayCropLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    array_length=128,\n    pad_value=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs a cropping of the input tensor to a certain length. If the tensor is shorter than the specified length, it is padded with specified pad value.</p> <p>TODO: Currently only supports cropping the final dimension of the tensor.</p> <p>:param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param array_length: The length to crop or pad the arrays to. Defaults to 128. :param pad_value: The value to pad the arrays with. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/array_crop.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Union[str, int, float] = None,\n    output_dtype: Union[str, int, float] = None,\n    array_length: int = 128,\n    pad_value: Union[str, int, float] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises the ArrayCropLayer.\n\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param array_length: The length to crop or pad the arrays to. Defaults to 128.\n    :param pad_value: The value to pad the arrays with. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    if array_length &lt; 1:\n        raise ValueError(\"Array length must be greater than 0.\")\n    self.array_length = array_length\n\n    if pad_value is None:\n        raise ValueError(\"Pad value must be provided and not None.\")\n    self.pad_value = pad_value\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/array_crop/#src.kamae.tensorflow.layers.array_crop.ArrayCropLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/array_crop/#src.kamae.tensorflow.layers.array_crop.ArrayCropLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Crops the tensor to specified length and pads with specified value.</p> <p>:param inputs: Tensor to split. :returns: Cropped and padded tensor</p> Source code in <code>src/kamae/tensorflow/layers/array_crop.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Crops the tensor to specified length and pads with specified value.\n\n    :param inputs: Tensor to split.\n    :returns: Cropped and padded tensor\n    \"\"\"\n    inputs_shape = tf.shape(inputs)\n\n    # Crop final dimension of tensor\n    crop_length = tf.minimum(self.array_length, inputs_shape[-1])\n    cropped = inputs[..., :crop_length]\n\n    # Pad final dim of tensor if necessary\n    padding_length = tf.maximum(self.array_length - inputs_shape[-1], 0)\n    paddings = [[0, 0]] * (inputs_shape.shape[0] - 1) + [[0, padding_length]]\n    padded = tf.pad(cropped, paddings, constant_values=self.pad_value)\n    new_shape = tf.concat(\n        [\n            tf.shape(padded)[:-1],\n            tf.expand_dims(tf.constant(self.array_length), axis=-1),\n        ],\n        axis=0,\n    )\n    return tf.reshape(padded, new_shape)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/array_crop/#src.kamae.tensorflow.layers.array_crop.ArrayCropLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the ArrayCrop layer. Used for saving and loading from a model.</p> <p>Specifically, adds the <code>array_length</code> amd `pad_value to the config.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/array_crop.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the ArrayCrop layer.\n    Used for saving and loading from a model.\n\n    Specifically, adds the `array_length` amd `pad_value to the config.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"array_length\": self.array_length, \"pad_value\": self.pad_value})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/array_split/","title":"array_split","text":""},{"location":"reference/src/kamae/tensorflow/layers/array_split/#src.kamae.tensorflow.layers.array_split.ArraySplitLayer","title":"ArraySplitLayer","text":"<pre><code>ArraySplitLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    axis=-1,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs a splitting of the input tensor into a list of tensors. Expands dimensions to ensure the output tensors are the same shape as the input.</p> <p>:param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param axis: Axis to split on. Defaults to -1.</p> Source code in <code>src/kamae/tensorflow/layers/array_split.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    axis: int = -1,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises the ArraySplitLayer layer.\n\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param axis: Axis to split on. Defaults to -1.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.axis = axis\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/array_split/#src.kamae.tensorflow.layers.array_split.ArraySplitLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/array_split/#src.kamae.tensorflow.layers.array_split.ArraySplitLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Splits the input tensor along the specified axis.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if an iterable of tensors is passed in.</p> <p>:param inputs: Tensor to split. :returns: List of split tensors.</p> Source code in <code>src/kamae/tensorflow/layers/array_split.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; List[Tensor]:\n    \"\"\"\n    Splits the input tensor along the specified axis.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that the input\n    is a single tensor. Raises an error if an iterable of tensors is passed\n    in.\n\n    :param inputs: Tensor to split.\n    :returns: List of split tensors.\n    \"\"\"\n    return [\n        tf.expand_dims(y, axis=self.axis)\n        for y in tf.unstack(inputs, axis=self.axis)\n    ]\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/array_split/#src.kamae.tensorflow.layers.array_split.ArraySplitLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the VectorSplit layer. Used for saving and loading from a model.</p> <p>Specifically, adds the <code>axis</code> to the config.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/array_split.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the VectorSplit layer.\n    Used for saving and loading from a model.\n\n    Specifically, adds the `axis` to the config.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"axis\": self.axis})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/array_subtract_minimum/","title":"array_subtract_minimum","text":""},{"location":"reference/src/kamae/tensorflow/layers/array_subtract_minimum/#src.kamae.tensorflow.layers.array_subtract_minimum.ArraySubtractMinimumLayer","title":"ArraySubtractMinimumLayer","text":"<pre><code>ArraySubtractMinimumLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    axis=-1,\n    pad_value=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>TensorFlow layer that computes the difference across an axis from the minimum non-paded element in the input tensor.</p> <p>It takes a tensor of numerical value and calculates the differences between each value and the minimum value in the tensor. The calculation preserves the pad value elements.</p> <p>The principal use case for this layer is to calculate the time difference from the first event to all events in a sequence, where the tensor is a array of timestamps.</p> <p>:param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param axis: The axis along which the differences are calculated. Defaults to -1. :param pad_value: The value to be considered as padding. Defaults to <code>None</code>. :returns: None</p> Source code in <code>src/kamae/tensorflow/layers/array_subtract_minimum.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    axis: int = -1,\n    pad_value: Optional[Union[int, float]] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises the ArraySubtractMinimum layer.\n\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param axis: The axis along which the differences are calculated.\n    Defaults to -1.\n    :param pad_value: The value to be considered as padding. Defaults to `None`.\n    :returns: None\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.axis = axis\n    self.pad_value = pad_value\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/array_subtract_minimum/#src.kamae.tensorflow.layers.array_subtract_minimum.ArraySubtractMinimumLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/array_subtract_minimum/#src.kamae.tensorflow.layers.array_subtract_minimum.ArraySubtractMinimumLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the calculation of the differences on the input tensor.</p> Example <p>input_tensor = tf.Tensor([    [19, 18, 13, 11, 10, -1, -1, -1],    [12, 2, 1, -1, -1, -1, -1, -1],    ] ) layer = ArraySubtractMinimumLayer() differences = layer(input_tensor) print(differences) Output: tf.Tensor([[    [9, 8, 3, 1, 0, -1, -1, -1],    [11, 1, 0, -1, -1, -1, -1, -1],    ]</p> <p>)</p> <p>:param inputs: The input tensor. :returns: Tensor of differences from the minimum (non-padded) timestamp.</p> Source code in <code>src/kamae/tensorflow/layers/array_subtract_minimum.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the calculation of the differences on the input tensor.\n\n    Example:\n     input_tensor = tf.Tensor([\n        [19, 18, 13, 11, 10, -1, -1, -1],\n        [12, 2, 1, -1, -1, -1, -1, -1],\n        ]\n     )\n     layer = ArraySubtractMinimumLayer()\n     differences = layer(input_tensor)\n     print(differences)\n     Output: tf.Tensor([[\n        [9, 8, 3, 1, 0, -1, -1, -1],\n        [11, 1, 0, -1, -1, -1, -1, -1],\n        ]\n    )\n\n    :param inputs: The input tensor.\n    :returns: Tensor of differences from the minimum (non-padded) timestamp.\n    \"\"\"\n    if self.pad_value is None:\n        # If pad value is not defined, then the smallest value in the tensor is\n        # considered as the first value and subtracted from all the values.\n        first_value = tf.reduce_min(inputs, axis=self.axis)\n        subtracted_val = tf.subtract(inputs, tf.expand_dims(first_value, self.axis))\n        return subtracted_val\n\n    # Otherwise, we find the smallest non padded value and subtract it from all\n    # the values. Padded values are preserved.\n    inputs, pad_tensor = self._force_cast_to_compatible_numeric_type(\n        inputs, self.pad_value\n    )\n    first_non_pad_value = tf.reduce_min(\n        tf.where(tf.equal(inputs, pad_tensor), inputs.dtype.max, inputs),\n        axis=self.axis,\n    )\n    subtracted_val = tf.subtract(\n        inputs, tf.expand_dims(first_non_pad_value, self.axis)\n    )\n    return tf.where(tf.equal(inputs, pad_tensor), inputs, subtracted_val)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/array_subtract_minimum/#src.kamae.tensorflow.layers.array_subtract_minimum.ArraySubtractMinimumLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Returns the configuration of the layer. Used for saving and loading from a model.</p> <p>:returns: Dictionary of the configuration of the layer</p> Source code in <code>src/kamae/tensorflow/layers/array_subtract_minimum.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Returns the configuration of the layer.\n    Used for saving and loading from a model.\n\n    :returns: Dictionary of the configuration of the layer\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"pad_value\": self.pad_value,\n            \"axis\": self.axis,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/base/","title":"base","text":""},{"location":"reference/src/kamae/tensorflow/layers/base/#src.kamae.tensorflow.layers.base.BaseLayer","title":"BaseLayer","text":"<pre><code>BaseLayer(\n    name=None, input_dtype=None, output_dtype=None, **kwargs\n)\n</code></pre> <p>               Bases: <code>Layer</code>, <code>ABC</code></p> <p>Abstract base layer that performs casting of inputs and outputs to specified data types. All layers should inherit from this class.</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: Input data type of the layer. If specified, inputs will be cast to this data type before any computation is performed. Defaults to <code>None</code>. :param output_dtype: Output data type of the layer. Defaults to <code>None</code>. If specified, the output will be cast to this data type before being returned.</p> Source code in <code>src/kamae/tensorflow/layers/base.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises the BaseLayer.\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: Input data type of the layer. If specified, inputs will be\n    cast to this data type before any computation is performed. Defaults to `None`.\n    :param output_dtype: Output data type of the layer. Defaults to `None`. If\n    specified, the output will be cast to this data type before being returned.\n    \"\"\"\n    super().__init__(name=name, **kwargs)\n    # We handle casting of inputs and outputs in the call method\n    # Allowing keras to also autocast causes issues in some layers that require\n    # 64 bit precision. Such as timestamp layers after the year 2038.\n    self._autocast = False\n    # Needed to ensure keras 3 does not autocast inputs to float32\n    self._convert_input_args = False\n    self._input_dtype = input_dtype\n    self._output_dtype = output_dtype\n    self.true_bool_strings = [\"true\", \"t\", \"yes\", \"y\", \"1\"]\n    self.false_bool_strings = [\"false\", \"f\", \"no\", \"n\", \"0\"]\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/base/#src.kamae.tensorflow.layers.base.BaseLayer.compatible_dtypes","title":"compatible_dtypes  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>List of compatible data types for the layer. If the computation can be performed on any data type, return None.</p> <p>:returns: List of compatible data types for the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/base/#src.kamae.tensorflow.layers.base.BaseLayer._call","title":"_call  <code>abstractmethod</code>","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>The internal call method that should be implemented by the layer.</p> <p>:param inputs: The input tensor(s) to the layer. :returns: The output tensor(s) of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/base.py</code> <pre><code>@abstractmethod\ndef _call(\n    self, inputs: Union[Tensor, List[Tensor]], **kwargs: Any\n) -&gt; Union[Tensor, List[Tensor]]:\n    \"\"\"\n    The internal call method that should be implemented by the layer.\n\n    :param inputs: The input tensor(s) to the layer.\n    :returns: The output tensor(s) of the layer.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/base/#src.kamae.tensorflow.layers.base.BaseLayer._cast","title":"_cast","text":"<pre><code>_cast(inputs, cast_dtype)\n</code></pre> <p>Casts inputs to the desired dtype.</p> <p>:param inputs: Input tensor. :param cast_dtype: Dtype to cast to. :returns: Tensor cast to the desired dtype.</p> Source code in <code>src/kamae/tensorflow/layers/base.py</code> <pre><code>def _cast(self, inputs: Tensor, cast_dtype: str) -&gt; Tensor:\n    \"\"\"\n    Casts inputs to the desired dtype.\n\n    :param inputs: Input tensor.\n    :param cast_dtype: Dtype to cast to.\n    :returns: Tensor cast to the desired dtype.\n    \"\"\"\n    if inputs.dtype.name == \"string\" or cast_dtype == \"string\":\n        # If input tensor is a string tensor, or we are casting to a string,\n        # we need to use the string_cast function.\n        return self._string_cast(inputs, cast_dtype)\n    else:\n        return self._numeric_cast(inputs, cast_dtype)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/base/#src.kamae.tensorflow.layers.base.BaseLayer._cast_input_output_tensors","title":"_cast_input_output_tensors","text":"<pre><code>_cast_input_output_tensors(tensors, ingress)\n</code></pre> <p>Casts either the input or output tensors to the given input/output dtype, if specified. Ingress is a boolean that indicates whether we are casting the input (True) or output (False) tensors.</p> <p>:param tensors: The input or output tensor(s) to the layer to be cast. :param ingress: Boolean indicating whether we are casting the input (True) or output (False) tensors. :returns: The input or output tensor(s) cast to the desired input/output_dtype.</p> Source code in <code>src/kamae/tensorflow/layers/base.py</code> <pre><code>def _cast_input_output_tensors(\n    self, tensors: Union[Tensor, List[Tensor]], ingress: bool\n) -&gt; Union[Tensor, List[Tensor]]:\n    \"\"\"\n    Casts either the input or output tensors to the given input/output dtype, if\n    specified. Ingress is a boolean that indicates whether we are casting the\n    input (True) or output (False) tensors.\n\n    :param tensors: The input or output tensor(s) to the layer to be cast.\n    :param ingress: Boolean indicating whether we are casting the input (True) or\n    output (False) tensors.\n    :returns: The input or output tensor(s) cast to the desired input/output_dtype.\n    \"\"\"\n    if ingress:\n        cast_dtype = self._input_dtype\n        if (\n            cast_dtype is not None\n            and self.compatible_dtypes is not None\n            and cast_dtype not in [dtype.name for dtype in self.compatible_dtypes]\n        ):\n            raise ValueError(\n                f\"\"\"input_dtype {cast_dtype} is not a compatible dtype for\n                this layer. Compatible dtypes are {[\n                    dtype.name for dtype in self.compatible_dtypes\n                ]}.\"\"\"\n            )\n    else:\n        cast_dtype = self._output_dtype\n\n    if cast_dtype is not None:\n        if tf.is_tensor(tensors):\n            return (\n                self._cast(tensors, cast_dtype)\n                if tensors.dtype.name != cast_dtype\n                else tensors\n            )\n        return [\n            self._cast(inp, cast_dtype) if inp.dtype.name != cast_dtype else inp\n            for inp in tensors\n        ]\n    return tensors\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/base/#src.kamae.tensorflow.layers.base.BaseLayer._check_input_dtypes_compatible","title":"_check_input_dtypes_compatible","text":"<pre><code>_check_input_dtypes_compatible(inputs)\n</code></pre> <p>Checks if the input tensors are compatible with the compatible_dtypes of the layer.</p> <p>:param inputs: The input tensor(s) to the layer. :raises ValueError: If the input tensors are not compatible with the compatible_dtypes of the layer. :returns: None</p> Source code in <code>src/kamae/tensorflow/layers/base.py</code> <pre><code>def _check_input_dtypes_compatible(self, inputs: List[Tensor]) -&gt; None:\n    \"\"\"\n    Checks if the input tensors are compatible with the compatible_dtypes of the\n    layer.\n\n    :param inputs: The input tensor(s) to the layer.\n    :raises ValueError: If the input tensors are not compatible with the\n    compatible_dtypes of the layer.\n    :returns: None\n    \"\"\"\n    for inp in inputs:\n        if (\n            self.compatible_dtypes is not None\n            and inp.dtype not in self.compatible_dtypes\n        ):\n            raise TypeError(\n                f\"\"\"Input tensor with dtype {inp.dtype.name}\n                is not a compatible dtype for this layer.\n                Compatible dtypes are {[\n                    dtype.name for dtype in self.compatible_dtypes\n                ]}.\"\"\"\n            )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/base/#src.kamae.tensorflow.layers.base.BaseLayer._float_to_string_cast","title":"_float_to_string_cast  <code>staticmethod</code>","text":"<pre><code>_float_to_string_cast(inputs)\n</code></pre> <p>Casts a float tensor to a string tensor. Ensures that the precision of the float does not impact the string representation. Specifically, we want the string to be the shortest possible representation of the float, i.e. 1.145000 -&gt; \"1.145\".</p> <p>However, we also want to ensure that the string representation of the float has a decimal point, i.e. 2.00000 -&gt; \"2.0\" and not \"2\".</p> <p>:param inputs: Input string tensor :returns: Float tensor.</p> Source code in <code>src/kamae/tensorflow/layers/base.py</code> <pre><code>@staticmethod\ndef _float_to_string_cast(inputs: Tensor) -&gt; Tensor:\n    \"\"\"\n    Casts a float tensor to a string tensor. Ensures that the precision of the float\n    does not impact the string representation. Specifically, we want the string\n    to be the shortest possible representation of the float,\n    i.e. 1.145000 -&gt; \"1.145\".\n\n    However, we also want to ensure that the string representation of the float\n    has a decimal point, i.e. 2.00000 -&gt; \"2.0\" and not \"2\".\n\n    :param inputs: Input string tensor\n    :returns: Float tensor.\n    \"\"\"\n    # This gives 1.145000 -&gt; \"1.145\" and 2.00000 -&gt; \"2\".\n    # We need to add a decimal point to the second example.\n    shortest_float_string = tf.strings.as_string(inputs, shortest=True)\n\n    # Find strings without decimal points\n    no_decimal = tf.logical_not(\n        tf.strings.regex_full_match(\n            shortest_float_string, \"-?\\d*\\.\\d*\"  # noqa W605\n        )\n    )\n    # Create decimal point constant string\n    decimal_string = tf.constant(\".0\")\n\n    # Add decimal point to string without decimal points\n    return tf.where(\n        no_decimal,\n        tf.strings.join([shortest_float_string, decimal_string]),\n        shortest_float_string,\n    )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/base/#src.kamae.tensorflow.layers.base.BaseLayer._force_cast_to_compatible_numeric_type","title":"_force_cast_to_compatible_numeric_type","text":"<pre><code>_force_cast_to_compatible_numeric_type(inputs, constant)\n</code></pre> <p>Casts an input tensor and a single constant to compatible tensors.</p> <p>If the provided input is a float, create the constant tensor as a float of the same precision. If the provided input is an integer, check if the constant is non-floating, and if so, create the constant tensor as an integer of the same precision. If the constant is floating, cast the input to a float with the same precision as its integer dtype and create the constant tensor likewise.</p> <p>:param inputs: Input numeric tensor :param constant: The constant to cast to the compatible dtype. :returns: Tuple of tensors cast to compatible types</p> Source code in <code>src/kamae/tensorflow/layers/base.py</code> <pre><code>def _force_cast_to_compatible_numeric_type(\n    self, inputs: Tensor, constant: Union[float, int]\n) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"\n    Casts an input tensor and a single constant to compatible tensors.\n\n    If the provided input is a float, create the constant tensor as a float of the\n    same precision. If the provided input is an integer, check if the constant is\n    non-floating, and if so, create the constant tensor as an integer of the same\n    precision. If the constant is floating, cast the input to a float with the same\n    precision as its integer dtype and create the constant tensor likewise.\n\n    :param inputs: Input numeric tensor\n    :param constant: The constant to cast to the compatible dtype.\n    :returns: Tuple of tensors cast to compatible types\n    \"\"\"\n    if inputs.dtype.is_floating:\n        if isinstance(constant, float):\n            return inputs, tf.constant(constant, dtype=inputs.dtype)\n        return inputs, tf.constant(float(constant), dtype=inputs.dtype)\n    if inputs.dtype.is_integer:\n        if isinstance(constant, int):\n            return inputs, tf.constant(constant, dtype=inputs.dtype)\n        if isinstance(constant, float) and constant.is_integer():\n            return inputs, tf.constant(int(constant), dtype=inputs.dtype)\n        if isinstance(constant, float):\n            precision = inputs.dtype.size * 8\n            return (\n                self._cast(inputs, f\"float{precision}\"),\n                tf.constant(constant, dtype=f\"float{precision}\"),\n            )\n    raise TypeError(\n        \"inputs must be a numeric tensor and constant must be a numeric value.\"\n    )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/base/#src.kamae.tensorflow.layers.base.BaseLayer._from_string_cast","title":"_from_string_cast","text":"<pre><code>_from_string_cast(inputs, cast_dtype)\n</code></pre> <p>Casts inputs to the desired dtype when inputs are a string tensor.</p> <p>:param inputs: String tensor :param cast_dtype: Dtype to cast to. :returns: Tensor cast to the desired dtype.</p> Source code in <code>src/kamae/tensorflow/layers/base.py</code> <pre><code>def _from_string_cast(self, inputs: Tensor, cast_dtype: str) -&gt; Tensor:\n    \"\"\"\n    Casts inputs to the desired dtype when inputs are a string tensor.\n\n    :param inputs: String tensor\n    :param cast_dtype: Dtype to cast to.\n    :returns: Tensor cast to the desired dtype.\n    \"\"\"\n    if inputs.dtype.name != \"string\":\n        raise TypeError(\"inputs is not a string Tensor.\")\n    if cast_dtype in [\"float32\", \"float64\", \"int32\", \"int64\"]:\n        # If the casting dtype is supported by tf.strings.to_number, we use that.\n        return tf.strings.to_number(inputs, out_type=cast_dtype)\n    elif tf.as_dtype(cast_dtype).is_integer:\n        # If the casting dtype is an integer, we need to cast to int64 first\n        intermediate_cast = tf.strings.to_number(inputs, out_type=\"int64\")\n        return tf.cast(intermediate_cast, cast_dtype)\n    elif tf.as_dtype(cast_dtype).is_floating:\n        # If the casting dtype is a float, we need to cast to float64 first\n        intermediate_cast = tf.strings.to_number(inputs, out_type=\"float64\")\n        return tf.cast(intermediate_cast, cast_dtype)\n    elif tf.as_dtype(cast_dtype).is_bool:\n        # If the casting dtype is a boolean, we need to use a custom function\n        # to cast the string to boolean.\n        return self._string_to_bool_cast(inputs)\n    else:\n        raise TypeError(f\"Casting string to dtype {cast_dtype} is not supported.\")\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/base/#src.kamae.tensorflow.layers.base.BaseLayer._numeric_cast","title":"_numeric_cast  <code>staticmethod</code>","text":"<pre><code>_numeric_cast(inputs, cast_dtype)\n</code></pre> <p>Casts a numeric tensor to the desired (non-string) dtype.</p> <p>:param inputs: Input numeric tensor :param cast_dtype: Dtype to cast to. :returns: Tensor cast to the desired dtype.</p> Source code in <code>src/kamae/tensorflow/layers/base.py</code> <pre><code>@staticmethod\ndef _numeric_cast(inputs: Tensor, cast_dtype: str) -&gt; Tensor:\n    \"\"\"\n    Casts a numeric tensor to the desired (non-string) dtype.\n\n    :param inputs: Input numeric tensor\n    :param cast_dtype: Dtype to cast to.\n    :returns: Tensor cast to the desired dtype.\n    \"\"\"\n    return tf.cast(inputs, cast_dtype)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/base/#src.kamae.tensorflow.layers.base.BaseLayer._string_cast","title":"_string_cast","text":"<pre><code>_string_cast(inputs, cast_dtype)\n</code></pre> <p>Casts from and to string tensors.</p> <p>Either inputs is a string tensor, and we want to cast it to the desired dtype, or inputs is not a string tensor, and we want to cast it to a string tensor.</p> <p>:param inputs: Input tensor. :param cast_dtype: Dtype to cast to. :returns: Tensor cast to the desired dtype.</p> Source code in <code>src/kamae/tensorflow/layers/base.py</code> <pre><code>def _string_cast(self, inputs: Tensor, cast_dtype: str) -&gt; Tensor:\n    \"\"\"\n    Casts from and to string tensors.\n\n    Either inputs is a string tensor, and we want to cast it to the desired dtype,\n    or inputs is not a string tensor, and we want to cast it to a string tensor.\n\n    :param inputs: Input tensor.\n    :param cast_dtype: Dtype to cast to.\n    :returns: Tensor cast to the desired dtype.\n    \"\"\"\n    if inputs.dtype.name == \"string\" and cast_dtype == \"string\":\n        return inputs\n    if cast_dtype == \"string\":\n        return self._to_string_cast(inputs)\n    return self._from_string_cast(inputs, cast_dtype)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/base/#src.kamae.tensorflow.layers.base.BaseLayer._string_to_bool_cast","title":"_string_to_bool_cast","text":"<pre><code>_string_to_bool_cast(inputs)\n</code></pre> <p>Casts a string tensor to a bool tensor.</p> <p>:param inputs: Input string tensor :returns: Bool tensor.</p> Source code in <code>src/kamae/tensorflow/layers/base.py</code> <pre><code>def _string_to_bool_cast(self, inputs: Tensor) -&gt; Tensor:\n    \"\"\"\n    Casts a string tensor to a bool tensor.\n\n    :param inputs: Input string tensor\n    :returns: Bool tensor.\n    \"\"\"\n    if inputs.dtype.name != \"string\":\n        raise TypeError(\n            f\"Expected a string tensor, but got a {inputs.dtype.name} tensor.\"\n        )\n\n    # Replace true strings with \"1\" and false strings with \"0\"\n    is_bool_true_string_tensor = [\n        tf.strings.lower(inputs) == bool_string\n        for bool_string in self.true_bool_strings\n    ]\n    is_bool_false_string_tensor = [\n        tf.strings.lower(inputs) == bool_string\n        for bool_string in self.false_bool_strings\n    ]\n\n    string_bool_tensor = tf.where(\n        reduce(tf.math.logical_or, is_bool_true_string_tensor),\n        tf.constant(\"1\"),\n        inputs,\n    )\n    string_bool_tensor = tf.where(\n        reduce(tf.math.logical_or, is_bool_false_string_tensor),\n        tf.constant(\"0\"),\n        string_bool_tensor,\n    )\n\n    # If we have other strings that are not \"1\" or \"0\", these are invalid.\n    # We insert these as \"NULL\" values so that the casting will fail.\n    string_bool_tensor_with_invalid = tf.where(\n        tf.math.logical_or(string_bool_tensor == \"1\", string_bool_tensor == \"0\"),\n        string_bool_tensor,\n        tf.constant(\"NULL\"),\n    )\n\n    bool_float_tensor = tf.strings.to_number(\n        string_bool_tensor_with_invalid, out_type=tf.float32\n    )\n    return tf.cast(bool_float_tensor, tf.bool)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/base/#src.kamae.tensorflow.layers.base.BaseLayer._to_string_cast","title":"_to_string_cast","text":"<pre><code>_to_string_cast(inputs)\n</code></pre> <p>Casts inputs to string tensor.</p> <p>:param inputs: Input tensor. :returns: String tensor.</p> Source code in <code>src/kamae/tensorflow/layers/base.py</code> <pre><code>def _to_string_cast(self, inputs: Tensor) -&gt; Tensor:\n    \"\"\"\n    Casts inputs to string tensor.\n\n    :param inputs: Input tensor.\n    :returns: String tensor.\n    \"\"\"\n    if inputs.dtype.is_floating:\n        return self._float_to_string_cast(inputs)\n    return tf.strings.as_string(inputs)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/base/#src.kamae.tensorflow.layers.base.BaseLayer.call","title":"call","text":"<pre><code>call(inputs, **kwargs)\n</code></pre> <p>Casts inputs to the given <code>input_dtype</code>, calls the internal <code>_call</code> method, and casts the outputs to the given <code>output_dtype</code>.</p> <p>:param inputs: The input tensor(s) to the layer. :returns: The output tensor(s) of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/base.py</code> <pre><code>@allow_single_or_multiple_tensor_input\ndef call(\n    self, inputs: Iterable[Tensor], **kwargs: Any\n) -&gt; Union[Tensor, List[Tensor]]:\n    \"\"\"\n    Casts inputs to the given `input_dtype`, calls the internal `_call` method, and\n    casts the outputs to the given `output_dtype`.\n\n    :param inputs: The input tensor(s) to the layer.\n    :returns: The output tensor(s) of the layer.\n    \"\"\"\n    # Cast inputs to a compatible dtype for the layer\n    casted_inputs = self.cast_input_tensors(inputs=inputs)\n    # Check if the input tensors are now compatible with the layer\n    self._check_input_dtypes_compatible(inputs=casted_inputs)\n    # Call the internal _call method\n    outputs = self._call(inputs=casted_inputs, **kwargs)\n    # Cast outputs to the desired output_dtype\n    casted_outputs = self.cast_output_tensors(outputs=outputs)\n    return casted_outputs\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/base/#src.kamae.tensorflow.layers.base.BaseLayer.cast_input_tensors","title":"cast_input_tensors","text":"<pre><code>cast_input_tensors(inputs)\n</code></pre> <p>Casts the input tensors to the given input dtype, if specified. All tensors are cast to this. This might not be ideal, there may be layers where some inputs are expected to be different types. In these cases, the subclass should implement the cast_input_tensors method.</p> <p>:param inputs: The input tensor(s) to the layer. :returns: The input tensor(s) cast to the desired input_dtype.</p> Source code in <code>src/kamae/tensorflow/layers/base.py</code> <pre><code>def cast_input_tensors(\n    self, inputs: Union[Tensor, List[Tensor]]\n) -&gt; Union[Tensor, List[Tensor]]:\n    \"\"\"\n    Casts the input tensors to the given input dtype, if specified. All tensors are\n    cast to this. This might not be ideal, there may be layers where some inputs are\n    expected to be different types. In these cases, the subclass should\n    implement the cast_input_tensors method.\n\n    :param inputs: The input tensor(s) to the layer.\n    :returns: The input tensor(s) cast to the desired input_dtype.\n    \"\"\"\n    return self._cast_input_output_tensors(tensors=inputs, ingress=True)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/base/#src.kamae.tensorflow.layers.base.BaseLayer.cast_output_tensors","title":"cast_output_tensors","text":"<pre><code>cast_output_tensors(outputs)\n</code></pre> <p>Casts the output tensors to the given output dtype, if specified. All tensors are cast to this. This might not be ideal, there may be layers where some outputs are expected to be different types. In these cases, the subclass should implement the cast_output_tensors method.</p> <p>:param outputs: The output tensor(s) of the layer. :returns: The output tensor(s) cast to the desired output_dtype.</p> Source code in <code>src/kamae/tensorflow/layers/base.py</code> <pre><code>def cast_output_tensors(\n    self, outputs: Union[Tensor, List[Tensor]]\n) -&gt; Union[Tensor, List[Tensor]]:\n    \"\"\"\n    Casts the output tensors to the given output dtype, if specified. All tensors\n    are cast to this. This might not be ideal, there may be layers where some\n    outputs are expected to be different types. In these cases, the subclass should\n    implement the cast_output_tensors method.\n\n    :param outputs: The output tensor(s) of the layer.\n    :returns: The output tensor(s) cast to the desired output_dtype.\n    \"\"\"\n    return self._cast_input_output_tensors(tensors=outputs, ingress=False)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/base/#src.kamae.tensorflow.layers.base.BaseLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the BaseLayer. Used for saving and loading from a model.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/base.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the BaseLayer.\n    Used for saving and loading from a model.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"name\": self.name,\n            \"input_dtype\": self._input_dtype,\n            \"output_dtype\": self._output_dtype,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/bearing_angle/","title":"bearing_angle","text":""},{"location":"reference/src/kamae/tensorflow/layers/bearing_angle/#src.kamae.tensorflow.layers.bearing_angle.BearingAngleLayer","title":"BearingAngleLayer","text":"<pre><code>BearingAngleLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    lat_lon_constant=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Computes the Bearing angle operation on a given input tensor. If lat_lon_constant is not set, inputs must be a list of 4 tensors, in the order of lat1, lon1, lat2, lon2. If lat_lon_constant is set, inputs must be a tensor of 2 tensors, in the order of lat1, lon1.</p> <p>We DO NOT check if the lat/lon values are out of bounds. For lat, this is [-90, 90] and for lon, this is [-180, 180].</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param lat_lon_constant: The lat/lons to use in the bearing angle calculation. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/bearing_angle.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    lat_lon_constant: Optional[List[float]] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the BearingAngleLayer layer\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param lat_lon_constant: The lat/lons to use in the bearing angle\n    calculation. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    if lat_lon_constant is not None and len(lat_lon_constant) != 2:\n        raise ValueError(\"If set, lat_lon_constant must be a list of 2 floats\")\n    self.lat_lon_constant = lat_lon_constant\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/bearing_angle/#src.kamae.tensorflow.layers.bearing_angle.BearingAngleLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/bearing_angle/#src.kamae.tensorflow.layers.bearing_angle.BearingAngleLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Computes the bearing angle between two lat/lon pairs.</p> <p>Decorated with @enforce_multiple_tensor_input to ensure that the input is an iterable of tensors. Raises an error if a single tensor is passed.</p> <p>After decoration, we check the length of the inputs to ensure we have the right number of lat/lon tensors.</p> <p>:param inputs: Iterable of tensors. :returns: Tensor of bearing angles.</p> Source code in <code>src/kamae/tensorflow/layers/bearing_angle.py</code> <pre><code>@enforce_multiple_tensor_input\ndef _call(self, inputs: Iterable[Tensor], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Computes the bearing angle between two lat/lon pairs.\n\n    Decorated with @enforce_multiple_tensor_input to ensure that the input\n    is an iterable of tensors. Raises an error if a single tensor is passed.\n\n    After decoration, we check the length of the inputs to ensure we have the right\n    number of lat/lon tensors.\n\n    :param inputs: Iterable of tensors.\n    :returns: Tensor of bearing angles.\n    \"\"\"\n    if self.lat_lon_constant is not None:\n        if not isinstance(inputs, list) or len(inputs) != 2:\n            raise ValueError(\n                \"\"\"If lat_lon_constant is set,\n            inputs must be a list of 2 tensors\"\"\"\n            )\n        return self.compute_bearing_angle(\n            inputs[0],\n            inputs[1],\n            tf.constant(self.lat_lon_constant[0]),\n            tf.constant(self.lat_lon_constant[1]),\n        )\n    else:\n        if not isinstance(inputs, list) or len(inputs) != 4:\n            raise ValueError(\n                \"\"\"If lat_lon_constant is not set,\n            inputs must be a list of 4 tensors\"\"\"\n            )\n        return self.compute_bearing_angle(\n            inputs[0],\n            inputs[1],\n            inputs[2],\n            inputs[3],\n        )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/bearing_angle/#src.kamae.tensorflow.layers.bearing_angle.BearingAngleLayer.compute_bearing_angle","title":"compute_bearing_angle","text":"<pre><code>compute_bearing_angle(lat1, lon1, lat2, lon2)\n</code></pre> <p>Computes the bearing angle between two lat/lon pairs.</p> <p>:param lat1: Tensor of latitudes of the first point. :param lon1: Tensor of longitudes of the first point. :param lat2: Tensor of latitudes of the second point. :param lon2: Tensor of longitudes of the second point. :returns: Tensor of bearing angles.</p> Source code in <code>src/kamae/tensorflow/layers/bearing_angle.py</code> <pre><code>def compute_bearing_angle(\n    self, lat1: Tensor, lon1: Tensor, lat2: Tensor, lon2: Tensor\n) -&gt; Tensor:\n    \"\"\"\n    Computes the bearing angle between two lat/lon pairs.\n\n    :param lat1: Tensor of latitudes of the first point.\n    :param lon1: Tensor of longitudes of the first point.\n    :param lat2: Tensor of latitudes of the second point.\n    :param lon2: Tensor of longitudes of the second point.\n    :returns: Tensor of bearing angles.\n    \"\"\"\n    lat1_radians = self.get_radians(lat1)\n    lon1_radians = self.get_radians(lon1)\n    lat2_radians = self.get_radians(lat2)\n    lon2_radians = self.get_radians(lon2)\n\n    lon_difference = lon2_radians - lon1_radians\n    # Bearing formula calculation\n    y = sin(lon_difference) * cos(lat2_radians)\n\n    x = cos(lat1_radians) * sin(lat2_radians)\n    x -= sin(lat1_radians) * cos(lat2_radians) * cos(lon_difference)\n\n    # Calculate bearing in degrees\n    bearing = atan2(y, x)\n    bearing_deg = mod(self.get_degrees(bearing) + 360, 360)\n    return bearing_deg\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/bearing_angle/#src.kamae.tensorflow.layers.bearing_angle.BearingAngleLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the Bearing Angle layer. Used for saving and loading from a model.</p> <p>Specifically, we add the <code>lat_lon_constant</code> and <code>unit</code> to the config.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/bearing_angle.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the Bearing Angle layer.\n    Used for saving and loading from a model.\n\n    Specifically, we add the `lat_lon_constant` and `unit` to the config.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"lat_lon_constant\": self.lat_lon_constant})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/bearing_angle/#src.kamae.tensorflow.layers.bearing_angle.BearingAngleLayer.get_degrees","title":"get_degrees  <code>staticmethod</code>","text":"<pre><code>get_degrees(radians)\n</code></pre> <p>Converts radians tensor to degrees.</p> <p>:param radians: Tensor of degrees. :returns: Tensor of degrees.</p> Source code in <code>src/kamae/tensorflow/layers/bearing_angle.py</code> <pre><code>@staticmethod\ndef get_degrees(radians: Tensor) -&gt; Tensor:\n    \"\"\"\n    Converts radians tensor to degrees.\n\n    :param radians: Tensor of degrees.\n    :returns: Tensor of degrees.\n    \"\"\"\n    return tf.cast(radians, dtype=tf.float64) * tf.constant(\n        180 / math.pi, dtype=tf.float64\n    )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/bearing_angle/#src.kamae.tensorflow.layers.bearing_angle.BearingAngleLayer.get_radians","title":"get_radians  <code>staticmethod</code>","text":"<pre><code>get_radians(degrees)\n</code></pre> <p>Converts degrees tensor to radians. We need to cast to float64 otherwise pi / 180 will lose precision.</p> <p>:param degrees: Tensor of degrees. :returns: Tensor of radians.</p> Source code in <code>src/kamae/tensorflow/layers/bearing_angle.py</code> <pre><code>@staticmethod\ndef get_radians(degrees: Tensor) -&gt; Tensor:\n    \"\"\"\n    Converts degrees tensor to radians. We need to cast to float64 otherwise\n    pi / 180 will lose precision.\n\n    :param degrees: Tensor of degrees.\n    :returns: Tensor of radians.\n    \"\"\"\n    return tf.cast(degrees, dtype=tf.float64) * tf.constant(\n        math.pi / 180, dtype=tf.float64\n    )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/bin/","title":"bin","text":""},{"location":"reference/src/kamae/tensorflow/layers/bin/#src.kamae.tensorflow.layers.bin.BinLayer","title":"BinLayer","text":"<pre><code>BinLayer(\n    condition_operators,\n    bin_values,\n    bin_labels,\n    default_label,\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs a binning operation on a given input tensor.</p> <p>The binning operation is performed by comparing the input tensor to a list of values using a list of operators. The bin label corresponding to the first condition that evaluates to True is returned.</p> <p>If no conditions evaluate to True, the default label is returned.</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param condition_operators: List of operators to use in the if statement. Can be one of:     - \"eq\": Equal to     - \"neq\": Not equal to     - \"lt\": Less than     - \"leq\": Less than or equal to     - \"gt\": Greater than     - \"geq\": Greater than or equal to :param bin_values: List of values to compare the input tensor to. Must be the same length as condition_operators. :param bin_labels: List of labels to use for each bin. Must be the same length as condition_operators. :param default_label: Label to use if none of the conditions are met.</p> Source code in <code>src/kamae/tensorflow/layers/bin.py</code> <pre><code>def __init__(\n    self,\n    condition_operators: List[str],\n    bin_values: List[float],\n    bin_labels: List[Union[float, int, str]],\n    default_label: Union[float, int, str],\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the BinLayer layer\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param condition_operators: List of operators to use in the if statement.\n    Can be one of:\n        - \"eq\": Equal to\n        - \"neq\": Not equal to\n        - \"lt\": Less than\n        - \"leq\": Less than or equal to\n        - \"gt\": Greater than\n        - \"geq\": Greater than or equal to\n    :param bin_values: List of values to compare the input tensor to. Must be the\n    same length as condition_operators.\n    :param bin_labels: List of labels to use for each bin. Must be the same length\n    as condition_operators.\n    :param default_label: Label to use if none of the conditions are met.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    if len(condition_operators) != len(bin_labels) != len(bin_values):\n        raise ValueError(\n            f\"\"\"condition_operators, bin_labels and bin_values must be the same\n            length. Got lengths: {len(condition_operators)}, {len(bin_labels)},\n            {len(bin_values)}\"\"\"\n        )\n    self.condition_operators = condition_operators\n    self.bin_values = bin_values\n    self.bin_labels = bin_labels\n    self.default_label = default_label\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/bin/#src.kamae.tensorflow.layers.bin.BinLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/bin/#src.kamae.tensorflow.layers.bin.BinLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs a binning operation on a given input tensor.</p> <p>Creates a string tensor of the same shape as the input tensor, where each element is the label of the bin that the corresponding element in the input tensor belongs to. The bin labels are determined by successively applying the condition operators to the input tensor, and returning the label of the first bin that the element belongs to.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param inputs: Tensor to perform the binning operation on. :returns: The binned input tensor.</p> Source code in <code>src/kamae/tensorflow/layers/bin.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs a binning operation on a given input tensor.\n\n    Creates a string tensor of the same shape as the input tensor, where each\n    element is the label of the bin that the corresponding element in the input\n    tensor belongs to. The bin labels are determined by successively applying\n    the condition operators to the input tensor, and returning the label of the\n    first bin that the element belongs to.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that the input\n    is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n\n    :param inputs: Tensor to perform the binning operation on.\n    :returns: The binned input tensor.\n    \"\"\"\n    cond_op_fns = [get_condition_operator(op) for op in self.condition_operators]\n\n    # Build default output tensor\n    outputs = tf.constant(self.default_label)\n\n    # Loop through the conditions.\n    # Reverse the list of conditions so that we start from the last condition\n    # and work backwards. This ensures that the first condition that is met\n    # is the one that is used.\n    conds = zip(cond_op_fns[::-1], self.bin_values[::-1], self.bin_labels[::-1])\n\n    for cond_op, value, label in conds:\n        # Ensure that the inputs and value are compatible dtypes\n        cast_input, cast_value = self._force_cast_to_compatible_numeric_type(\n            inputs, value\n        )\n        outputs = tf.where(\n            cond_op(\n                cast_input,\n                cast_value,\n            ),\n            tf.constant(label),\n            outputs,\n        )\n\n    return outputs\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/bin/#src.kamae.tensorflow.layers.bin.BinLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the Bin layer. Used for saving and loading from a model.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/bin.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the Bin layer.\n    Used for saving and loading from a model.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"condition_operators\": self.condition_operators,\n            \"bin_values\": self.bin_values,\n            \"bin_labels\": self.bin_labels,\n            \"default_label\": self.default_label,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/bloom_encode/","title":"bloom_encode","text":""},{"location":"reference/src/kamae/tensorflow/layers/bloom_encode/#src.kamae.tensorflow.layers.bloom_encode.BloomEncodeLayer","title":"BloomEncodeLayer","text":"<pre><code>BloomEncodeLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    num_hash_fns=3,\n    num_bins=None,\n    mask_value=None,\n    feature_cardinality=None,\n    use_heuristic_num_bins=False,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs a bloom encoding on the input tensor. Uses multiple hash functions to encode the input tensor, significantly reducing the dimensionality of the input and also avoiding collisions. See paper for more details. https://arxiv.org/pdf/1706.03993.pdf</p> <p>In Kamae we actually use the same hash function for all the hash functions, but we use a salt to make sure that the hash functions are different. Therefore, this can be seen as a psuedo-bloom encoding.</p> <p>:param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param num_hash_fns: Number of hash functions to use. Defaults to 3. The paper suggests a range of 2-4 hash functions for optimal performance. :param num_bins: Number of hash bins. Note that this includes the <code>mask_value</code> bin, so the effective number of bins is <code>(num_bins - 1)</code> if <code>mask_value</code> is set. If <code>use_heuristic_num_bins</code> is set to True, then this parameter is ignored and the number of bins is automatically set. See the description of this parameter below for how the heuristic is built. :param mask_value: A value that represents masked inputs, which are mapped to index 0. Defaults to None, meaning no mask term will be added and the hashing will start at index 0. :param feature_cardinality: The cardinality of the input tensor. Needed to use the heuristic to set the number of bins. Defaults to None, meaning the number of bins will not be set using the heuristic and must be set manually. :param use_heuristic_num_bins: If set to True, the number of bins is automatically set by fixing the ratio of the feature dimensionality to the number of bins to be b/f = 0.2. This ratio was found to be optimal in the paper for a wide variety of usecases. Therefore, num_bins = feature_cardinality * 0.2. This reduces the cardinality of the input tensor by 5x. Requires the <code>feature_cardinality</code> parameter to be set. Defaults to False.</p> Source code in <code>src/kamae/tensorflow/layers/bloom_encode.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    num_hash_fns: int = 3,\n    num_bins: Optional[int] = None,\n    mask_value: Union[int, str] = None,\n    feature_cardinality: Optional[int] = None,\n    use_heuristic_num_bins: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Intialises the BloomEncodeLayer layer.\n\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param num_hash_fns: Number of hash functions to use. Defaults to 3.\n    The paper suggests a range of 2-4 hash functions for optimal performance.\n    :param num_bins: Number of hash bins. Note that this includes the `mask_value`\n    bin, so the effective number of bins is `(num_bins - 1)` if `mask_value`\n    is set. If `use_heuristic_num_bins` is set to True, then this parameter is\n    ignored and the number of bins is automatically set. See the description of this\n    parameter below for how the heuristic is built.\n    :param mask_value: A value that represents masked inputs, which are mapped to\n    index 0. Defaults to None, meaning no mask term will be added and the\n    hashing will start at index 0.\n    :param feature_cardinality: The cardinality of the input tensor. Needed to use\n    the heuristic to set the number of bins. Defaults to None, meaning the number of\n    bins will not be set using the heuristic and must be set manually.\n    :param use_heuristic_num_bins: If set to True, the number of bins is\n    automatically set by fixing the ratio of the feature dimensionality to the\n    number of bins to be b/f = 0.2. This ratio was found to be optimal in the paper\n    for a wide variety of usecases. Therefore, num_bins = feature_cardinality * 0.2.\n    This reduces the cardinality of the input tensor by 5x.\n    Requires the `feature_cardinality` parameter to be set. Defaults to False.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    if num_hash_fns &lt; 2:\n        raise ValueError(\"The number of hash functions must be at least 2.\")\n    self.num_hash_fns = num_hash_fns\n    self.mask_value = mask_value\n    self.feature_cardinality = feature_cardinality\n    self.use_heuristic_num_bins = use_heuristic_num_bins\n\n    if use_heuristic_num_bins and feature_cardinality is None:\n        raise ValueError(\n            \"\"\"If use_heuristic_num_bins is set to True, then the\n            feature_cardinality parameter must be set.\"\"\"\n        )\n    if num_bins is None and not use_heuristic_num_bins:\n        raise ValueError(\n            \"\"\"If use_heuristic_num_bins is set to False, then the\n            num_bins parameter must be set.\"\"\"\n        )\n    self.num_bins = (\n        num_bins\n        if not use_heuristic_num_bins\n        else max(round(feature_cardinality * 0.2), 2)\n    )\n    # We need to create multiple hashing layers if we have a mask_value, as the\n    # mask_value needs salting in the same manner as the input tensor. Hence it is\n    # not constant across the hash functions. If the mask_value is None, then we\n    # can use the same hash function for all the hash functions.\n    if mask_value is None:\n        hash_fn = Hashing(num_bins=self.num_bins)\n        self.hash_fns = {f\"{i}\": hash_fn for i in range(self.num_hash_fns)}\n    else:\n        self.hash_fns = {\n            f\"{i}\": Hashing(\n                num_bins=self.num_bins,\n                mask_value=f\"{self.mask_value}{i}\",\n            )\n            for i in range(self.num_hash_fns)\n        }\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/bloom_encode/#src.kamae.tensorflow.layers.bloom_encode.BloomEncodeLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/bloom_encode/#src.kamae.tensorflow.layers.bloom_encode.BloomEncodeLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the bloom encoding on the input tensor.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param inputs: Input tensor to be encoded. :returns: Encoded tensor.</p> Source code in <code>src/kamae/tensorflow/layers/bloom_encode.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the bloom encoding on the input tensor.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that the input\n    is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n\n    :param inputs: Input tensor to be encoded.\n    :returns: Encoded tensor.\n    \"\"\"\n    # Expand dimensions to add the bloom encoding dimension for two scenarios:\n    # 1. If the final dimension is not 1, in which case we do not want to use\n    # this dimension for the encoding.\n    # 2. If the rank of the tensor is less than 2, then we have a single dimensional\n    # tensor thus we add a dimension for the encoding.\n    expanded_inputs = (\n        tf.expand_dims(inputs, axis=-1)\n        if inputs.shape[-1] != 1 or len(inputs.shape) &lt; 2\n        else inputs\n    )\n    # Salt the inputs to create multiple hash functions\n    # Add `i` to the input tensor, where `i` represents the ith hash function.\n    salted_inputs = [\n        tf.strings.join(\n            [expanded_inputs, tf.zeros_like(expanded_inputs)], separator=str(i)\n        )\n        for i in range(self.num_hash_fns)\n    ]\n    # Hash the salted inputs.\n    hashed_inputs = [\n        self.hash_fns[f\"{i}\"](salted_inputs[i]) for i in range(self.num_hash_fns)\n    ]\n    return tf.concat(hashed_inputs, axis=-1)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/bloom_encode/#src.kamae.tensorflow.layers.bloom_encode.BloomEncodeLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Returns the configuration of the BloomEncode layer.</p> <p>:returns: Configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/bloom_encode.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Returns the configuration of the BloomEncode layer.\n\n    :returns: Configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"num_hash_fns\": self.num_hash_fns,\n            \"num_bins\": self.num_bins,\n            \"mask_value\": self.mask_value,\n            \"feature_cardinality\": self.feature_cardinality,\n            \"use_heuristic_num_bins\": self.use_heuristic_num_bins,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/bucketize/","title":"bucketize","text":""},{"location":"reference/src/kamae/tensorflow/layers/bucketize/#src.kamae.tensorflow.layers.bucketize.BucketizeLayer","title":"BucketizeLayer","text":"<pre><code>BucketizeLayer(\n    splits,\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs a bucketing operation on the input tensor. Given a list of splits, the input tensor is bucketed into the corresponding bucket. For example, if the splits are [0, 1, 2, 3], then the input tensor is bucketed into 4 buckets: (-inf, 0), [0, 1), [1, 2), [2, 3), [3, inf). These buckets are int64 values, starting from 1. The 0 index is reserved for padding values.</p> <p>:param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param splits: The splits to use for bucketing.</p> Source code in <code>src/kamae/tensorflow/layers/bucketize.py</code> <pre><code>def __init__(\n    self,\n    splits: List[float],\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises the BucketizeLayer layer.\n\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param splits: The splits to use for bucketing.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    if splits != sorted(splits):\n        raise ValueError(\"`splits` argument must be a sorted list!\")\n    self.splits = splits\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/bucketize/#src.kamae.tensorflow.layers.bucketize.BucketizeLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/bucketize/#src.kamae.tensorflow.layers.bucketize.BucketizeLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the bucketing operation on the input tensor.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param inputs: Input tensor to bucket. :returns: Bucketed tensor.</p> Source code in <code>src/kamae/tensorflow/layers/bucketize.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the bucketing operation on the input tensor.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that\n    the input is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n\n    :param inputs: Input tensor to bucket.\n    :returns: Bucketed tensor.\n    \"\"\"\n    # We add 1 to the output of the bucket layer so that we can use\n    # 0 index as a padding value.\n    bucketed_outputs = tf.raw_ops.Bucketize(input=inputs, boundaries=self.splits)\n    return self._cast(tf.math.add(bucketed_outputs, 1), \"int64\")\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/bucketize/#src.kamae.tensorflow.layers.bucketize.BucketizeLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the Bucketizer layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>splits</code> argument to the base config.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/bucketize.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the Bucketizer layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `splits` argument to the base config.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"splits\": self.splits})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/conditional_standard_scale/","title":"conditional_standard_scale","text":""},{"location":"reference/src/kamae/tensorflow/layers/conditional_standard_scale/#src.kamae.tensorflow.layers.conditional_standard_scale.ConditionalStandardScaleLayer","title":"ConditionalStandardScaleLayer","text":"<pre><code>ConditionalStandardScaleLayer(\n    mean,\n    variance,\n    name=None,\n    axis=-1,\n    input_dtype=None,\n    output_dtype=None,\n    skip_zeros=False,\n    epsilon=0,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>NormalizeLayer</code></p> <p>Performs the standard scaling of the input with a masking condition. This layer will shift and scale inputs into a distribution centered around 0 with standard deviation 1. It accomplishes this by precomputing the mean and variance of the data, and calling <code>(input - mean) / sqrt(var)</code> at runtime. The skip_zeros parameter allows to apply the standard scaling process only when input is not equal to zero. If equal to zero, it will remain zero in the output value as it was in the input value.</p> <p>will be broadcast to the shape of the kept axes above; if the value(s) cannot be broadcast, an error will be raised when this layer's <code>build()</code> method is called. :param variance: The variance value(s) to use during normalization. The passed value(s) will be broadcast to the shape of the kept axes above; if the value(s) cannot be broadcast, an error will be raised when this layer's <code>build()</code> method is called. :param name: The name of the layer. Defaults to <code>None</code>. :param axis: Integer, tuple of integers, or None. The axis or axes that should have a separate mean and variance for each index in the shape. For example, if shape is <code>(None, 5)</code> and <code>axis=1</code>, the layer will track 5 separate mean and variance values for the last axis. If <code>axis</code> is set to <code>None</code>, the layer will normalize all elements in the input by a scalar mean and variance. Defaults to -1, where the last axis of the input is assumed to be a feature dimension and is normalized per index. Note that in the specific case of batched scalar inputs where the only axis is the batch axis, the default will normalize each index in the batch separately. In this case, consider passing <code>axis=None</code>. :param skip_zeros: If True, in addition to the masking operation, do not apply the scaling when the values to scale are equal to zero. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param epsilon: Small value to add to conditional check of zeros. Valid only when skipZeros is True. Defaults to 1e-4. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/conditional_standard_scale.py</code> <pre><code>def __init__(\n    self,\n    mean: Union[List[float], np.array],\n    variance: Union[List[float], np.array],\n    name: Optional[str] = None,\n    axis: Optional[Union[int, tuple[int]]] = -1,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    skip_zeros: bool = False,\n    epsilon: float = 0,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Intialise the ConditionalStandardScaleLayer layer.\n    :param mean: The mean value(s) to use during normalization. The passed value(s)\n    will be broadcast to the shape of the kept axes above; if the value(s)\n    cannot be broadcast, an error will be raised when this layer's\n    `build()` method is called.\n    :param variance: The variance value(s) to use during normalization. The passed\n    value(s) will be broadcast to the shape of the kept axes above; if the\n    value(s) cannot be broadcast, an error will be raised when this\n    layer's `build()` method is called.\n    :param name: The name of the layer. Defaults to `None`.\n    :param axis: Integer, tuple of integers, or None. The axis or axes that should\n    have a separate mean and variance for each index in the shape. For\n    example, if shape is `(None, 5)` and `axis=1`, the layer will track 5\n    separate mean and variance values for the last axis. If `axis` is set\n    to `None`, the layer will normalize all elements in the input by a\n    scalar mean and variance. Defaults to -1, where the last axis of the\n    input is assumed to be a feature dimension and is normalized per\n    index. Note that in the specific case of batched scalar inputs where\n    the only axis is the batch axis, the default will normalize each index\n    in the batch separately. In this case, consider passing `axis=None`.\n    :param skip_zeros: If True, in addition to the masking operation,\n    do not apply the scaling when the values to scale are equal to zero.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param epsilon: Small value to add to conditional check of zeros. Valid only\n    when skipZeros is True. Defaults to 1e-4.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name,\n        input_dtype=input_dtype,\n        output_dtype=output_dtype,\n        mean=mean,\n        variance=variance,\n        axis=axis,\n        **kwargs,\n    )\n    self.skip_zeros = skip_zeros\n    self.epsilon = epsilon\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/conditional_standard_scale/#src.kamae.tensorflow.layers.conditional_standard_scale.ConditionalStandardScaleLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs normalization on the input tensor(s) by calling the keras ConditionalStandardScaleLayer layer. It applies the scaling only to values matching the mask condition, if set. It applies the scaling only to values not equal to zero, if skip_zeros is set. Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable. :param inputs: Input tensor to perform the normalization on. :returns: The input tensor with the normalization applied.</p> Source code in <code>src/kamae/tensorflow/layers/conditional_standard_scale.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs normalization on the input tensor(s) by calling the keras\n    ConditionalStandardScaleLayer layer.\n    It applies the scaling only to values matching the mask condition, if set.\n    It applies the scaling only to values not equal to zero, if skip_zeros is set.\n    Decorated with `@enforce_single_tensor_input` to ensure that\n    the input is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n    :param inputs: Input tensor to perform the normalization on.\n    :returns: The input tensor with the normalization applied.\n    \"\"\"\n    # Ensure mean and variance match input dtype.\n    mean = self._cast(self.mean, inputs.dtype.name)\n    variance = self._cast(self.variance, inputs.dtype.name)\n    normalized_outputs = tf.math.divide_no_nan(\n        tf.math.subtract(inputs, mean),\n        tf.math.maximum(\n            tf.sqrt(variance), tf.constant(self.epsilon, dtype=inputs.dtype)\n        ),\n    )\n    # output is 0 if variance is 0\n    normalized_outputs = tf.where(\n        tf.equal(variance, 0),\n        tf.zeros_like(normalized_outputs),\n        normalized_outputs,\n    )\n    if self.skip_zeros:\n        eps = tf.constant(self.epsilon, dtype=inputs.dtype)\n        normalized_outputs = tf.where(\n            tf.abs(inputs) &lt;= eps,  # x = (0 +- eps)\n            tf.zeros_like(normalized_outputs),\n            normalized_outputs,\n        )\n    return normalized_outputs\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/conditional_standard_scale/#src.kamae.tensorflow.layers.conditional_standard_scale.ConditionalStandardScaleLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the ConditionalStandardScaleLayer layer. Used for saving and loading from a model. Specifically adds additional parameters to the base configuration. :returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/conditional_standard_scale.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the ConditionalStandardScaleLayer layer.\n    Used for saving and loading from a model.\n    Specifically adds additional parameters to the base configuration.\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"skip_zeros\": self.skip_zeros,\n            \"epsilon\": self.epsilon,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/cosine_similarity/","title":"cosine_similarity","text":""},{"location":"reference/src/kamae/tensorflow/layers/cosine_similarity/#src.kamae.tensorflow.layers.cosine_similarity.CosineSimilarityLayer","title":"CosineSimilarityLayer","text":"<pre><code>CosineSimilarityLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    axis=-1,\n    keepdims=False,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Computes the cosine similarity between two input tensors.</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param axis: The axis along which to compute the cosine similarity. Defaults to <code>-1</code>. :param keepdims: Whether to keep the shape of the input tensor. Defaults to <code>False</code>.</p> Source code in <code>src/kamae/tensorflow/layers/cosine_similarity.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    axis: int = -1,\n    keepdims: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the CosineSimilarityLayer layer\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param axis: The axis along which to compute the cosine similarity. Defaults to\n    `-1`.\n    :param keepdims: Whether to keep the shape of the input tensor. Defaults to\n    `False`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.axis = axis\n    self.keepdims = keepdims\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/cosine_similarity/#src.kamae.tensorflow.layers.cosine_similarity.CosineSimilarityLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/cosine_similarity/#src.kamae.tensorflow.layers.cosine_similarity.CosineSimilarityLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Computes the cosine similarity between two input tensors. If <code>keepdims</code> is <code>True</code>, the shape is retained. Otherwise, the shape is reduced along the specified axis.</p> <p>Decorated with @enforce_multiple_tensor_input to ensure that the input is an iterable of tensors. Raises an error if a single tensor is passed.</p> <p>After decoration, we check the length of the inputs to ensure we have the right number of input tensors.</p> <p>:param inputs: List of two tensors to compute the cosine similarity between. :returns: The tensor resulting from the cosine similarity.</p> Source code in <code>src/kamae/tensorflow/layers/cosine_similarity.py</code> <pre><code>@enforce_multiple_tensor_input\ndef _call(self, inputs: Iterable[Tensor], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Computes the cosine similarity between two input tensors. If `keepdims` is\n    `True`, the shape is retained. Otherwise, the shape is reduced along the\n    specified axis.\n\n    Decorated with @enforce_multiple_tensor_input to ensure that the input\n    is an iterable of tensors. Raises an error if a single tensor is passed.\n\n    After decoration, we check the length of the inputs to ensure we have the right\n    number of input tensors.\n\n    :param inputs: List of two tensors to compute the cosine similarity between.\n    :returns: The tensor resulting from the cosine similarity.\n    \"\"\"\n    if len(inputs) != 2:\n        raise ValueError(\n            f\"Expected 2 inputs, received {len(inputs)} inputs instead.\"\n        )\n    x = tf.nn.l2_normalize(inputs[0], axis=self.axis)\n    y = tf.nn.l2_normalize(inputs[1], axis=self.axis)\n\n    return tf.reduce_sum(tf.multiply(x, y), axis=self.axis, keepdims=self.keepdims)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/cosine_similarity/#src.kamae.tensorflow.layers.cosine_similarity.CosineSimilarityLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the CosineSimilarity layer. Used for saving and loading from a model.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/cosine_similarity.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the CosineSimilarity layer.\n    Used for saving and loading from a model.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"axis\": self.axis, \"keepdims\": self.keepdims})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/current_date/","title":"current_date","text":""},{"location":"reference/src/kamae/tensorflow/layers/current_date/#src.kamae.tensorflow.layers.current_date.CurrentDateLayer","title":"CurrentDateLayer","text":"<pre><code>CurrentDateLayer(\n    name=None, input_dtype=None, output_dtype=None, **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Returns the current UTC date in yyyy-MM-dd format.</p> <p>:param name: Name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/current_date.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises an instance of the CurrentDateLayer layer.\n\n    :param name: Name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/current_date/#src.kamae.tensorflow.layers.current_date.CurrentDateLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer. Returns <code>None</code> as the layer only returns the current date as a string. It does not transform any input.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/current_date/#src.kamae.tensorflow.layers.current_date.CurrentDateLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Returns the current timestamp in yyyy-MM-dd format. Uses the input tensor to determine the shape of the output tensor.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param inputs: Input tensor to determine the shape of the output tensor. :returns: The current timestamp tensor in yyyy-MM-dd format.</p> Source code in <code>src/kamae/tensorflow/layers/current_date.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Returns the current timestamp in yyyy-MM-dd format.\n    Uses the input tensor to determine the shape of the output tensor.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that\n    the input is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n\n    :param inputs: Input tensor to determine the shape of the output tensor.\n    :returns: The current timestamp tensor in yyyy-MM-dd format.\n    \"\"\"\n    current_timestamp = tf.fill(tf.shape(inputs), tf.timestamp())\n    outputs = unix_timestamp_to_datetime(current_timestamp, False)\n    return outputs\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/current_date/#src.kamae.tensorflow.layers.current_date.CurrentDateLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the CurrentDate layer. Used for saving and loading from a model.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/current_date.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the CurrentDate layer.\n    Used for saving and loading from a model.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/current_date_time/","title":"current_date_time","text":""},{"location":"reference/src/kamae/tensorflow/layers/current_date_time/#src.kamae.tensorflow.layers.current_date_time.CurrentDateTimeLayer","title":"CurrentDateTimeLayer","text":"<pre><code>CurrentDateTimeLayer(\n    name=None, input_dtype=None, output_dtype=None, **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Returns the current timestamp in yyyy-MM-dd HHss.SSS format.</p> <p>NOTE: Parity between this and its Spark counterpart is very difficult at the millisecond level. We have to round the TensorFlow timestamp to the 3rd decimal place for milliseconds, because  Spark already truncates to 3 decimal places. Therefore, parity is not guaranteed at this precision.</p> <p>It is recommended not to rely on parity at the millisecond level.</p> <p>:param name: Name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/current_date_time.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises an instance of the CurrentDateTimeLayer layer.\n\n    :param name: Name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/current_date_time/#src.kamae.tensorflow.layers.current_date_time.CurrentDateTimeLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer. Returns <code>None</code> as the layer only returns the current date as a string. It does not transform any input.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/current_date_time/#src.kamae.tensorflow.layers.current_date_time.CurrentDateTimeLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Returns the current timestamp in yyyy-MM-dd HHss format. Uses the input tensor to determine the shape of the output tensor.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param inputs: Input tensor to determine the shape of the output tensor. :returns: The current timestamp tensor in yyyy-MM-dd format.</p> Source code in <code>src/kamae/tensorflow/layers/current_date_time.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Returns the current timestamp in yyyy-MM-dd HH:mm:ss format.\n    Uses the input tensor to determine the shape of the output tensor.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that\n    the input is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n\n    :param inputs: Input tensor to determine the shape of the output tensor.\n    :returns: The current timestamp tensor in yyyy-MM-dd format.\n    \"\"\"\n    current_timestamp = tf.fill(tf.shape(inputs), tf.timestamp())\n    outputs = unix_timestamp_to_datetime(current_timestamp, True)\n    return outputs\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/current_date_time/#src.kamae.tensorflow.layers.current_date_time.CurrentDateTimeLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the CurrentDateTime layer. Used for saving and loading from a model.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/current_date_time.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the CurrentDateTime layer.\n    Used for saving and loading from a model.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/current_unix_timestamp/","title":"current_unix_timestamp","text":""},{"location":"reference/src/kamae/tensorflow/layers/current_unix_timestamp/#src.kamae.tensorflow.layers.current_unix_timestamp.CurrentUnixTimestampLayer","title":"CurrentUnixTimestampLayer","text":"<pre><code>CurrentUnixTimestampLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    unit=\"s\",\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Returns the current unix timestamp in either seconds or milliseconds.</p> <p>NOTE: Parity between this and its Spark counterpart is very difficult at the millisecond level. TensorFlow provides much more precision of the timestamp, and has floating 64-bit precision of the unix timestamp in seconds. Whereas Spark 3.4.0 only supports millisecond precision (3 decimal places of unix timestamp in seconds). Therefore, parity is not guaranteed at this precision.</p> <p>It is recommended not to rely on parity at the millisecond level.</p> <p>:param name: Name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/current_unix_timestamp.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    unit: str = \"s\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises an instance of the CurrentUnixTimestampLayer layer.\n\n    :param name: Name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    if unit not in [\"milliseconds\", \"seconds\", \"ms\", \"s\"]:\n        raise ValueError(\n            \"\"\"Unit must be one of [\"milliseconds\", \"seconds\", \"ms\", \"s\"]\"\"\"\n        )\n    if unit == \"milliseconds\":\n        unit = \"ms\"\n    elif unit == \"seconds\":\n        unit = \"s\"\n    self.unit = unit\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/current_unix_timestamp/#src.kamae.tensorflow.layers.current_unix_timestamp.CurrentUnixTimestampLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer. Returns <code>None</code> as the layer only returns the current date as a string. It does not transform any input.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/current_unix_timestamp/#src.kamae.tensorflow.layers.current_unix_timestamp.CurrentUnixTimestampLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Returns the current unix timestamp in either seconds or milliseconds. Uses the input tensor to determine the shape of the output tensor.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param inputs: Input tensor to determine the shape of the output tensor. :returns: The current timestamp tensor in yyyy-MM-dd format.</p> Source code in <code>src/kamae/tensorflow/layers/current_unix_timestamp.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Returns the current unix timestamp in either seconds or milliseconds.\n    Uses the input tensor to determine the shape of the output tensor.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that\n    the input is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n\n    :param inputs: Input tensor to determine the shape of the output tensor.\n    :returns: The current timestamp tensor in yyyy-MM-dd format.\n    \"\"\"\n    current_timestamp_in_seconds = tf.fill(tf.shape(inputs), tf.timestamp())\n    return (\n        current_timestamp_in_seconds\n        if self.unit == \"s\"\n        else current_timestamp_in_seconds * 1000.0\n    )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/current_unix_timestamp/#src.kamae.tensorflow.layers.current_unix_timestamp.CurrentUnixTimestampLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the CurrentUnixTimestamp layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>unit</code> parameter to the config.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/current_unix_timestamp.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the CurrentUnixTimestamp layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `unit` parameter to the config.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n\n    config.update(\n        {\n            \"unit\": self.unit,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/date_add/","title":"date_add","text":""},{"location":"reference/src/kamae/tensorflow/layers/date_add/#src.kamae.tensorflow.layers.date_add.DateAddLayer","title":"DateAddLayer","text":"<pre><code>DateAddLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    num_days=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Adds or subtracts a number of days from a date(time) string.</p> <p>WARNING: This layer destroys the time component of the date column.</p> <p>:param num_days: Number of days to add or subtract. :param name: Name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/date_add.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    num_days: Optional[int] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises an instance of the DateAddLayer.\n\n    :param num_days: Number of days to add or subtract.\n    :param name: Name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    if num_days is not None and not isinstance(num_days, int):\n        raise ValueError(\n            f\"Expected `num_days` to be an integer, but got {num_days}.\"\n        )\n    if num_days is None and input_dtype is not None:\n        raise ValueError(\n            \"\"\"When `num_days` is not set, the layer expects two inputs of different\n            dtypes. Therefore input auto-casting via `input_dtype` is not supported.\n            \"\"\"\n        )\n    self.num_days = num_days\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/date_add/#src.kamae.tensorflow.layers.date_add.DateAddLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/date_add/#src.kamae.tensorflow.layers.date_add.DateAddLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Adds or subtracts a number of days from a date(time) string.</p> Source code in <code>src/kamae/tensorflow/layers/date_add.py</code> <pre><code>@allow_single_or_multiple_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Adds or subtracts a number of days from a date(time) string.\n    \"\"\"\n    if inputs[0].dtype != tf.string:\n        raise ValueError(\n            f\"Expected input dtype to be tf.string, but got {inputs[0].dtype}.\"\n        )\n    if self.num_days is not None:\n        if len(inputs) &gt; 1:\n            raise ValueError(\n                \"When `num_days` is set, the input should be a single tensor.\"\n            )\n        return datetime_add_days(\n            inputs[0],\n            tf.constant(self.num_days, dtype=tf.float64),\n            include_time=False,\n        )\n    else:\n        if len(inputs) != 2:\n            raise ValueError(\n                \"When `num_days` is not set, the input should be two tensors.\"\n            )\n        if not inputs[1].dtype.is_integer:\n            raise ValueError(\n                f\"\"\"Expected second input dtype to be integer, but got\n                {inputs[1].dtype}.\"\"\"\n            )\n        return datetime_add_days(\n            inputs[0],\n            # Casting is necessary since all datetime ops are in float64\n            # Furthermore, due to the input dtypes being different (e.g. first input\n            # must be tf.string, second input must be integer), we cast to\n            # potentially undo the auto-casting done by specifying input_dtype.\n            self._cast(inputs[1], cast_dtype=\"float64\"),\n            include_time=False,\n        )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/date_add/#src.kamae.tensorflow.layers.date_add.DateAddLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the DateAdd layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>num_days</code> to the config dictionary.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/date_add.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the DateAdd layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `num_days` to the config dictionary.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"num_days\": self.num_days})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/date_diff/","title":"date_diff","text":""},{"location":"reference/src/kamae/tensorflow/layers/date_diff/#src.kamae.tensorflow.layers.date_diff.DateDiffLayer","title":"DateDiffLayer","text":"<pre><code>DateDiffLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    default_value=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>A preprocessing layer that returns the difference between two dates in days.</p> <p>The inputs must be in yyyy-MM-dd (HHss.SSS) format and must be passed to the layer in the order [start date , end date]. The transformer will return a negative value if the order is reversed.</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/date_diff.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    default_value: Optional[int] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the DateDiffLayer layer.\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.default_value = default_value\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/date_diff/#src.kamae.tensorflow.layers.date_diff.DateDiffLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/date_diff/#src.kamae.tensorflow.layers.date_diff.DateDiffLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the date difference operation on two input tensors.</p> <p>Decorated with <code>@enforce_multiple_tensor_input</code> to ensure that the input is an iterable. Raises an error if a single tensor is passed.</p> <p>We also then check if the length of the iterable is 2. If not, we raise an error.</p> <p>:param inputs: Iterable of two tensors to perform the date difference operation on. :returns: Single tensor with the difference between the two dates in days.</p> Source code in <code>src/kamae/tensorflow/layers/date_diff.py</code> <pre><code>@enforce_multiple_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the date difference operation on two input tensors.\n\n    Decorated with `@enforce_multiple_tensor_input` to ensure that the input\n    is an iterable. Raises an error if a single tensor is passed.\n\n    We also then check if the length of the iterable is 2.\n    If not, we raise an error.\n\n    :param inputs: Iterable of two tensors to perform the date difference operation\n    on.\n    :returns: Single tensor with the difference between the two dates in days.\n    \"\"\"\n    if len(inputs) != 2:\n        raise ValueError(\"Input shape must be an iterable of two tensors\")\n\n    start_date, end_date = inputs\n    if self.default_value is not None:\n        # Trick to replace empty strings with a valid dummy date, that we ignore\n        # later. Otherwise, the date_difference function will raise an error\n        replaced_start_date = tf.where(\n            tf.equal(start_date, \"\"), \"2000-01-01 00:00:00.000\", start_date\n        )\n        replaced_end_date = tf.where(\n            tf.equal(end_date, \"\"), \"2000-01-01 00:00:00.000\", end_date\n        )\n        outputs = tf.where(\n            tf.logical_or(tf.equal(start_date, \"\"), tf.equal(end_date, \"\")),\n            tf.constant(self.default_value, dtype=tf.int64),\n            self.date_difference(replaced_end_date, replaced_start_date),\n        )\n    else:\n        outputs = self.date_difference(end_date, start_date)\n    return outputs\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/date_diff/#src.kamae.tensorflow.layers.date_diff.DateDiffLayer.date_difference","title":"date_difference","text":"<pre><code>date_difference(end_date, start_date)\n</code></pre> <p>Calculates the difference between two dates.</p> <p>:param end_date: Tensor of end dates. :param start_date: Tensor of start dates. :returns: Tensor of date difference in days.</p> Source code in <code>src/kamae/tensorflow/layers/date_diff.py</code> <pre><code>def date_difference(self, end_date: Tensor, start_date: Tensor) -&gt; Tensor:\n    \"\"\"\n    Calculates the difference between two dates.\n\n    :param end_date: Tensor of end dates.\n    :param start_date: Tensor of start dates.\n    :returns: Tensor of date difference in days.\n    \"\"\"\n    return datetime_total_days(end_date) - datetime_total_days(start_date)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/date_diff/#src.kamae.tensorflow.layers.date_diff.DateDiffLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the DateDiff layer. Used for saving and loading from a model.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/date_diff.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the DateDiff layer.\n    Used for saving and loading from a model.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"default_value\": self.default_value})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/date_parse/","title":"date_parse","text":""},{"location":"reference/src/kamae/tensorflow/layers/date_parse/#src.kamae.tensorflow.layers.date_parse.DateParseLayer","title":"DateParseLayer","text":"<pre><code>DateParseLayer(\n    date_part,\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    default_value=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Parses a date(time) string from yyyy-MM-dd (HHss.SSS) format into a specified date part tensor.</p> <p>Date parts can be one of the following: - <code>DayOfWeek</code> - day of week (Monday = 1, Sunday = 7) - <code>DayOfMonth</code> - day of month - <code>DayOfYear</code> - day of year e.g. (2021-01-01 = 1, 2021-12-31 = 365) - <code>MonthOfYear</code> - month of year - <code>Year</code> - year - <code>Hour</code> - hour e.g. (2021-01-01 00:00:00 = 0, 2021-01-01 23:59:59 = 23) - <code>Minute</code> - minute e.g. (2021-01-01 00:00:00 = 0, 2021-01-01 00:59:00 = 59) - <code>Second</code> - second e.g. (2021-01-01 00:00:00 = 0, 2021-01-01 00:00:59 = 59) - <code>Millisecond</code> - millisecond (2021-01-01 00:00:00.357 = 357)</p> <p>In the case a timestamp is not provided, all hour, minutes, seconds and milliseconds fields will be returned as 0.</p> <p>All date parts except seconds and milliseconds are returned as int32, but due to the precision of seconds and milliseconds, these are returned as int64 to prevent overflow.</p> <p>WARNING: Dates are not checked for validity, so if you pass in a date such as \"2020-02-30\" no errors will be thrown and you will get a nonsense output.</p> <p>:param date_part: Date part to extract from date. :param name: Name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param default_value: Default value to use when the date is the empty string. Empty strings can be used when the date is not available. :returns: None - class instantiated.</p> Source code in <code>src/kamae/tensorflow/layers/date_parse.py</code> <pre><code>def __init__(\n    self,\n    date_part: str,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    default_value: Optional[int] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises an instance of the DateParseLayer layer.\n\n    :param date_part: Date part to extract from date.\n    :param name: Name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param default_value: Default value to use when the date is the empty string.\n    Empty strings can be used when the date is not available.\n    :returns: None - class instantiated.\n    \"\"\"\n    self.allowed_date_parts = {\n        \"DayOfWeek\",\n        \"DayOfMonth\",\n        \"DayOfYear\",\n        \"MonthOfYear\",\n        \"Year\",\n        \"Hour\",\n        \"Minute\",\n        \"Second\",\n        \"Millisecond\",\n    }\n    if date_part not in self.allowed_date_parts:\n        raise ValueError(f\"date_part must be one of {self.allowed_date_parts}\")\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.date_part = date_part\n    self.default_value = default_value\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/date_parse/#src.kamae.tensorflow.layers.date_parse.DateParseLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/date_parse/#src.kamae.tensorflow.layers.date_parse.DateParseLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Extracts date part from date(time) string.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that only a single tensor is passed in. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param inputs: Tensor of date(time) strings in the yyyy-MM-dd (HHss.SSS) format. :returns: Date part tensor.</p> <p>WARNING: Dates are not checked for validity, so if you pass in a date such as \"2020-02-30\" no errors will be thrown and you will get a nonsense output.</p> Source code in <code>src/kamae/tensorflow/layers/date_parse.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Extracts date part from date(time) string.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that only a single\n    tensor is passed in. Raises an error if multiple tensors are passed\n    in as an iterable.\n\n    :param inputs: Tensor of date(time) strings in the yyyy-MM-dd (HH:mm:ss.SSS)\n    format.\n    :returns: Date part tensor.\n\n    WARNING: Dates are not checked for validity, so if you pass in a date such\n    as \"2020-02-30\" no errors will be thrown and you will get a nonsense output.\n    \"\"\"\n    if self.default_value is not None:\n        # Trick to replace empty strings with a valid dummy date, that we ignore\n        # later. Otherwise, the parse_date function will raise an error\n        replaced_date = tf.where(\n            tf.equal(inputs, \"\"), \"2000-01-01 00:00:00.000\", inputs\n        )\n        outputs = tf.where(\n            tf.equal(inputs, \"\"),\n            tf.constant(self.default_value, dtype=tf.int64),\n            self._parse_date(replaced_date, self.date_part),\n        )\n    else:\n        outputs = self._parse_date(inputs, self.date_part)\n    return outputs\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/date_parse/#src.kamae.tensorflow.layers.date_parse.DateParseLayer._parse_date","title":"_parse_date  <code>staticmethod</code>","text":"<pre><code>_parse_date(date_tensor, date_part)\n</code></pre> <p>Parse date(time) string into a dictionary of date part tensors.</p> <p>:param date_tensor: Tensor of date(time) strings in the YYYY-mm-dd (HH:MM:ss.SSS) format. :returns: Dictionary of date part tensors.</p> Source code in <code>src/kamae/tensorflow/layers/date_parse.py</code> <pre><code>@staticmethod\ndef _parse_date(date_tensor: Tensor, date_part: str) -&gt; Tensor:\n    \"\"\"\n    Parse date(time) string into a dictionary of date part tensors.\n\n    :param date_tensor: Tensor of date(time) strings in the\n    YYYY-mm-dd (HH:MM:ss.SSS) format.\n    :returns: Dictionary of date part tensors.\n    \"\"\"\n\n    date_part_functions = {\n        \"DayOfWeek\": datetime_weekday,\n        \"DayOfMonth\": datetime_day,\n        \"DayOfYear\": datetime_day_of_year,\n        \"MonthOfYear\": datetime_month,\n        \"Year\": datetime_year,\n        \"Hour\": datetime_hour,\n        \"Minute\": datetime_minute,\n        \"Second\": datetime_second,\n        \"Millisecond\": datetime_millisecond,\n    }\n\n    try:\n        return date_part_functions[date_part](date_tensor)\n    except KeyError:\n        raise ValueError(\n            f\"\"\"date_part must be one of {list(date_part_functions.keys())}\"\"\"\n        )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/date_parse/#src.kamae.tensorflow.layers.date_parse.DateParseLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the DateParse layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>date_part</code> to the config dictionary.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/date_parse.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the DateParse layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `date_part` to the config dictionary.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\"date_part\": self.date_part, \"default_value\": self.default_value}\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/date_time_to_unix_timestamp/","title":"date_time_to_unix_timestamp","text":""},{"location":"reference/src/kamae/tensorflow/layers/date_time_to_unix_timestamp/#src.kamae.tensorflow.layers.date_time_to_unix_timestamp.DateTimeToUnixTimestampLayer","title":"DateTimeToUnixTimestampLayer","text":"<pre><code>DateTimeToUnixTimestampLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    unit=\"s\",\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Returns the unix timestamp from a datetime in either yyyy-MM-dd HHss.SSS or yyyy-MM-dd format.</p> <p>:param name: Name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param unit: Unit of the timestamp. Can be <code>milliseconds</code> (or <code>ms</code>) or <code>seconds</code> (or <code>s</code>). Defaults to <code>s</code>.</p> Source code in <code>src/kamae/tensorflow/layers/date_time_to_unix_timestamp.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    unit: str = \"s\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises an instance of the DateTimeToUnixTimstamp layer.\n\n    :param name: Name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param unit: Unit of the timestamp. Can be `milliseconds` (or `ms`)\n    or `seconds` (or `s`). Defaults to `s`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    if unit not in [\"milliseconds\", \"seconds\", \"ms\", \"s\"]:\n        raise ValueError(\n            \"\"\"Unit must be one of [\"milliseconds\", \"seconds\", \"ms\", \"s\"]\"\"\"\n        )\n    if unit == \"milliseconds\":\n        unit = \"ms\"\n    if unit == \"seconds\":\n        unit = \"s\"\n    self.unit = unit\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/date_time_to_unix_timestamp/#src.kamae.tensorflow.layers.date_time_to_unix_timestamp.DateTimeToUnixTimestampLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/date_time_to_unix_timestamp/#src.kamae.tensorflow.layers.date_time_to_unix_timestamp.DateTimeToUnixTimestampLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Returns the unix timestamp from a datetime in either yyyy-MM-dd HHss.SSS or yyyy-MM-dd format.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param inputs: Input tensor to determine the shape of the output tensor. :returns: Unix timestamp in either milliseconds or seconds.</p> Source code in <code>src/kamae/tensorflow/layers/date_time_to_unix_timestamp.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Returns the unix timestamp from a datetime in either yyyy-MM-dd HH:mm:ss.SSS\n    or yyyy-MM-dd format.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that\n    the input is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n\n    :param inputs: Input tensor to determine the shape of the output tensor.\n    :returns: Unix timestamp in either milliseconds or seconds.\n    \"\"\"\n    # Timestamp needs to be in float64 for unix_timestamp_to_datetime\n    unix_timestamp_in_seconds = datetime_to_unix_timestamp(inputs)\n    return (\n        unix_timestamp_in_seconds\n        if self.unit == \"s\"\n        else unix_timestamp_in_seconds * 1000.0\n    )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/date_time_to_unix_timestamp/#src.kamae.tensorflow.layers.date_time_to_unix_timestamp.DateTimeToUnixTimestampLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the DateTimeToUnixTimstamp layer. Used for saving and loading from a model.</p> <p>Specifically sets the <code>unit</code> parameters in the config.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/date_time_to_unix_timestamp.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the DateTimeToUnixTimstamp layer.\n    Used for saving and loading from a model.\n\n    Specifically sets the `unit` parameters in the config.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"unit\": self.unit,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/divide/","title":"divide","text":""},{"location":"reference/src/kamae/tensorflow/layers/divide/#src.kamae.tensorflow.layers.divide.DivideLayer","title":"DivideLayer","text":"<pre><code>DivideLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    divisor=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs the divide(x, y) operation on a given input tensor. If divisor is not set, inputs must be a list. If divisor is set, inputs must be a tensor.</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param divisor: The divisor to divide the input by, defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/divide.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    divisor: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the DivideLayer layer\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param divisor: The divisor to divide the input by, defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.divisor = divisor\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/divide/#src.kamae.tensorflow.layers.divide.DivideLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/divide/#src.kamae.tensorflow.layers.divide.DivideLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the divide(x, y) operation on either an iterable of input tensors or a single input tensor and a constant.</p> <p>Decorated with <code>@allow_single_or_multiple_tensor_input</code> to ensure that the input is either a single tensor or an iterable of tensors. Returns this result as a list of tensors for easier use here.</p> <p>:param inputs: Single tensor or iterable of tensors to perform the divide(x, y) operation on. :returns: The tensor resulting from the divide(x, y) operation.</p> Source code in <code>src/kamae/tensorflow/layers/divide.py</code> <pre><code>@allow_single_or_multiple_tensor_input\ndef _call(self, inputs: Union[Tensor, Iterable[Tensor]], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the divide(x, y) operation on either an iterable of input tensors or\n    a single input tensor and a constant.\n\n    Decorated with `@allow_single_or_multiple_tensor_input` to ensure that the input\n    is either a single tensor or an iterable of tensors. Returns this result as a\n    list of tensors for easier use here.\n\n    :param inputs: Single tensor or iterable of tensors to perform the\n    divide(x, y) operation on.\n    :returns: The tensor resulting from the divide(x, y) operation.\n    \"\"\"\n    if self.divisor is not None:\n        if len(inputs) &gt; 1:\n            raise ValueError(\"If divisor is set, cannot have multiple inputs\")\n        return tf.math.divide_no_nan(\n            inputs[0], tf.constant(self.divisor, dtype=inputs[0].dtype)\n        )\n    else:\n        if not len(inputs) &gt; 1:\n            raise ValueError(\"If divisor is not set, must have multiple inputs\")\n        return reduce(tf.math.divide_no_nan, inputs)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/divide/#src.kamae.tensorflow.layers.divide.DivideLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the Divide layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>divisor</code> to the config dictionary.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/divide.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the Divide layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `divisor` to the config dictionary.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"divisor\": self.divisor})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/exp/","title":"exp","text":""},{"location":"reference/src/kamae/tensorflow/layers/exp/#src.kamae.tensorflow.layers.exp.ExpLayer","title":"ExpLayer","text":"<pre><code>ExpLayer(\n    name=None, input_dtype=None, output_dtype=None, **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs the exp(x) operation on a given input tensor</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/exp.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the exp layer\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/exp/#src.kamae.tensorflow.layers.exp.ExpLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/exp/#src.kamae.tensorflow.layers.exp.ExpLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the exp(x) operation on a given input tensor.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param inputs: Tensor to perform the exp(x) operation on. :returns: The exp of the input tensor.</p> Source code in <code>src/kamae/tensorflow/layers/exp.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the exp(x) operation on a given input tensor.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that the input\n    is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n\n    :param inputs: Tensor to perform the exp(x) operation on.\n    :returns: The exp of the input tensor.\n    \"\"\"\n    return tf.math.exp(inputs)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/exp/#src.kamae.tensorflow.layers.exp.ExpLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the exp layer. Used for saving and loading from a model.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/exp.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the exp layer.\n    Used for saving and loading from a model.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/exponent/","title":"exponent","text":""},{"location":"reference/src/kamae/tensorflow/layers/exponent/#src.kamae.tensorflow.layers.exponent.ExponentLayer","title":"ExponentLayer","text":"<pre><code>ExponentLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    exponent=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs the x^exponent operation on a given input tensor</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param exponent: The exponent to raise the input to, defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/exponent.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    exponent: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the exponent layer\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param exponent: The exponent to raise the input to, defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.exponent = exponent\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/exponent/#src.kamae.tensorflow.layers.exponent.ExponentLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/exponent/#src.kamae.tensorflow.layers.exponent.ExponentLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the x^exponent operation on a given input tensor.</p> <p>Decorated with <code>@allow_single_or_multiple_tensor_input</code> to ensure that the input is either a single tensor or an iterable of tensors. Returns this result as a list of tensors for easier use here.</p> <p>:param inputs: Single tensor or iterable of tensors to perform the x^pow  operation on. :returns: The tensor raised to the power of the exponent.</p> Source code in <code>src/kamae/tensorflow/layers/exponent.py</code> <pre><code>@allow_single_or_multiple_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the x^exponent operation on a given input tensor.\n\n    Decorated with `@allow_single_or_multiple_tensor_input` to ensure that the input\n    is either a single tensor or an iterable of tensors. Returns this result as a\n    list of tensors for easier use here.\n\n    :param inputs: Single tensor or iterable of tensors to perform the x^pow\n     operation on.\n    :returns: The tensor raised to the power of the exponent.\n    \"\"\"\n    if self.exponent is not None:\n        if len(inputs) &gt; 1:\n            raise ValueError(\"If exponent is set, cannot have multiple inputs\")\n        return tf.math.pow(\n            inputs[0],\n            self._cast(tf.constant(self.exponent), cast_dtype=inputs[0].dtype.name),\n        )\n    else:\n        if not len(inputs) == 2:\n            raise ValueError(\"If exponent is not set, must have exactly 2 inputs\")\n        return tf.math.pow(inputs[0], inputs[1])\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/exponent/#src.kamae.tensorflow.layers.exponent.ExponentLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the exp layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>exponent</code> to the config dictionary</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/exponent.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the exp layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `exponent` to the config dictionary\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"exponent\": self.exponent})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/hash_index/","title":"hash_index","text":""},{"location":"reference/src/kamae/tensorflow/layers/hash_index/#src.kamae.tensorflow.layers.hash_index.HashIndexLayer","title":"HashIndexLayer","text":"<pre><code>HashIndexLayer(\n    num_bins,\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    mask_value=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Wrapper around the Keras Hashing layer which hashes and bins categorical features.</p> <p>This layer transforms categorical inputs to hashed output. It element-wise converts ints or strings to ints in a fixed range. The stable hash function uses <code>tensorflow::ops::Fingerprint</code> to produce the same output consistently across all platforms.</p> <p>This layer uses FarmHash64, which provides a consistent hashed output across different platforms and is stable across invocations, regardless of device and context, by mixing the input bits thoroughly.</p> <p>:param num_bins: Number of hash bins. Note that this includes the <code>mask_value</code> bin, so the effective number of bins is <code>(num_bins - 1)</code> if <code>mask_value</code> is set. :param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param mask_value: A value that represents masked inputs, which are mapped to index 0. Defaults to None, meaning no mask term will be added and the hashing will start at index 0.</p> Source code in <code>src/kamae/tensorflow/layers/hash_index.py</code> <pre><code>def __init__(\n    self,\n    num_bins: int,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    mask_value: Optional[Union[int, str]] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Intialise the HashIndexLayer layer.\n\n    :param num_bins: Number of hash bins. Note that this includes the `mask_value`\n    bin, so the effective number of bins is `(num_bins - 1)` if `mask_value`\n    is set.\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param mask_value: A value that represents masked inputs, which are mapped to\n    index 0. Defaults to None, meaning no mask term will be added and the\n    hashing will start at index 0.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.num_bins = num_bins\n    self.mask_value = mask_value\n    self.hash_indexer = Hashing(name=name, num_bins=num_bins, mask_value=mask_value)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/hash_index/#src.kamae.tensorflow.layers.hash_index.HashIndexLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/hash_index/#src.kamae.tensorflow.layers.hash_index.HashIndexLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the hash indexing on the input tensor by calling the underlying Hashing layer.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param inputs: Input tensor to be hashed. :returns: Hashed and bucketed tensor.</p> Source code in <code>src/kamae/tensorflow/layers/hash_index.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the hash indexing on the input tensor by calling the underlying\n    Hashing layer.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that the input\n    is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n\n    :param inputs: Input tensor to be hashed.\n    :returns: Hashed and bucketed tensor.\n    \"\"\"\n    return self.hash_indexer(inputs)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/hash_index/#src.kamae.tensorflow.layers.hash_index.HashIndexLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Returns the configuration of the HashIndexLayer layer.</p> <p>:returns: Configuration of the HashIndexLayer layer.</p> Source code in <code>src/kamae/tensorflow/layers/hash_index.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Returns the configuration of the HashIndexLayer layer.\n\n    :returns: Configuration of the HashIndexLayer layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"num_bins\": self.num_bins, \"mask_value\": self.mask_value})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/haversine_distance/","title":"haversine_distance","text":""},{"location":"reference/src/kamae/tensorflow/layers/haversine_distance/#src.kamae.tensorflow.layers.haversine_distance.HaversineDistanceLayer","title":"HaversineDistanceLayer","text":"<pre><code>HaversineDistanceLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    lat_lon_constant=None,\n    unit=\"km\",\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Computes the haversine distance operation on a given input tensor. If lat_lon_constant is not set, inputs must be a list of 4 tensors, in the order of lat1, lon1, lat2, lon2. If lat_lon_constant is set, inputs must be a tensor of 2 tensors, in the order of lat1, lon1.</p> <p>We DO NOT check if the lat/lon values are out of bounds. For lat, this is [-90, 90] and for lon, this is [-180, 180].</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param lat_lon_constant: The lat/lons to use in the haversine distance. :param unit: The unit of the distance. Must be either 'km' or 'miles'. calculation. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/haversine_distance.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    lat_lon_constant: Optional[List[float]] = None,\n    unit: str = \"km\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the HaversineDistanceLayer layer\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param lat_lon_constant: The lat/lons to use in the haversine distance.\n    :param unit: The unit of the distance. Must be either 'km' or 'miles'.\n    calculation. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    if lat_lon_constant is not None and len(lat_lon_constant) != 2:\n        raise ValueError(\"If set, lat_lon_constant must be a list of 2 floats\")\n    self.lat_lon_constant = lat_lon_constant\n    if unit not in [\"km\", \"miles\"]:\n        raise ValueError(\"unit must be either 'km' or 'miles'\")\n    self.unit = unit\n    self.earth_radius = 6371.0 if unit == \"km\" else 3958.8\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/haversine_distance/#src.kamae.tensorflow.layers.haversine_distance.HaversineDistanceLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/haversine_distance/#src.kamae.tensorflow.layers.haversine_distance.HaversineDistanceLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Computes the haversine distance between two lat/lon pairs.</p> <p>Decorated with @enforce_multiple_tensor_input to ensure that the input is an iterable of tensors. Raises an error if a single tensor is passed.</p> <p>After decoration, we check the length of the inputs to ensure we have the right number of lat/lon tensors.</p> <p>:param inputs: Iterable of tensors. :returns: Tensor of haversine distances.</p> Source code in <code>src/kamae/tensorflow/layers/haversine_distance.py</code> <pre><code>@enforce_multiple_tensor_input\ndef _call(self, inputs: Iterable[Tensor], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Computes the haversine distance between two lat/lon pairs.\n\n    Decorated with @enforce_multiple_tensor_input to ensure that the input\n    is an iterable of tensors. Raises an error if a single tensor is passed.\n\n    After decoration, we check the length of the inputs to ensure we have the right\n    number of lat/lon tensors.\n\n    :param inputs: Iterable of tensors.\n    :returns: Tensor of haversine distances.\n    \"\"\"\n    if self.lat_lon_constant is not None:\n        if not isinstance(inputs, list) or len(inputs) != 2:\n            raise ValueError(\n                \"\"\"If lat_lon_constant is set,\n            inputs must be a list of 2 tensors\"\"\"\n            )\n        return self.compute_haversine_distance(\n            inputs[0],\n            inputs[1],\n            tf.constant(self.lat_lon_constant[0]),\n            tf.constant(self.lat_lon_constant[1]),\n        )\n    else:\n        if not isinstance(inputs, list) or len(inputs) != 4:\n            raise ValueError(\n                \"\"\"If lat_lon_constant is not set,\n            inputs must be a list of 4 tensors\"\"\"\n            )\n        return self.compute_haversine_distance(\n            inputs[0],\n            inputs[1],\n            inputs[2],\n            inputs[3],\n        )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/haversine_distance/#src.kamae.tensorflow.layers.haversine_distance.HaversineDistanceLayer.compute_haversine_distance","title":"compute_haversine_distance","text":"<pre><code>compute_haversine_distance(lat1, lon1, lat2, lon2)\n</code></pre> <p>Computes the haversine distance between two lat/lon pairs.</p> <p>:param lat1: Tensor of latitudes of the first point. :param lon1: Tensor of longitudes of the first point. :param lat2: Tensor of latitudes of the second point. :param lon2: Tensor of longitudes of the second point. :returns: Tensor of haversine distances.</p> Source code in <code>src/kamae/tensorflow/layers/haversine_distance.py</code> <pre><code>def compute_haversine_distance(\n    self, lat1: Tensor, lon1: Tensor, lat2: Tensor, lon2: Tensor\n) -&gt; Tensor:\n    \"\"\"\n    Computes the haversine distance between two lat/lon pairs.\n\n    :param lat1: Tensor of latitudes of the first point.\n    :param lon1: Tensor of longitudes of the first point.\n    :param lat2: Tensor of latitudes of the second point.\n    :param lon2: Tensor of longitudes of the second point.\n    :returns: Tensor of haversine distances.\n    \"\"\"\n    lat1_radians = self.get_radians(lat1)\n    lon1_radians = self.get_radians(lon1)\n    lat2_radians = self.get_radians(lat2)\n    lon2_radians = self.get_radians(lon2)\n\n    lat_diff = lat2_radians - lat1_radians\n    lon_diff = lon2_radians - lon1_radians\n\n    a = tf.math.pow(tf.math.sin(lat_diff / 2.0), 2.0) + tf.math.cos(\n        lat1_radians\n    ) * tf.math.cos(lat2_radians) * tf.math.pow(tf.math.sin(lon_diff / 2.0), 2.0)\n    c = 2.0 * tf.math.asin(pow(a, 0.5))\n    # Radius of earth in kilometers.\n    r = tf.constant(self.earth_radius, dtype=c.dtype)\n    return c * r\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/haversine_distance/#src.kamae.tensorflow.layers.haversine_distance.HaversineDistanceLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the HaversineDistance layer. Used for saving and loading from a model.</p> <p>Specifically, we add the <code>lat_lon_constant</code> and <code>unit</code> to the config.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/haversine_distance.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the HaversineDistance layer.\n    Used for saving and loading from a model.\n\n    Specifically, we add the `lat_lon_constant` and `unit` to the config.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"lat_lon_constant\": self.lat_lon_constant, \"unit\": self.unit})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/haversine_distance/#src.kamae.tensorflow.layers.haversine_distance.HaversineDistanceLayer.get_radians","title":"get_radians  <code>staticmethod</code>","text":"<pre><code>get_radians(degrees)\n</code></pre> <p>Converts degrees tensor to radians. We need to cast to float64 otherwise pi / 180 will lose precision.</p> <p>:param degrees: Tensor of degrees. :returns: Tensor of radians.</p> Source code in <code>src/kamae/tensorflow/layers/haversine_distance.py</code> <pre><code>@staticmethod\ndef get_radians(degrees: Tensor) -&gt; Tensor:\n    \"\"\"\n    Converts degrees tensor to radians. We need to cast to float64 otherwise\n    pi / 180 will lose precision.\n\n    :param degrees: Tensor of degrees.\n    :returns: Tensor of radians.\n    \"\"\"\n    return tf.cast(degrees, dtype=tf.float64) * tf.constant(\n        math.pi / 180, dtype=tf.float64\n    )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/identity/","title":"identity","text":""},{"location":"reference/src/kamae/tensorflow/layers/identity/#src.kamae.tensorflow.layers.identity.IdentityLayer","title":"IdentityLayer","text":"<pre><code>IdentityLayer(\n    name=None, input_dtype=None, output_dtype=None, **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs an identity transform on the input tensor.</p> <p>:param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/identity.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises the IdentityLayer layer.\n\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/identity/#src.kamae.tensorflow.layers.identity.IdentityLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/identity/#src.kamae.tensorflow.layers.identity.IdentityLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs an identity transform on the input tensor.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param inputs: Tensor to be apply the identity transform to. :returns: The input tensor.</p> Source code in <code>src/kamae/tensorflow/layers/identity.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs an identity transform on the input tensor.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that\n    the input is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n\n    :param inputs: Tensor to be apply the identity transform to.\n    :returns: The input tensor.\n    \"\"\"\n    return tf.identity(inputs)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/identity/#src.kamae.tensorflow.layers.identity.IdentityLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the Identity layer. Used for saving and loading from a model.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/identity.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the Identity layer.\n    Used for saving and loading from a model.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/if_statement/","title":"if_statement","text":""},{"location":"reference/src/kamae/tensorflow/layers/if_statement/#src.kamae.tensorflow.layers.if_statement.IfStatementLayer","title":"IfStatementLayer","text":"<pre><code>IfStatementLayer(\n    condition_operator,\n    value_to_compare=None,\n    result_if_true=None,\n    result_if_false=None,\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs an if statement on the input tensor, returning a tensor of the same shape as the input tensor.</p> <p>The condition operator can be one of the following: - \"eq\": Equal to - \"neq\": Not equal to - \"lt\": Less than - \"le\": Less than or equal to - \"gt\": Greater than - \"ge\": Greater than or equal to</p> <p>If the condition is true, the result is the result_if_true value. If the condition is false, the result is the result_if_false value.</p> <p>If any of [value_to_compare, result_if_true, result_if_false] are None, we assume they are passed in as inputs to the layer in the above order. If all of them are not None, then inputs is expected to be a tensor.</p> <p>:param condition_operator: Operator to use in the if statement. Can be one of:     - \"eq\": Equal to     - \"neq\": Not equal to     - \"lt\": Less than     - \"leq\": Less than or equal to     - \"gt\": Greater than     - \"geq\": Greater than or equal to :param value_to_compare: Value to compare the input tensor to. If None, we assume it is passed in as an input to the layer. :param result_if_true: Value to return if the condition is true. If None, we assume it is passed in as an input to the layer. :param result_if_false: Value to return if the condition is false. If None, we assume it is passed in as an input to the layer. :param name: The name of the layer. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/if_statement.py</code> <pre><code>def __init__(\n    self,\n    condition_operator: str,\n    value_to_compare: Union[float, int, str, bool] = None,\n    result_if_true: Union[float, int, str, bool] = None,\n    result_if_false: Union[float, int, str, bool] = None,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises the IfStatementLayer layer.\n\n    :param condition_operator: Operator to use in the if statement. Can be one of:\n        - \"eq\": Equal to\n        - \"neq\": Not equal to\n        - \"lt\": Less than\n        - \"leq\": Less than or equal to\n        - \"gt\": Greater than\n        - \"geq\": Greater than or equal to\n    :param value_to_compare: Value to compare the input tensor to. If None, we\n    assume it is passed in as an input to the layer.\n    :param result_if_true: Value to return if the condition is true. If None,\n    we assume it is passed in as an input to the layer.\n    :param result_if_false: Value to return if the condition is false. If\n    None, we assume it is passed in as an input to the layer.\n    :param name: The name of the layer. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.condition_operator = condition_operator\n    self.value_to_compare = value_to_compare\n    self.result_if_true = result_if_true\n    self.result_if_false = result_if_false\n\n    if (\n        self.value_to_compare is not None\n        and not isinstance(self.value_to_compare, Number)\n        and self.condition_operator not in [\"eq\", \"neq\"]\n    ):\n        raise TypeError(\n            \"\"\"value_to_compare must be a number for condition operators\n            other than eq and neq.\"\"\"\n        )\n\n    if self.result_if_true is not None and self.result_if_false is not None:\n        if not isinstance(self.result_if_true, type(self.result_if_false)):\n            raise TypeError(\n                \"\"\"If provided, result_if_true and result_if_false must be of the\n                same type.\"\"\"\n            )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/if_statement/#src.kamae.tensorflow.layers.if_statement.IfStatementLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/if_statement/#src.kamae.tensorflow.layers.if_statement.IfStatementLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the numerical if statement on the inputs. If the inputs are a tensor, we assume that the value_to_compare, result_if_true, and result_if_false are provided. If the inputs are not a tensor, we assume any not provided are provided as inputs to the layer.</p> <p>Decorated with <code>@allow_single_or_multiple_tensor_input</code> to ensure that the input is either a single tensor or an iterable of tensors. Returns this result as a list of tensors for easier use here.</p> <p>:param inputs: Tensor or list of tensors. :returns: Tensor after computing the numerical if statement.</p> Source code in <code>src/kamae/tensorflow/layers/if_statement.py</code> <pre><code>@allow_single_or_multiple_tensor_input\ndef _call(self, inputs: Union[Tensor, Iterable[Tensor]], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the numerical if statement on the inputs. If the inputs are a tensor,\n    we assume that the value_to_compare, result_if_true, and result_if_false are\n    provided. If the inputs are not a tensor, we assume any not provided are\n    provided as inputs to the layer.\n\n    Decorated with `@allow_single_or_multiple_tensor_input` to ensure that the input\n    is either a single tensor or an iterable of tensors. Returns this result as a\n    list of tensors for easier use here.\n\n    :param inputs: Tensor or list of tensors.\n    :returns: Tensor after computing the numerical if statement.\n    \"\"\"\n    condition_op = get_condition_operator(self.condition_operator)\n    if not len(inputs) &gt; 1:\n        # If the input is a tensor, we assume that the value_to_compare,\n        # result_if_true, and result_if_false are provided\n        if any(\n            [\n                v is None\n                for v in [\n                    self.value_to_compare,\n                    self.result_if_true,\n                    self.result_if_false,\n                ]\n            ]\n        ):\n            raise ValueError(\n                \"If inputs is a tensor, value_to_compare, result_if_true, and \"\n                \"result_if_false must be specified.\"\n            )\n        if inputs[0].dtype.is_floating or inputs[0].dtype.is_integer:\n            inputs, value_to_compare = self._force_cast_to_compatible_numeric_type(\n                inputs[0], self.value_to_compare\n            )\n        else:\n            inputs = inputs[0]\n            value_to_compare = tf.constant(\n                self.value_to_compare, dtype=inputs.dtype\n            )\n        cond = tf.where(\n            condition_op(inputs, value_to_compare),\n            tf.constant(self.result_if_true),\n            tf.constant(self.result_if_false),\n        )\n        return cond\n    else:\n        # If the input is a list, we assume that the value_to_compare,\n        # result_if_true, and result_if_false are potentially provided in the inputs\n        input_tensors = self._construct_input_tensors(inputs)\n        # Ensure the results are the casted to the input dtype if specified\n        result_if_true = self._create_casted_tensor_from_tensor_or_constant(\n            input_tensors[2]\n        )\n        result_if_false = self._create_casted_tensor_from_tensor_or_constant(\n            input_tensors[3]\n        )\n\n        if isinstance(input_tensors[1], tf.Tensor):\n            # If the value to compare is a tensor, we cast it to the input dtype\n            inputs = input_tensors[0]\n            value_to_compare = self._cast(\n                input_tensors[1], cast_dtype=input_tensors[0].dtype.name\n            )\n        elif (\n            input_tensors[0].dtype.is_floating or input_tensors[0].dtype.is_integer\n        ):\n            # If the inputs are numeric we force cast it to a compatible dtype\n            inputs, value_to_compare = self._force_cast_to_compatible_numeric_type(\n                input_tensors[0], input_tensors[1]\n            )\n        else:\n            # The inputs are not numeric, so we just do the regular casting\n            inputs = input_tensors[0]\n            value_to_compare = self._cast(\n                tf.constant(input_tensors[1]), inputs.dtype.name\n            )\n\n        cond = tf.where(\n            condition_op(\n                inputs,\n                value_to_compare,\n            ),\n            result_if_true,\n            result_if_false,\n        )\n        return cond\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/if_statement/#src.kamae.tensorflow.layers.if_statement.IfStatementLayer._construct_input_tensors","title":"_construct_input_tensors","text":"<pre><code>_construct_input_tensors(inputs)\n</code></pre> <p>Constructs the input tensors for the layer in the case where all the optional parameters are not specified. We need to run through the provided inputs and either select an input or the specified parameter.</p> <p>Specifically for this layer, we assume the inputs are in the following order: [input_tensor, value_to_compare, result_if_true, result_if_false]</p> <p>Any but the input tensor can be None.</p> <p>:param inputs: List of input tensors. :returns: List of input tensors potentially containing constant tensors for the optional parameters.</p> Source code in <code>src/kamae/tensorflow/layers/if_statement.py</code> <pre><code>def _construct_input_tensors(\n    self, inputs: Iterable[tf.Tensor]\n) -&gt; Iterable[tf.Tensor]:\n    \"\"\"\n    Constructs the input tensors for the layer in the case where all the optional\n    parameters are not specified. We need to run through the provided inputs and\n    either select an input or the specified parameter.\n\n    Specifically for this layer, we assume the inputs are in the following order:\n    [input_tensor, value_to_compare, result_if_true, result_if_false]\n\n    Any but the input tensor can be None.\n\n    :param inputs: List of input tensors.\n    :returns: List of input tensors potentially containing constant tensors for the\n    optional parameters.\n    \"\"\"\n    optional_params = [\n        self.value_to_compare,\n        self.result_if_true,\n        self.result_if_false,\n    ]\n    # Setup the inputs. Keep a counter to know how many tensors from inputs have\n    # been used.\n    input_col_counter = 1\n    # First input is always the input tensor\n    multiple_inputs = [inputs[0]]\n    for param in optional_params:\n        if param is None:\n            # If the param is None, we assume it is an input tensor at the next\n            # index\n            multiple_inputs.append(inputs[input_col_counter])\n            input_col_counter += 1\n        else:\n            # Otherwise, we create a constant tensor for the parameter\n            # and do not increment the counter.\n            multiple_inputs.append(param)\n    return multiple_inputs\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/if_statement/#src.kamae.tensorflow.layers.if_statement.IfStatementLayer._create_casted_tensor_from_tensor_or_constant","title":"_create_casted_tensor_from_tensor_or_constant","text":"<pre><code>_create_casted_tensor_from_tensor_or_constant(value)\n</code></pre> <p>Creates a tensor from a tensor or constant value. If the input value is not a tensor, we assume it is a constant and create a tensor from it. If self.input_dtype is not None, we cast the tensor to the specified dtype.</p> Source code in <code>src/kamae/tensorflow/layers/if_statement.py</code> <pre><code>def _create_casted_tensor_from_tensor_or_constant(\n    self, value: Union[tf.Tensor, Any]\n) -&gt; tf.Tensor:\n    \"\"\"\n    Creates a tensor from a tensor or constant value.\n    If the input value is not a tensor, we assume it is a constant and create a\n    tensor from it. If self.input_dtype is not None, we cast the tensor to the\n    specified dtype.\n    \"\"\"\n    if not isinstance(value, tf.Tensor):\n        value = tf.constant(value)\n    return (\n        value\n        if self._input_dtype is None\n        else self._cast(tf.constant(value), self._input_dtype)\n    )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/if_statement/#src.kamae.tensorflow.layers.if_statement.IfStatementLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the IfStatement layer.</p> <p>Specifically adds the following to the base configuration: - condition_operator - value_to_compare - result_if_true - result_if_false</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/if_statement.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the IfStatement layer.\n\n    Specifically adds the following to the base configuration:\n    - condition_operator\n    - value_to_compare\n    - result_if_true\n    - result_if_false\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"condition_operator\": self.condition_operator,\n            \"value_to_compare\": self.value_to_compare,\n            \"result_if_true\": self.result_if_true,\n            \"result_if_false\": self.result_if_false,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/impute/","title":"impute","text":""},{"location":"reference/src/kamae/tensorflow/layers/impute/#src.kamae.tensorflow.layers.impute.ImputeLayer","title":"ImputeLayer","text":"<pre><code>ImputeLayer(\n    impute_value,\n    mask_value,\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs imputation on the input. Where the input data is equal to the specified mask value, this layer will replace the data with the impute value calculated at preprocessing time. The impute value is either the mean or median and is computed while ignoring rows in the data which are equal to the mask value or are null.</p> <p>:param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param mask_value: Value which should be replaced by the impute value at inference.</p> Source code in <code>src/kamae/tensorflow/layers/impute.py</code> <pre><code>def __init__(\n    self,\n    impute_value: Union[float, str, int],\n    mask_value: Union[float, str, int],\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialise the ImputeLayer layer.\n    :param impute_value: The value to use for imputation.\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param mask_value: Value which should be replaced by the\n    impute value at inference.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.impute_value = impute_value\n    self.mask_value = mask_value\n    if not isinstance(self.mask_value, type(self.impute_value)):\n        raise ValueError(\n            \"The mask value and impute value must be of the same type.\"\n        )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/impute/#src.kamae.tensorflow.layers.impute.ImputeLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/impute/#src.kamae.tensorflow.layers.impute.ImputeLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs imputation on the input tensor(s) by calling the keras ImputeLayer layer. It imputes over values which are equal to the mask_value. Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable. :param inputs: Input tensor to perform the imputation on. :returns: The input tensor with the imputation applied.</p> Source code in <code>src/kamae/tensorflow/layers/impute.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs imputation on the input tensor(s) by calling the keras\n    ImputeLayer layer. It imputes over values which are equal to the\n    mask_value.\n    Decorated with `@enforce_single_tensor_input` to ensure that\n    the input is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n    :param inputs: Input tensor to perform the imputation on.\n    :returns: The input tensor with the imputation applied.\n    \"\"\"\n    if inputs.dtype.is_floating or inputs.dtype.is_integer:\n        inputs, mask = self._force_cast_to_compatible_numeric_type(\n            inputs, self.mask_value\n        )\n        inputs, impute_value = self._force_cast_to_compatible_numeric_type(\n            inputs, self.impute_value\n        )\n    else:\n        mask = self._cast(tf.constant(self.mask_value), inputs.dtype.name)\n        impute_value = self._cast(tf.constant(self.impute_value), inputs.dtype.name)\n\n    mask = tf.equal(inputs, mask)\n    imputed_outputs = tf.where(\n        mask,\n        impute_value,\n        inputs,\n    )\n\n    return imputed_outputs\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/impute/#src.kamae.tensorflow.layers.impute.ImputeLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the ImputeLayer layer. Used for saving and loading from a model. Specifically adds additional parameters to the base configuration. :returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/impute.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the ImputeLayer layer.\n    Used for saving and loading from a model.\n    Specifically adds additional parameters to the base configuration.\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"impute_value\": self.impute_value,\n            \"mask_value\": self.mask_value,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/lambda_function/","title":"lambda_function","text":""},{"location":"reference/src/kamae/tensorflow/layers/lambda_function/#src.kamae.tensorflow.layers.lambda_function.LambdaFunctionLayer","title":"LambdaFunctionLayer","text":"<pre><code>LambdaFunctionLayer(\n    function,\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code>, <code>Lambda</code></p> <p>Performs the lambda function operation on a given input tensor</p> <p>WARNING: This layer relies on a <code>tf.keras.layers.Lambda</code> layer which have (de)serialization limitations!</p> <p><code>Lambda</code> layers are saved by serializing the Python bytecode, which is fundamentally non-portable. They should only be loaded in the same environment where they were saved.</p> <p>:param function: The lambda function to apply to the input tensor(s). :param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/lambda_function.py</code> <pre><code>def __init__(\n    self,\n    function: Callable[[Union[Tensor, List[Tensor]]], Union[Tensor, List[Tensor]]],\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the LambdaFunction layer\n\n    :param function: The lambda function to apply to the input tensor(s).\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name,\n        input_dtype=input_dtype,\n        output_dtype=output_dtype,\n        function=function,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/lambda_function/#src.kamae.tensorflow.layers.lambda_function.LambdaFunctionLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/lambda_function/#src.kamae.tensorflow.layers.lambda_function.LambdaFunctionLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Transforms the input tensor(s) by applying the lambda function.</p> <p>Decorated with <code>@allow_single_or_multiple_tensor_input</code> to ensure that the input is either a single tensor or an iterable of tensors. Returns this result as a list of tensors for easier use here.</p> <p>:param inputs: Tensor(s) to apply the lambda function to. :returns: The transformed tensor(s).</p> Source code in <code>src/kamae/tensorflow/layers/lambda_function.py</code> <pre><code>@allow_single_or_multiple_tensor_input\ndef _call(\n    self, inputs: Union[Tensor, Iterable[Tensor]], **kwargs: Any\n) -&gt; Union[Tensor, Iterable[Tensor]]:\n    \"\"\"\n    Transforms the input tensor(s) by applying the lambda function.\n\n    Decorated with `@allow_single_or_multiple_tensor_input` to ensure that the input\n    is either a single tensor or an iterable of tensors. Returns this result as a\n    list of tensors for easier use here.\n\n    :param inputs: Tensor(s) to apply the lambda function to.\n    :returns: The transformed tensor(s).\n    \"\"\"\n    if len(inputs) == 1:\n        return tf.keras.layers.Lambda.call(self, inputs[0], **kwargs)\n    return tf.keras.layers.Lambda.call(self, inputs, **kwargs)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/lambda_function/#src.kamae.tensorflow.layers.lambda_function.LambdaFunctionLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the LambdaFunction layer. Used for saving and loading from a model. Calls the parent class's get_config method which deals with serialising the function.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/lambda_function.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the LambdaFunction layer.\n    Used for saving and loading from a model.\n    Calls the parent class's get_config method which deals with serialising the\n    function.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/list_max/","title":"list_max","text":""},{"location":"reference/src/kamae/tensorflow/layers/list_max/#src.kamae.tensorflow.layers.list_max.ListMaxLayer","title":"ListMaxLayer","text":"<pre><code>ListMaxLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    top_n=None,\n    sort_order=\"asc\",\n    min_filter_value=None,\n    nan_fill_value=0.0,\n    axis=1,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Calculate the max across the axis dimension. - If one tensor is passed, the transformer calculates the max of the tensor based on all the items in the given axis dimension. - If inputCols is set, the transformer calculates the max of the first tensor based on second tensor's topN items in the same given axis dimension.</p> <p>By using the topN items to calculate the statistics, we can better approximate the real statistics in production. It is suggested to use a large enough topN to get a good approximation of the statistics, and an important feature to sort on, such as item's past production.</p> <p>Example: calculate the max price in the same query, based only on the top N items sorted by descending production.</p> <p>WARNING: The code is fully tested for axis=1 only. Further testing is needed.</p> <p>WARNING: The code can be affected by the value of the padding items. Always make sure to filter out the padding items value with min_filter_value.</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param top_n: The number of top items to consider when calculating the max. :param sort_order: The order to sort the second tensor by. Defaults to <code>asc</code>. :param min_filter_value: The minimum filter value to ignore values during calculation. Defaults to None (no filter). :param nan_fill_value: The value to fill NaNs results with. Defaults to 0. :param axis: The axis to calculate the statistics across. Defaults to 1.</p> Source code in <code>src/kamae/tensorflow/layers/list_max.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    top_n: Optional[int] = None,\n    sort_order: str = \"asc\",\n    min_filter_value: Optional[float] = None,\n    nan_fill_value: float = 0.0,\n    axis: int = 1,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the Listwise Max layer.\n\n    WARNING: The code is fully tested for axis=1 only. Further testing is needed.\n\n    WARNING: The code can be affected by the value of the padding items. Always\n    make sure to filter out the padding items value with min_filter_value.\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param top_n: The number of top items to consider when calculating the max.\n    :param sort_order: The order to sort the second tensor by. Defaults to `asc`.\n    :param min_filter_value: The minimum filter value to ignore values during\n    calculation. Defaults to None (no filter).\n    :param nan_fill_value: The value to fill NaNs results with. Defaults to 0.\n    :param axis: The axis to calculate the statistics across. Defaults to 1.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.top_n = top_n\n    self.sort_order = sort_order\n    self.min_filter_value = min_filter_value\n    self.nan_fill_value = nan_fill_value\n    self.axis = axis\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/list_max/#src.kamae.tensorflow.layers.list_max.ListMaxLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/list_max/#src.kamae.tensorflow.layers.list_max.ListMaxLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Calculate the listwise max, optionally sorting and filtering based on the second input tensor.</p> <p>:param inputs: The iterable tensor for the feature. :returns: Thew new tensor result column.</p> Source code in <code>src/kamae/tensorflow/layers/list_max.py</code> <pre><code>@allow_single_or_multiple_tensor_input\ndef _call(self, inputs: Iterable[Tensor], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Calculate the listwise max, optionally sorting and\n    filtering based on the second input tensor.\n\n    :param inputs: The iterable tensor for the feature.\n    :returns: Thew new tensor result column.\n    \"\"\"\n    val_tensor = inputs[0]\n    output_shape = tf.shape(val_tensor)\n\n    with_sort = True if len(inputs) == 2 else False\n    sort_tensor = inputs[1] if with_sort else None\n\n    if with_sort and self.top_n is None:\n        raise ValueError(\"topN must be specified when using a sort column.\")\n\n    if with_sort:\n        # Get the values corresponding to the top N item in the sort tensor\n        filtered_tensor = get_top_n(\n            val_tensor=val_tensor,\n            axis=self.axis,\n            sort_tensor=sort_tensor,\n            sort_order=self.sort_order,\n            top_n=self.top_n,\n        )\n    else:\n        filtered_tensor = val_tensor\n\n    # Apply the mask to filter out elements less than or equal to the threshold\n    if self.min_filter_value is not None:\n        mask = tf.greater_equal(filtered_tensor, self.min_filter_value)\n        neg_inf = val_tensor.dtype.min\n        cond = tf.where(mask, filtered_tensor, neg_inf)\n        listwise_stat = tf.reduce_max(cond, axis=self.axis, keepdims=True)\n        is_integer = listwise_stat.dtype.is_integer\n        nan_val = int(self.nan_fill_value) if is_integer else self.nan_fill_value\n        listwise_max = tf.where(listwise_stat != neg_inf, listwise_stat, nan_val)\n\n    else:\n        # Calculate the mean without filtering\n        listwise_max = tf.reduce_max(filtered_tensor, axis=self.axis, keepdims=True)\n\n    # Broadcast the stat to each item in the list\n    # WARNING: If filter creates empty items list, the result will be NaN\n    listwise_max = tf.broadcast_to(listwise_max, output_shape)\n\n    return listwise_max\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/list_max/#src.kamae.tensorflow.layers.list_max.ListMaxLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the layer. Used for saving and loading from a model.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/list_max.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the layer.\n    Used for saving and loading from a model.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"top_n\": self.top_n,\n            \"sort_order\": self.sort_order,\n            \"min_filter_value\": self.min_filter_value,\n            \"nan_fill_value\": self.nan_fill_value,\n            \"axis\": self.axis,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/list_mean/","title":"list_mean","text":""},{"location":"reference/src/kamae/tensorflow/layers/list_mean/#src.kamae.tensorflow.layers.list_mean.ListMeanLayer","title":"ListMeanLayer","text":"<pre><code>ListMeanLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    top_n=None,\n    sort_order=\"asc\",\n    min_filter_value=None,\n    nan_fill_value=0.0,\n    axis=1,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Calculate the mean across the axis dimension. - If one tensor is passed, the transformer calculates the mean of the tensor based on all the items in the given axis dimension. - If inputCols is set, the transformer calculates the mean of the first tensor based on second tensor's topN items in the same given axis dimension.</p> <p>By using the topN items to calculate the statistics, we can better approximate the real statistics in production. It is suggested to use a large enough topN to get a good approximation of the statistics, and an important feature to sort on, such as item's past production.</p> <p>Example: calculate the mean price in the same query, based only on the top N items sorted by descending production.</p> <p>WARNING: The code is fully tested for axis=1 only. Further testing is needed.</p> <p>WARNING: The code can be affected by the value of the padding items. Always make sure to filter out the padding items value with min_filter_value.</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param top_n: The number of top items to consider when calculating the mean. :param sort_order: The order to sort the second tensor by. Defaults to <code>asc</code>. :param min_filter_value: The minimum filter value to ignore values during calculation. Defaults to None (no filter). :param nan_fill_value: The value to fill NaNs results with. Defaults to 0. :param axis: The axis to calculate the statistics across. Defaults to 1.</p> Source code in <code>src/kamae/tensorflow/layers/list_mean.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    top_n: Optional[int] = None,\n    sort_order: str = \"asc\",\n    min_filter_value: Optional[float] = None,\n    nan_fill_value: float = 0.0,\n    axis: int = 1,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the Listwise Mean layer.\n\n    WARNING: The code is fully tested for axis=1 only. Further testing is needed.\n\n    WARNING: The code can be affected by the value of the padding items. Always\n    make sure to filter out the padding items value with min_filter_value.\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param top_n: The number of top items to consider when calculating the mean.\n    :param sort_order: The order to sort the second tensor by. Defaults to `asc`.\n    :param min_filter_value: The minimum filter value to ignore values during\n    calculation. Defaults to None (no filter).\n    :param nan_fill_value: The value to fill NaNs results with. Defaults to 0.\n    :param axis: The axis to calculate the statistics across. Defaults to 1.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.top_n = top_n\n    self.sort_order = sort_order\n    self.min_filter_value = min_filter_value\n    self.nan_fill_value = nan_fill_value\n    self.axis = axis\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/list_mean/#src.kamae.tensorflow.layers.list_mean.ListMeanLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/list_mean/#src.kamae.tensorflow.layers.list_mean.ListMeanLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Calculate the listwise mean, optionally sorting and filtering based on the second input tensor.</p> <p>:param inputs: The iterable tensor for the feature. :returns: Thew new tensor result column.</p> Source code in <code>src/kamae/tensorflow/layers/list_mean.py</code> <pre><code>@allow_single_or_multiple_tensor_input\ndef _call(self, inputs: Iterable[Tensor], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Calculate the listwise mean, optionally sorting and\n    filtering based on the second input tensor.\n\n    :param inputs: The iterable tensor for the feature.\n    :returns: Thew new tensor result column.\n    \"\"\"\n    val_tensor = inputs[0]\n    output_shape = tf.shape(val_tensor)\n\n    with_sort = True if len(inputs) == 2 else False\n    sort_tensor = inputs[1] if with_sort else None\n\n    if with_sort and self.top_n is None:\n        raise ValueError(\"topN must be specified when using a sort column.\")\n\n    if with_sort:\n        # Get the values corresponding to the top N item in the sort tensor\n        filtered_tensor = get_top_n(\n            val_tensor=val_tensor,\n            axis=self.axis,\n            sort_tensor=sort_tensor,\n            sort_order=self.sort_order,\n            top_n=self.top_n,\n        )\n    else:\n        filtered_tensor = val_tensor\n\n    # Apply the mask to filter out elements less than or equal to the threshold\n    if self.min_filter_value is not None:\n        mask = tf.greater_equal(filtered_tensor, self.min_filter_value)\n        nan_tensor = tf.constant(float(\"nan\"), dtype=val_tensor.dtype)\n        filtered_tensor = tf.where(mask, filtered_tensor, nan_tensor)\n        mask = tf.math.is_finite(filtered_tensor)\n        listwise_sum = tf.reduce_sum(\n            tf.where(mask, filtered_tensor, tf.zeros_like(filtered_tensor)),\n            axis=self.axis,\n            keepdims=True,\n        )\n        listwise_count = tf.reduce_sum(\n            tf.cast(mask, dtype=listwise_sum.dtype),\n            axis=self.axis,\n            keepdims=True,\n        )\n        listwise_mean = tf.math.divide_no_nan(listwise_sum, listwise_count)\n\n    else:\n        # Calculate the mean without filtering\n        listwise_mean = tf.reduce_mean(\n            filtered_tensor,\n            axis=self.axis,\n            keepdims=True,\n        )\n\n    # Fill nan\n    is_integer = listwise_mean.dtype.is_integer\n    nan_val = int(self.nan_fill_value) if is_integer else self.nan_fill_value\n    listwise_mean = tf.where(\n        tf.math.is_nan(listwise_mean),\n        tf.constant(nan_val, dtype=listwise_mean.dtype),\n        listwise_mean,\n    )\n\n    # Broadcast the stat to each item in the list\n    # WARNING: If filter creates empty items list, the result will be NaN\n    listwise_mean = tf.broadcast_to(listwise_mean, output_shape)\n\n    return listwise_mean\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/list_mean/#src.kamae.tensorflow.layers.list_mean.ListMeanLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the layer. Used for saving and loading from a model.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/list_mean.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the layer.\n    Used for saving and loading from a model.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"top_n\": self.top_n,\n            \"sort_order\": self.sort_order,\n            \"min_filter_value\": self.min_filter_value,\n            \"nan_fill_value\": self.nan_fill_value,\n            \"axis\": self.axis,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/list_median/","title":"list_median","text":""},{"location":"reference/src/kamae/tensorflow/layers/list_median/#src.kamae.tensorflow.layers.list_median.ListMedianLayer","title":"ListMedianLayer","text":"<pre><code>ListMedianLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    top_n=None,\n    sort_order=\"asc\",\n    min_filter_value=None,\n    nan_fill_value=0.0,\n    axis=1,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Calculate the median across the axis dimension. - If one tensor is passed, the transformer calculates the median of the tensor based on all the items in the given axis dimension. - If inputCols is set, the transformer calculates the median of the first tensor based on second tensor's topN items in the same given axis dimension.</p> <p>By using the topN items to calculate the statistics, we can better approximate the real statistics in production. It is suggested to use a large enough topN to get a good approximation of the statistics, and an important feature to sort on, such as item's past production.</p> <p>Example: calculate the median price in the same query, based only on the top N items sorted by descending production.</p> <p>WARNING: The code is fully tested for axis=1 only. Further testing is needed.</p> <p>WARNING: The code can be affected by the value of the padding items. Always make sure to filter out the padding items value with min_filter_value.</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param top_n: The number of top items to consider when calculating the median. :param sort_order: The order to sort the second tensor by. Defaults to <code>asc</code>. :param min_filter_value: The minimum filter value to ignore values during calculation. Defaults to None (no filter). :param nan_fill_value: The value to fill NaNs results with. Defaults to 0. :param axis: The axis to calculate the statistics across. Defaults to 1.</p> Source code in <code>src/kamae/tensorflow/layers/list_median.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    top_n: Optional[int] = None,\n    sort_order: str = \"asc\",\n    min_filter_value: Optional[float] = None,\n    nan_fill_value: float = 0.0,\n    axis: int = 1,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the Listwise Median layer.\n\n    WARNING: The code is fully tested for axis=1 only. Further testing is needed.\n\n    WARNING: The code can be affected by the value of the padding items. Always\n    make sure to filter out the padding items value with min_filter_value.\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param top_n: The number of top items to consider when calculating the median.\n    :param sort_order: The order to sort the second tensor by. Defaults to `asc`.\n    :param min_filter_value: The minimum filter value to ignore values during\n    calculation. Defaults to None (no filter).\n    :param nan_fill_value: The value to fill NaNs results with. Defaults to 0.\n    :param axis: The axis to calculate the statistics across. Defaults to 1.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.top_n = top_n\n    self.sort_order = sort_order\n    self.min_filter_value = min_filter_value\n    self.nan_fill_value = nan_fill_value\n    self.axis = axis\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/list_median/#src.kamae.tensorflow.layers.list_median.ListMedianLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/list_median/#src.kamae.tensorflow.layers.list_median.ListMedianLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Calculate the listwise median, optionally sorting and filtering based on the second input tensor.</p> <p>:param inputs: The iterable tensor for the feature. :returns: The new tensor result column.</p> Source code in <code>src/kamae/tensorflow/layers/list_median.py</code> <pre><code>@allow_single_or_multiple_tensor_input\ndef _call(self, inputs: Iterable[Tensor], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Calculate the listwise median, optionally sorting and\n    filtering based on the second input tensor.\n\n    :param inputs: The iterable tensor for the feature.\n    :returns: The new tensor result column.\n    \"\"\"\n    val_tensor = inputs[0]\n    output_shape = tf.shape(val_tensor)\n\n    with_sort = True if len(inputs) == 2 else False\n    sort_tensor = inputs[1] if with_sort else None\n\n    if with_sort and self.top_n is None:\n        raise ValueError(\"topN must be specified when using a sort column.\")\n\n    if with_sort:\n        # Get the values corresponding to the top N item in the sort tensor\n        filtered_tensor = get_top_n(\n            val_tensor=val_tensor,\n            axis=self.axis,\n            sort_tensor=sort_tensor,\n            sort_order=self.sort_order,\n            top_n=self.top_n,\n        )\n    else:\n        filtered_tensor = val_tensor\n\n    # Assign nan to elements less than or equal to the threshold\n    if self.min_filter_value is not None:\n        filtered_tensor = tf.where(\n            filtered_tensor &gt;= self.min_filter_value,\n            filtered_tensor,\n            tf.constant(float(\"nan\"), dtype=val_tensor.dtype),\n        )\n    else:\n        filtered_tensor = filtered_tensor\n\n    # Get the number of non-nan values\n    num_valid_values = tf.reduce_sum(\n        tf.cast(tf.math.is_finite(filtered_tensor), tf.int32), axis=self.axis\n    )\n\n    # Sort the values along the list dimension\n    sorted_filtered_tensor = self.sort_with_nans_last(filtered_tensor)\n\n    # Calculate the indices of the median values\n    lower_index = (num_valid_values - 1) // 2\n    upper_index = tf.minimum(lower_index + 1, num_valid_values - 1)\n\n    # Gather the median values for each feature\n    batch_size = tf.shape(filtered_tensor)[0]\n    batch_indices = tf.range(batch_size)[:, tf.newaxis, tf.newaxis]\n    lower_indices = tf.concat([batch_indices, lower_index[:, tf.newaxis]], axis=-1)\n    lower_medians = tf.gather_nd(sorted_filtered_tensor, lower_indices)\n    upper_indices = tf.concat([batch_indices, upper_index[:, tf.newaxis]], axis=-1)\n    upper_medians = tf.gather_nd(sorted_filtered_tensor, upper_indices)\n\n    # Calculate the average of lower and upper medians for even cases\n    listwise_median = tf.where(\n        tf.math.mod(num_valid_values[:, tf.newaxis], 2) == 0,\n        (lower_medians + upper_medians) / 2.0,\n        lower_medians,\n    )\n\n    # Fill nan\n    is_integer = listwise_median.dtype.is_integer\n    nan_val = int(self.nan_fill_value) if is_integer else self.nan_fill_value\n    listwise_median = tf.where(\n        tf.math.is_nan(listwise_median),\n        tf.constant(nan_val, dtype=listwise_median.dtype),\n        listwise_median,\n    )\n\n    # Broadcast the stat to each item in the list\n    # WARNING: If filter creates empty items list, the result will be NaN\n    listwise_median = tf.broadcast_to(listwise_median, output_shape)\n\n    return listwise_median\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/list_median/#src.kamae.tensorflow.layers.list_median.ListMedianLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the layer. Used for saving and loading from a model.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/list_median.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the layer.\n    Used for saving and loading from a model.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"top_n\": self.top_n,\n            \"sort_order\": self.sort_order,\n            \"min_filter_value\": self.min_filter_value,\n            \"nan_fill_value\": self.nan_fill_value,\n            \"axis\": self.axis,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/list_median/#src.kamae.tensorflow.layers.list_median.ListMedianLayer.sort_with_nans_last","title":"sort_with_nans_last","text":"<pre><code>sort_with_nans_last(tensor)\n</code></pre> <p>Sorts a tensor while placing NaN values at the end along the specified axis.</p> <p>:param tensor: The input tensor. :param axis: The axis along which to sort. :returns: The sorted tensor with NaN values placed at the end.</p> Source code in <code>src/kamae/tensorflow/layers/list_median.py</code> <pre><code>def sort_with_nans_last(self, tensor: Tensor) -&gt; Tensor:\n    \"\"\"\n    Sorts a tensor while placing NaN values at the end along the specified axis.\n\n    :param tensor: The input tensor.\n    :param axis: The axis along which to sort.\n    :returns: The sorted tensor with NaN values placed at the end.\n    \"\"\"\n    # Replace NaNs with a very large value to move them to the end\n    masked_tensor = tf.where(tf.math.is_nan(tensor), tensor.dtype.max, tensor)\n\n    # Sort the tensor along the specified axis\n    sorted_masked_tensor = tf.sort(masked_tensor, axis=self.axis)\n\n    # Replace the very large values back with NaN after sorting\n    sorted_masked_tensor = tf.where(\n        tf.equal(sorted_masked_tensor, tensor.dtype.max),\n        tf.constant(float(\"nan\"), dtype=tensor.dtype),\n        sorted_masked_tensor,\n    )\n\n    return sorted_masked_tensor\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/list_min/","title":"list_min","text":""},{"location":"reference/src/kamae/tensorflow/layers/list_min/#src.kamae.tensorflow.layers.list_min.ListMinLayer","title":"ListMinLayer","text":"<pre><code>ListMinLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    top_n=None,\n    sort_order=\"asc\",\n    min_filter_value=None,\n    nan_fill_value=0.0,\n    axis=1,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Calculate the min across the axis dimension. - If one tensor is passed, the transformer calculates the min of the tensor based on all the items in the given axis dimension. - If inputCols is set, the transformer calculates the min of the first tensor based on second tensor's topN items in the same given axis dimension.</p> <p>By using the topN items to calculate the statistics, we can better approximate the real statistics in production. It is suggested to use a large enough topN to get a good approximation of the statistics, and an important feature to sort on, such as item's past production.</p> <p>Example: calculate the min price in the same query, based only on the top N items sorted by descending production.</p> <p>WARNING: The code is fully tested for axis=1 only. Further testing is needed.</p> <p>WARNING: The code can be affected by the value of the padding items. Always make sure to filter out the padding items value with min_filter_value.</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param top_n: The number of top items to consider when calculating the min. :param sort_order: The order to sort the second tensor by. Defaults to <code>asc</code>. :param min_filter_value: The minimum filter value to ignore values during calculation. Defaults to None (no filter). :param nan_fill_value: The value to fill NaNs results with. Defaults to 0. :param axis: The axis to calculate the statistics across. Defaults to 1.</p> Source code in <code>src/kamae/tensorflow/layers/list_min.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    top_n: Optional[int] = None,\n    sort_order: str = \"asc\",\n    min_filter_value: Optional[float] = None,\n    nan_fill_value: float = 0.0,\n    axis: int = 1,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the Listwise Min layer.\n\n    WARNING: The code is fully tested for axis=1 only. Further testing is needed.\n\n    WARNING: The code can be affected by the value of the padding items. Always\n    make sure to filter out the padding items value with min_filter_value.\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param top_n: The number of top items to consider when calculating the min.\n    :param sort_order: The order to sort the second tensor by. Defaults to `asc`.\n    :param min_filter_value: The minimum filter value to ignore values during\n    calculation. Defaults to None (no filter).\n    :param nan_fill_value: The value to fill NaNs results with. Defaults to 0.\n    :param axis: The axis to calculate the statistics across. Defaults to 1.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.top_n = top_n\n    self.sort_order = sort_order\n    self.min_filter_value = min_filter_value\n    self.nan_fill_value = nan_fill_value\n    self.axis = axis\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/list_min/#src.kamae.tensorflow.layers.list_min.ListMinLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/list_min/#src.kamae.tensorflow.layers.list_min.ListMinLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Calculate the listwise min, optionally sorting and filtering based on the second input tensor.</p> <p>:param inputs: The iterable tensor for the feature. :returns: Thew new tensor result column.</p> Source code in <code>src/kamae/tensorflow/layers/list_min.py</code> <pre><code>@allow_single_or_multiple_tensor_input\ndef _call(self, inputs: Iterable[Tensor], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Calculate the listwise min, optionally sorting and\n    filtering based on the second input tensor.\n\n    :param inputs: The iterable tensor for the feature.\n    :returns: Thew new tensor result column.\n    \"\"\"\n    val_tensor = inputs[0]\n    output_shape = tf.shape(val_tensor)\n\n    with_sort = True if len(inputs) == 2 else False\n    sort_tensor = inputs[1] if with_sort else None\n\n    if with_sort and self.top_n is None:\n        raise ValueError(\"topN must be specified when using a sort column.\")\n\n    if with_sort:\n        # Get the values corresponding to the top N item in the sort tensor\n        filtered_tensor = get_top_n(\n            val_tensor=val_tensor,\n            axis=self.axis,\n            sort_tensor=sort_tensor,\n            sort_order=self.sort_order,\n            top_n=self.top_n,\n        )\n    else:\n        filtered_tensor = val_tensor\n\n    # Apply the mask to filter out elements less than or equal to the threshold\n    if self.min_filter_value is not None:\n        mask = tf.greater_equal(filtered_tensor, self.min_filter_value)\n        inf = val_tensor.dtype.max\n        cond = tf.where(mask, filtered_tensor, inf)\n        listwise_stat = tf.reduce_min(cond, axis=self.axis, keepdims=True)\n        is_integer = listwise_stat.dtype.is_integer\n        nan_val = int(self.nan_fill_value) if is_integer else self.nan_fill_value\n        fill_val = tf.constant(nan_val, dtype=listwise_stat.dtype)\n        listwise_min = tf.where(listwise_stat != inf, listwise_stat, fill_val)\n\n    else:\n        # Calculate the mean without filtering\n        listwise_min = tf.reduce_min(filtered_tensor, axis=self.axis, keepdims=True)\n\n    # Broadcast the stat to each item in the list\n    # WARNING: If filter creates empty items list, the result will be NaN\n    listwise_min = tf.broadcast_to(listwise_min, output_shape)\n\n    return listwise_min\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/list_min/#src.kamae.tensorflow.layers.list_min.ListMinLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the layer. Used for saving and loading from a model.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/list_min.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the layer.\n    Used for saving and loading from a model.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"top_n\": self.top_n,\n            \"sort_order\": self.sort_order,\n            \"min_filter_value\": self.min_filter_value,\n            \"nan_fill_value\": self.nan_fill_value,\n            \"axis\": self.axis,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/list_rank/","title":"list_rank","text":""},{"location":"reference/src/kamae/tensorflow/layers/list_rank/#src.kamae.tensorflow.layers.list_rank.ListRankLayer","title":"ListRankLayer","text":"<pre><code>ListRankLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    sort_order=\"desc\",\n    axis=1,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Calculate the rank across the axis dimension.</p> <p>Example: calculate the rank of items within a query, given the score.</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param sort_order: The order to sort the input tensor by. Defaults to 'desc' :param axis: The axis to calculate the rank across. Defaults to 1.</p> Source code in <code>src/kamae/tensorflow/layers/list_rank.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    sort_order: str = \"desc\",\n    axis: int = 1,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the Listwise Rank layer.\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param sort_order: The order to sort the input tensor by. Defaults to 'desc'\n    :param axis: The axis to calculate the rank across. Defaults to 1.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.sort_order = sort_order\n    self.axis = axis\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/list_rank/#src.kamae.tensorflow.layers.list_rank.ListRankLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/list_rank/#src.kamae.tensorflow.layers.list_rank.ListRankLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Calculate the rank.</p> <p>:param inputs: The iterable tensor for the feature. :returns: The new tensor result column.</p> Source code in <code>src/kamae/tensorflow/layers/list_rank.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Iterable[Tensor], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Calculate the rank.\n\n    :param inputs: The iterable tensor for the feature.\n    :returns: The new tensor result column.\n    \"\"\"\n    return tf.math.add(\n        tf.argsort(\n            tf.argsort(\n                inputs,\n                axis=self.axis,\n                direction=\"ASCENDING\" if self.sort_order == \"asc\" else \"DESCENDING\",\n                stable=True,\n            ),\n            axis=self.axis,\n            stable=True,\n        ),\n        1,\n    )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/list_rank/#src.kamae.tensorflow.layers.list_rank.ListRankLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the layer. Used for saving and loading from a model.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/list_rank.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the layer.\n    Used for saving and loading from a model.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"axis\": self.axis,\n            \"sort_order\": self.sort_order,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/list_std_dev/","title":"list_std_dev","text":""},{"location":"reference/src/kamae/tensorflow/layers/list_std_dev/#src.kamae.tensorflow.layers.list_std_dev.ListStdDevLayer","title":"ListStdDevLayer","text":"<pre><code>ListStdDevLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    top_n=None,\n    sort_order=\"asc\",\n    min_filter_value=None,\n    nan_fill_value=0.0,\n    axis=1,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Calculate the average across the axis dimension. - If one tensor is passed, the transformer calculates the average of the tensor based on all the items in the given axis dimension. - If inputCols is set, the transformer calculates the average of the first tensor based on second tensor's topN items in the same given axis dimension.</p> <p>By using the topN items to calculate the statistics, we can better approximate the real statistics in production. It is suggested to use a large enough topN to get a good approximation of the statistics, and an important feature to sort on, such as item's past production.</p> <p>Example: calculate the average price in the same query, based only on the top N items sorted by descending production.</p> <p>WARNING: The code is fully tested for axis=1 only. Further testing is needed.</p> <p>WARNING: The code can be affected by the value of the padding items. Always make sure to filter out the padding items value with min_filter_value.</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param top_n: The number of top items to consider when calculating the average. :param sort_order: The order to sort the second tensor by. Defaults to <code>asc</code>. :param min_filter_value: The minimum filter value to ignore values during calculation. Defaults to None (no filter). :param nan_fill_value: The value to fill NaNs results with. Defaults to 0. :param axis: The axis to calculate the statistics across. Defaults to 1.</p> Source code in <code>src/kamae/tensorflow/layers/list_std_dev.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    top_n: Optional[int] = None,\n    sort_order: str = \"asc\",\n    min_filter_value: Optional[float] = None,\n    nan_fill_value: float = 0.0,\n    axis: int = 1,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the Listwise Average layer.\n\n    WARNING: The code is fully tested for axis=1 only. Further testing is needed.\n\n    WARNING: The code can be affected by the value of the padding items. Always\n    make sure to filter out the padding items value with min_filter_value.\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param top_n: The number of top items to consider when calculating the average.\n    :param sort_order: The order to sort the second tensor by. Defaults to `asc`.\n    :param min_filter_value: The minimum filter value to ignore values during\n    calculation. Defaults to None (no filter).\n    :param nan_fill_value: The value to fill NaNs results with. Defaults to 0.\n    :param axis: The axis to calculate the statistics across. Defaults to 1.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.top_n = top_n\n    self.sort_order = sort_order\n    self.min_filter_value = min_filter_value\n    self.nan_fill_value = nan_fill_value\n    self.axis = axis\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/list_std_dev/#src.kamae.tensorflow.layers.list_std_dev.ListStdDevLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/list_std_dev/#src.kamae.tensorflow.layers.list_std_dev.ListStdDevLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Calculate the listwise average, optionally sorting and filtering based on the second input tensor.</p> <p>:param inputs: The iterable tensor for the feature. :returns: Thew new tensor result column.</p> Source code in <code>src/kamae/tensorflow/layers/list_std_dev.py</code> <pre><code>@allow_single_or_multiple_tensor_input\ndef _call(self, inputs: Iterable[Tensor], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Calculate the listwise average, optionally sorting and\n    filtering based on the second input tensor.\n\n    :param inputs: The iterable tensor for the feature.\n    :returns: Thew new tensor result column.\n    \"\"\"\n    val_tensor = inputs[0]\n    output_shape = tf.shape(val_tensor)\n\n    with_sort = True if len(inputs) == 2 else False\n    sort_tensor = inputs[1] if with_sort else None\n\n    if with_sort and self.top_n is None:\n        raise ValueError(\"topN must be specified when using a sort column.\")\n\n    if with_sort:\n        # Get the values corresponding to the top N item in the sort tensor\n        filtered_tensor = get_top_n(\n            val_tensor=val_tensor,\n            axis=self.axis,\n            sort_tensor=sort_tensor,\n            sort_order=self.sort_order,\n            top_n=self.top_n,\n        )\n    else:\n        filtered_tensor = val_tensor\n\n    # Apply the mask to filter out elements less than or equal to the threshold\n    if self.min_filter_value is not None:\n        mask = tf.greater_equal(filtered_tensor, self.min_filter_value)\n        nan_tensor = tf.constant(float(\"nan\"), dtype=val_tensor.dtype)\n        filtered_tensor = tf.where(mask, filtered_tensor, nan_tensor)\n        mask = tf.math.is_finite(filtered_tensor)\n        numerator = tf.reduce_sum(\n            tf.where(mask, filtered_tensor, tf.zeros_like(filtered_tensor)),\n            axis=self.axis,\n            keepdims=True,\n        )\n        denominator = tf.reduce_sum(\n            tf.cast(mask, dtype=numerator.dtype),\n            axis=self.axis,\n            keepdims=True,\n        )\n        listwise_mean = tf.truediv(numerator, denominator)\n\n    else:\n        # Calculate the mean without filtering\n        listwise_mean = tf.reduce_mean(\n            filtered_tensor,\n            axis=self.axis,\n            keepdims=True,\n        )\n\n    # Calculate the squared differences from the mean\n    squared_diff = tf.square(filtered_tensor - listwise_mean)\n\n    # Calculate the sample variance by dividing the sum of squared diff by (N - 1)\n    mask = tf.math.is_finite(squared_diff)\n    listwise_sum = tf.reduce_sum(\n        tf.where(mask, squared_diff, tf.zeros_like(squared_diff)),\n        axis=self.axis,\n        keepdims=True,\n    )\n    listwise_count = tf.reduce_sum(\n        tf.cast(mask, dtype=listwise_sum.dtype),\n        axis=self.axis,\n        keepdims=True,\n    )\n    listwise_variance = tf.math.divide_no_nan(listwise_sum, (listwise_count - 1))\n    listwise_stddev = tf.sqrt(listwise_variance)\n\n    # Fill nan\n    is_integer = listwise_stddev.dtype.is_integer\n    nan_val = int(self.nan_fill_value) if is_integer else self.nan_fill_value\n    listwise_stddev = tf.where(\n        tf.math.is_nan(listwise_stddev),\n        tf.constant(nan_val, dtype=listwise_mean.dtype),\n        listwise_stddev,\n    )\n\n    # Broadcast the stat to each item in the list\n    # WARNING: If filter creates empty items list, the result will be NaN\n    listwise_stddev = tf.broadcast_to(listwise_stddev, output_shape)\n\n    return listwise_stddev\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/list_std_dev/#src.kamae.tensorflow.layers.list_std_dev.ListStdDevLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the layer. Used for saving and loading from a model.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/list_std_dev.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the layer.\n    Used for saving and loading from a model.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"top_n\": self.top_n,\n            \"sort_order\": self.sort_order,\n            \"min_filter_value\": self.min_filter_value,\n            \"nan_fill_value\": self.nan_fill_value,\n            \"axis\": self.axis,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/log/","title":"log","text":""},{"location":"reference/src/kamae/tensorflow/layers/log/#src.kamae.tensorflow.layers.log.LogLayer","title":"LogLayer","text":"<pre><code>LogLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    alpha=0.0,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs the log(alpha + x) operation on a given input tensor</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param alpha: Alpha value to use in the log(alpha + x) operation, defaults to 0.0.</p> Source code in <code>src/kamae/tensorflow/layers/log.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    alpha: float = 0.0,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the LogLayer layer\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param alpha: Alpha value to use in the log(alpha + x) operation,\n    defaults to 0.0.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.alpha = alpha\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/log/#src.kamae.tensorflow.layers.log.LogLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/log/#src.kamae.tensorflow.layers.log.LogLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the log(alpha + x) operation on a given input tensor</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param inputs: Input tensor to perform the log(alpha + x) operation on. :returns: The input tensor with the log(alpha + x) operation applied.</p> Source code in <code>src/kamae/tensorflow/layers/log.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the log(alpha + x) operation on a given input tensor\n\n    Decorated with `@enforce_single_tensor_input` to ensure that\n    the input is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n\n    :param inputs: Input tensor to perform the log(alpha + x) operation on.\n    :returns: The input tensor with the log(alpha + x) operation applied.\n    \"\"\"\n    return tf.math.log(tf.math.add(inputs, self.alpha))\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/log/#src.kamae.tensorflow.layers.log.LogLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the LogAlphaP layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>alpha</code> value to the configuration.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/log.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the LogAlphaP layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `alpha` value to the configuration.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"alpha\": self.alpha})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/logical_and/","title":"logical_and","text":""},{"location":"reference/src/kamae/tensorflow/layers/logical_and/#src.kamae.tensorflow.layers.logical_and.LogicalAndLayer","title":"LogicalAndLayer","text":"<pre><code>LogicalAndLayer(\n    name=None, input_dtype=None, output_dtype=None, **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs the and(x, y) operation on a given input tensor.</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/logical_and.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the LogicalAndLayer layer\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/logical_and/#src.kamae.tensorflow.layers.logical_and.LogicalAndLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/logical_and/#src.kamae.tensorflow.layers.logical_and.LogicalAndLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the and(x, y) operation on an iterable of input tensors</p> <p>Decorated with <code>@enforce_multiple_tensor_input</code> to ensure that the input is an iterable of tensors. Raises an error if a single tensor is passed in.</p> <p>:param inputs: Iterable of tensors to perform the and(x, y) operation on. :returns: The tensor resulting from the and(x, y) operation.</p> Source code in <code>src/kamae/tensorflow/layers/logical_and.py</code> <pre><code>@enforce_multiple_tensor_input\ndef _call(self, inputs: Iterable[Tensor], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the and(x, y) operation on an iterable of input tensors\n\n    Decorated with `@enforce_multiple_tensor_input` to ensure that the input\n    is an iterable of tensors. Raises an error if a single tensor is passed\n    in.\n\n    :param inputs: Iterable of tensors to perform the and(x, y) operation on.\n    :returns: The tensor resulting from the and(x, y) operation.\n    \"\"\"\n    if len(inputs) == 1:\n        raise ValueError(\"Expected multiple inputs, but got a single input\")\n    return reduce(tf.math.logical_and, inputs)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/logical_and/#src.kamae.tensorflow.layers.logical_and.LogicalAndLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the LogicalAnd layer. Used for saving and loading from a model.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/logical_and.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the LogicalAnd layer.\n    Used for saving and loading from a model.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/logical_not/","title":"logical_not","text":""},{"location":"reference/src/kamae/tensorflow/layers/logical_not/#src.kamae.tensorflow.layers.logical_not.LogicalNotLayer","title":"LogicalNotLayer","text":"<pre><code>LogicalNotLayer(\n    name=None, input_dtype=None, output_dtype=None, **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs the not operation on a given input tensor.</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/logical_not.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the LogicalNotLayer layer\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/logical_not/#src.kamae.tensorflow.layers.logical_not.LogicalNotLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/logical_not/#src.kamae.tensorflow.layers.logical_not.LogicalNotLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the not operation on a single input tensor</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param inputs: Input tensor to perform the not operation on. :returns: The tensor resulting from the or(x, y) operation.</p> Source code in <code>src/kamae/tensorflow/layers/logical_not.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the not operation on a single input tensor\n\n    Decorated with `@enforce_single_tensor_input` to ensure that the input\n    is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n\n    :param inputs: Input tensor to perform the not operation on.\n    :returns: The tensor resulting from the or(x, y) operation.\n    \"\"\"\n    return tf.math.logical_not(inputs)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/logical_not/#src.kamae.tensorflow.layers.logical_not.LogicalNotLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the LogicalNot layer. Used for saving and loading from a model.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/logical_not.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the LogicalNot layer.\n    Used for saving and loading from a model.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/logical_or/","title":"logical_or","text":""},{"location":"reference/src/kamae/tensorflow/layers/logical_or/#src.kamae.tensorflow.layers.logical_or.LogicalOrLayer","title":"LogicalOrLayer","text":"<pre><code>LogicalOrLayer(\n    name=None, input_dtype=None, output_dtype=None, **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs the or(x, y) operation on a given input tensor.</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/logical_or.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the LogicalOrLayer layer\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/logical_or/#src.kamae.tensorflow.layers.logical_or.LogicalOrLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/logical_or/#src.kamae.tensorflow.layers.logical_or.LogicalOrLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the or(x, y) operation on an iterable of input tensors</p> <p>Decorated with <code>@enforce_multiple_tensor_input</code> to ensure that the input is an iterable of tensors. Raises an error if a single tensor is passed in.</p> <p>:param inputs: Iterable of tensors to perform the or(x, y) operation on. :returns: The tensor resulting from the or(x, y) operation.</p> Source code in <code>src/kamae/tensorflow/layers/logical_or.py</code> <pre><code>@enforce_multiple_tensor_input\ndef _call(self, inputs: Iterable[Tensor], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the or(x, y) operation on an iterable of input tensors\n\n    Decorated with `@enforce_multiple_tensor_input` to ensure that the input\n    is an iterable of tensors. Raises an error if a single tensor is passed\n    in.\n\n    :param inputs: Iterable of tensors to perform the or(x, y) operation on.\n    :returns: The tensor resulting from the or(x, y) operation.\n    \"\"\"\n    if len(inputs) == 1:\n        raise ValueError(\"Expected multiple inputs, but got a single input\")\n    return reduce(tf.math.logical_or, inputs)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/logical_or/#src.kamae.tensorflow.layers.logical_or.LogicalOrLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the LogicalOr layer. Used for saving and loading from a model.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/logical_or.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the LogicalOr layer.\n    Used for saving and loading from a model.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/max/","title":"max","text":""},{"location":"reference/src/kamae/tensorflow/layers/max/#src.kamae.tensorflow.layers.max.MaxLayer","title":"MaxLayer","text":"<pre><code>MaxLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    max_constant=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs the max(x, y) operation on a given input tensor. If max_constant is not set, inputs are assumed to be a list of tensors and the max of all the tensors is computed. If max_constant is set, inputs must be a tensor.</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param max_constant: The constant to max against the input, defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/max.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    max_constant: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MaxLayer layer\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param max_constant: The constant to max against the input, defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.max_constant = max_constant\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/max/#src.kamae.tensorflow.layers.max.MaxLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/max/#src.kamae.tensorflow.layers.max.MaxLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the max(x, y) operation on either an iterable of input tensors or a single input tensor and a constant.</p> <p>Decorated with <code>@allow_single_or_multiple_tensor_input</code> to ensure that the input is either a single tensor or an iterable of tensors. Returns this result as a list of tensors for easier use here.</p> <p>:param inputs: Single tensor or iterable of tensors to perform the max(x, y) operation on. :returns: The tensor resulting from the max(x, y) operation.</p> Source code in <code>src/kamae/tensorflow/layers/max.py</code> <pre><code>@allow_single_or_multiple_tensor_input\ndef _call(self, inputs: Union[Tensor, Iterable[Tensor]], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the max(x, y) operation on either an iterable of input tensors or\n    a single input tensor and a constant.\n\n    Decorated with `@allow_single_or_multiple_tensor_input` to ensure that the input\n    is either a single tensor or an iterable of tensors. Returns this result as a\n    list of tensors for easier use here.\n\n    :param inputs: Single tensor or iterable of tensors to perform the\n    max(x, y) operation on.\n    :returns: The tensor resulting from the max(x, y) operation.\n    \"\"\"\n    if self.max_constant is not None:\n        if len(inputs) &gt; 1:\n            raise ValueError(\"If max_constant is set, cannot have multiple inputs\")\n        cast_input, cast_max_constant = self._force_cast_to_compatible_numeric_type(\n            inputs[0], self.max_constant\n        )\n        return tf.math.maximum(\n            cast_input,\n            cast_max_constant,\n        )\n    else:\n        if not len(inputs) &gt; 1:\n            raise ValueError(\n                \"If max_constant is not set, must have multiple inputs\"\n            )\n        return reduce(tf.math.maximum, inputs)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/max/#src.kamae.tensorflow.layers.max.MaxLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the Max layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>max_constant</code> to the config dictionary.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/max.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the Max layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `max_constant` to the config dictionary.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"max_constant\": self.max_constant})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/mean/","title":"mean","text":""},{"location":"reference/src/kamae/tensorflow/layers/mean/#src.kamae.tensorflow.layers.mean.MeanLayer","title":"MeanLayer","text":"<pre><code>MeanLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    mean_constant=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs the mean(x, y) operation on a given input tensor. If mean_constant is not set, inputs are assumed to be a list of tensors and the mean of all the tensors is computed. If mean_constant is set, inputs must be a tensor.</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param mean_constant: The constant to mean against the input, defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/mean.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    mean_constant: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the Mean layer\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param mean_constant: The constant to mean against the input, defaults\n    to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.mean_constant = mean_constant\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/mean/#src.kamae.tensorflow.layers.mean.MeanLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/mean/#src.kamae.tensorflow.layers.mean.MeanLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the mean(x, y) operation on either an iterable of input tensors or a single input tensor and a constant.</p> <p>Decorated with <code>@allow_single_or_multiple_tensor_input</code> to ensure that the input is either a single tensor or an iterable of tensors. Returns this result as a list of tensors for easier use here.</p> <p>:param inputs: Single tensor or iterable of tensors to perform the mean(x, y) operation on. :returns: The tensor resulting from the mean(x, y) operation.</p> Source code in <code>src/kamae/tensorflow/layers/mean.py</code> <pre><code>@allow_single_or_multiple_tensor_input\ndef _call(self, inputs: Union[Tensor, Iterable[Tensor]], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the mean(x, y) operation on either an iterable of input tensors or\n    a single input tensor and a constant.\n\n    Decorated with `@allow_single_or_multiple_tensor_input` to ensure that the input\n    is either a single tensor or an iterable of tensors. Returns this result as a\n    list of tensors for easier use here.\n\n    :param inputs: Single tensor or iterable of tensors to perform the\n    mean(x, y) operation on.\n    :returns: The tensor resulting from the mean(x, y) operation.\n    \"\"\"\n    if self.mean_constant is not None:\n        if len(inputs) &gt; 1:\n            raise ValueError(\"If mean_constant is set, inputs must be a tensor\")\n        (\n            cast_input,\n            cast_mean_constant,\n        ) = self._force_cast_to_compatible_numeric_type(\n            inputs[0],\n            self.mean_constant,\n        )\n        return tf.truediv(tf.math.add(cast_input, cast_mean_constant), 2)\n    else:\n        if not len(inputs) &gt; 1:\n            raise ValueError(\n                \"If mean_constant is not set, must have multiple inputs\"\n            )\n\n        return tf.truediv(reduce(tf.math.add, inputs), len(inputs))\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/mean/#src.kamae.tensorflow.layers.mean.MeanLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the Mean layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>mean_constant</code> to the config dictionary.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/mean.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the Mean layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `mean_constant` to the config dictionary.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"mean_constant\": self.mean_constant})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/min/","title":"min","text":""},{"location":"reference/src/kamae/tensorflow/layers/min/#src.kamae.tensorflow.layers.min.MinLayer","title":"MinLayer","text":"<pre><code>MinLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    min_constant=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs the min(x, y) operation on a given input tensor. If min_constant is not set, inputs are assumed to be a list of tensors and the min of all the tensors is computed. If min_constant is set, inputs must be a tensor.</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param min_constant: The constant to min against the input, defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/min.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    min_constant: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MinLayer layer\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param min_constant: The constant to min against the input, defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.min_constant = min_constant\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/min/#src.kamae.tensorflow.layers.min.MinLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/min/#src.kamae.tensorflow.layers.min.MinLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the min(x, y) operation on either an iterable of input tensors or a single input tensor and a constant.</p> <p>Decorated with <code>@allow_single_or_multiple_tensor_input</code> to ensure that the input is either a single tensor or an iterable of tensors. Returns this result as a list of tensors for easier use here.</p> <p>:param inputs: Single tensor or iterable of tensors to perform the min(x, y) operation on. :returns: The tensor resulting from the min(x, y) operation.</p> Source code in <code>src/kamae/tensorflow/layers/min.py</code> <pre><code>@allow_single_or_multiple_tensor_input\ndef _call(self, inputs: Union[Tensor, Iterable[Tensor]], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the min(x, y) operation on either an iterable of input tensors or\n    a single input tensor and a constant.\n\n    Decorated with `@allow_single_or_multiple_tensor_input` to ensure that the input\n    is either a single tensor or an iterable of tensors. Returns this result as a\n    list of tensors for easier use here.\n\n    :param inputs: Single tensor or iterable of tensors to perform the\n    min(x, y) operation on.\n    :returns: The tensor resulting from the min(x, y) operation.\n    \"\"\"\n    if self.min_constant is not None:\n        if len(inputs) &gt; 1:\n            raise ValueError(\"If min_constant is set, inputs must be a tensor\")\n        cast_input, cast_min_constant = self._force_cast_to_compatible_numeric_type(\n            inputs[0], self.min_constant\n        )\n        return tf.math.minimum(\n            cast_input,\n            cast_min_constant,\n        )\n    else:\n        if not len(inputs) &gt; 1:\n            raise ValueError(\n                \"If min_constant is not set, must have multiple inputs\"\n            )\n\n        return reduce(tf.math.minimum, inputs)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/min/#src.kamae.tensorflow.layers.min.MinLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the Min layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>min_constant</code> to the config dictionary.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/min.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the Min layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `min_constant` to the config dictionary.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"min_constant\": self.min_constant})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/min_hash_index/","title":"min_hash_index","text":""},{"location":"reference/src/kamae/tensorflow/layers/min_hash_index/#src.kamae.tensorflow.layers.min_hash_index.MinHashIndexLayer","title":"MinHashIndexLayer","text":"<pre><code>MinHashIndexLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    num_permutations=128,\n    mask_value=None,\n    axis=-1,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs min hashing of the input tensor as described here: https://en.wikipedia.org/wiki/MinHash</p> <p>MinHash approximates the Jaccard similarity between sets by hashing the elements of the sets and returning a fixed-length signature. This length is determined by the num_permutations parameter, which defaults to 128. The output is an array of integer bits.</p> <p>Setting the mask_value parameter allows you to ignore a specific value in the input column when computing the min hash. This is useful if you have padded arrays as then a padded array with the same unique elements as another non-padded array will be considered equal.</p> <p>The minimum is computed across the last dimension of the input tensor.</p> <p>:param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param num_permutations: Number of permutations to use for the min hashing.     Defaults to 128. :param mask_value: A value that represents masked inputs, which are ignored when computing the min hash. Defaults to None, meaning no mask term will be added. :param axis: The axis along which to compute the min hash. Defaults to -1 (last axis).</p> Source code in <code>src/kamae/tensorflow/layers/min_hash_index.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    num_permutations: int = 128,\n    mask_value: Optional[str] = None,\n    axis: int = -1,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises the MinHashIndexLayer layer.\n\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param num_permutations: Number of permutations to use for the min hashing.\n        Defaults to 128.\n    :param mask_value: A value that represents masked inputs, which are ignored when\n    computing the min hash. Defaults to None, meaning no mask term will be added.\n    :param axis: The axis along which to compute the min hash.\n    Defaults to -1 (last axis).\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.num_permutations = num_permutations\n    self.axis = axis\n    self.mask_value = mask_value\n    self.hash_fn = Hashing(\n        # Set the number of bins to the maximum integer value. We just want to hash\n        # the input without binning it, so we use the maximum integer value.\n        num_bins=tf.int32.max\n    )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/min_hash_index/#src.kamae.tensorflow.layers.min_hash_index.MinHashIndexLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/min_hash_index/#src.kamae.tensorflow.layers.min_hash_index.MinHashIndexLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the min hash indexing on the input tensor.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param inputs: Input tensor to be encoded. :returns: Encoded tensor.</p> Source code in <code>src/kamae/tensorflow/layers/min_hash_index.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the min hash indexing on the input tensor.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that the input\n    is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n\n    :param inputs: Input tensor to be encoded.\n    :returns: Encoded tensor.\n    \"\"\"\n    min_hash_signature = []\n    for i in range(self.num_permutations):\n        # Salt the input\n        salted_inputs = tf.strings.join(\n            [inputs, tf.zeros_like(inputs)], separator=str(i)\n        )\n        # Hash the salted inputs.\n        if self.mask_value is not None:\n            hashed_inputs = tf.where(\n                tf.equal(salted_inputs, f\"{self.mask_value}{i}\"),\n                # Use the maximum integer value for masked inputs, therefore it is\n                # never selected as the minimum.\n                tf.ones_like(salted_inputs, dtype=tf.int64) * tf.int32.max,\n                self.hash_fn(salted_inputs),\n            )\n        else:\n            hashed_inputs = self.hash_fn(salted_inputs)\n        min_hash_value = tf.reduce_min(hashed_inputs, axis=self.axis, keepdims=True)\n        min_hash_bit = min_hash_value &amp; 1\n        min_hash_signature.append(min_hash_bit)\n\n    # Concatenate the min hash values to form the final signature.\n    return tf.concat(min_hash_signature, axis=self.axis)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/min_hash_index/#src.kamae.tensorflow.layers.min_hash_index.MinHashIndexLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Returns the configuration of the MinHashIndex layer.</p> <p>:returns: Configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/min_hash_index.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Returns the configuration of the MinHashIndex layer.\n\n    :returns: Configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"num_permutations\": self.num_permutations,\n            \"mask_value\": self.mask_value,\n            \"axis\": self.axis,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/min_max_scale/","title":"min_max_scale","text":""},{"location":"reference/src/kamae/tensorflow/layers/min_max_scale/#src.kamae.tensorflow.layers.min_max_scale.MinMaxScaleLayer","title":"MinMaxScaleLayer","text":"<pre><code>MinMaxScaleLayer(\n    min,\n    max,\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    axis=-1,\n    mask_value=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs a min-max scaling operation on the input tensor(s). This is used to standardize/transform the input tensor to the range [0, 1] using the minimum and maximum values.</p> <p>Formula: (x - min)/(max - min)</p> <p>:param max: The max value(s) to use during scaling. :param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param axis: The axis that should have a separate min and max. For example, if shape is <code>(None, 5)</code> and <code>axis=1</code>, the layer will track 5 separate min and max values for the last axis. :param mask_value: Value which should be ignored during scaling.</p> Source code in <code>src/kamae/tensorflow/layers/min_max_scale.py</code> <pre><code>def __init__(\n    self,\n    min: Union[List[float], np.array],\n    max: Union[List[float], np.array],\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    axis: int = -1,\n    mask_value: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Intialise the MinMaxScaleLayer layer.\n    :param min: The min value(s) to use during scaling.\n    :param max: The max value(s) to use during scaling.\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param axis: The axis that should have a separate min and max. For\n    example, if shape is `(None, 5)` and `axis=1`, the layer will track 5\n    separate min and max values for the last axis.\n    :param mask_value: Value which should be ignored during scaling.\n    \"\"\"\n    super().__init__(\n        name=name,\n        input_dtype=input_dtype,\n        output_dtype=output_dtype,\n        **kwargs,\n    )  # Standardize `axis` to a tuple.\n    if axis is None:\n        axis = ()\n    elif isinstance(axis, int):\n        axis = (axis,)\n    else:\n        axis = tuple(axis)\n\n    self.axis = axis\n    self.input_min = min\n    self.input_max = max\n    self.mask_value = mask_value\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/min_max_scale/#src.kamae.tensorflow.layers.min_max_scale.MinMaxScaleLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/min_max_scale/#src.kamae.tensorflow.layers.min_max_scale.MinMaxScaleLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs normalization on the input tensor(s) to scale it to the range [0, 1] Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable. :param inputs: Input tensor to perform the normalization on. :returns: The input tensor with the normalization applied.</p> Source code in <code>src/kamae/tensorflow/layers/min_max_scale.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs normalization on the input tensor(s) to scale it to the range [0, 1]\n    Decorated with `@enforce_single_tensor_input` to ensure that\n    the input is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n    :param inputs: Input tensor to perform the normalization on.\n    :returns: The input tensor with the normalization applied.\n    \"\"\"\n    # Ensure min and max match input dtype.\n    min_tensor = self._cast(self.min, inputs.dtype.name)\n    max_tensor = self._cast(self.max, inputs.dtype.name)\n    normalized_outputs = tf.math.divide_no_nan(\n        tf.math.subtract(inputs, min_tensor),\n        tf.math.subtract(max_tensor, min_tensor),\n    )\n    if self.mask_value is not None:\n        mask = tf.equal(inputs, self.mask_value)\n        normalized_outputs = tf.where(\n            mask, inputs, self._cast(normalized_outputs, inputs.dtype.name)\n        )\n    return normalized_outputs\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/min_max_scale/#src.kamae.tensorflow.layers.min_max_scale.MinMaxScaleLayer.build","title":"build","text":"<pre><code>build(input_shape)\n</code></pre> <p>Builds shapes for the min and max tensors.</p> <p>Specifically, understands which axis to compute the scaling across and broadcasts the min and max tensors to match the input shape.</p> <p>:param input_shape: The shape of the input tensor. :returns: None - layer is built.</p> Source code in <code>src/kamae/tensorflow/layers/min_max_scale.py</code> <pre><code>def build(self, input_shape: Tuple[int]) -&gt; None:\n    \"\"\"\n    Builds shapes for the min and max tensors.\n\n    Specifically, understands which axis to compute the scaling across\n    and broadcasts the min and max tensors to match the input shape.\n\n    :param input_shape: The shape of the input tensor.\n    :returns: None - layer is built.\n    \"\"\"\n    super().build(input_shape)\n\n    if isinstance(input_shape, (list, tuple)) and all(\n        isinstance(shape, (tf.TensorShape, list, tuple)) for shape in input_shape\n    ):\n        # This seems to be needed to handle sending in multiple inputs as a list.\n        # Although this layer should only have one input, so this is a bit of a\n        # hack. We catch this nicely in call method with a decorator. Maybe we\n        # should do the same here?\n        input_shape = input_shape[0]\n\n    input_shape = tf.TensorShape(input_shape).as_list()\n    ndim = len(input_shape)\n    self._build_input_shape = input_shape\n\n    if any(a &lt; -ndim or a &gt;= ndim for a in self.axis):\n        raise ValueError(\n            f\"\"\"All `axis` values must be in the range [-ndim, ndim). \"\n            Found ndim: `{ndim}`, axis: {self.axis}\"\"\"\n        )\n\n    # Axes to be kept, replacing negative values with positive equivalents.\n    # Sorted to avoid transposing axes.\n    keep_axis = sorted([d if d &gt;= 0 else d + ndim for d in self.axis])\n    # All axes to be kept should have known shape.\n    for d in keep_axis:\n        if input_shape[d] is None:\n            raise ValueError(\n                f\"\"\"All `axis` values to be kept must have known shape. \"\n                Got axis: {self.axis},\n                input shape: {input_shape}, with unknown axis at index: {d}\"\"\"\n            )\n    # Broadcast any reduced axes.\n    broadcast_shape = [input_shape[d] if d in keep_axis else 1 for d in range(ndim)]\n    min_and_max_shape = tuple(input_shape[d] for d in keep_axis)\n    min_tensor = self.input_min * np.ones(min_and_max_shape)\n    max_tensor = self.input_max * np.ones(min_and_max_shape)\n    self.min = tf.reshape(min_tensor, broadcast_shape)\n    self.max = tf.reshape(max_tensor, broadcast_shape)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/min_max_scale/#src.kamae.tensorflow.layers.min_max_scale.MinMaxScaleLayer.build_from_config","title":"build_from_config","text":"<pre><code>build_from_config(config)\n</code></pre> <p>Builds the min/max tensor shapes from the provided configuration.</p> <p>Specifically it calls the <code>build</code> method with the input shape in order to construct the min and max tensors with the correct shape.</p> <p>:param config: Configuration dictionary containing the input shape. :returns: None - layer is built.</p> Source code in <code>src/kamae/tensorflow/layers/min_max_scale.py</code> <pre><code>def build_from_config(self, config: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Builds the min/max tensor shapes from the provided configuration.\n\n    Specifically it calls the `build` method with the input shape in order to\n    construct the min and max tensors with the correct shape.\n\n    :param config: Configuration dictionary containing the input shape.\n    :returns: None - layer is built.\n    \"\"\"\n    if config:\n        self.build(config[\"input_shape\"])\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/min_max_scale/#src.kamae.tensorflow.layers.min_max_scale.MinMaxScaleLayer.get_build_config","title":"get_build_config","text":"<pre><code>get_build_config()\n</code></pre> <p>Gets the build configuration of the MinMaxScaleLayer layer.</p> <p>Used for saving and loading from a model.</p> <p>:returns: Dictionary of the build configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/min_max_scale.py</code> <pre><code>def get_build_config(self) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Gets the build configuration of the MinMaxScaleLayer layer.\n\n    Used for saving and loading from a model.\n\n    :returns: Dictionary of the build configuration of the layer.\n    \"\"\"\n    if self._build_input_shape:\n        return {\"input_shape\": self._build_input_shape}\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/min_max_scale/#src.kamae.tensorflow.layers.min_max_scale.MinMaxScaleLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the MinMaxScaleLayer layer. Used for saving and loading from a model. Specifically adds additional parameters to the base configuration. :returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/min_max_scale.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the MinMaxScaleLayer layer.\n    Used for saving and loading from a model.\n    Specifically adds additional parameters to the base configuration.\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    # Ensure mean and variance are lists for serialization.\n    config.update(\n        {\n            \"min\": listify_tensors(self.input_min),\n            \"max\": listify_tensors(self.input_max),\n            \"axis\": self.axis,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/modulo/","title":"modulo","text":""},{"location":"reference/src/kamae/tensorflow/layers/modulo/#src.kamae.tensorflow.layers.modulo.ModuloLayer","title":"ModuloLayer","text":"<pre><code>ModuloLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    divisor=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs the modulo(x, y) operation on a given input tensor. If divisor is not set, inputs are assumed to be a list of two tensors and the first tensor is modulo'd by the second. If divisor is set, inputs must be a tensor.</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param divisor: The divisor to modulo the input by, defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/modulo.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    divisor: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the ModuloLayer layer\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param divisor: The divisor to modulo the input by, defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.divisor = divisor\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/modulo/#src.kamae.tensorflow.layers.modulo.ModuloLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/modulo/#src.kamae.tensorflow.layers.modulo.ModuloLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the modulo(x, y) operation on either an iterable of input tensors or a single input tensor and a constant.</p> <p>Decorated with <code>@allow_single_or_multiple_tensor_input</code> to ensure that the input is either a single tensor or an iterable of tensors. Returns this result as a list of tensors for easier use here.</p> <p>:param inputs: Single tensor or iterable of tensors to perform the modulo(x, y) operation on. :returns: The tensor resulting from the modulo(x, y) operation.</p> Source code in <code>src/kamae/tensorflow/layers/modulo.py</code> <pre><code>@allow_single_or_multiple_tensor_input\ndef _call(self, inputs: Union[Tensor, Iterable[Tensor]], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the modulo(x, y) operation on either an iterable of input tensors or\n    a single input tensor and a constant.\n\n    Decorated with `@allow_single_or_multiple_tensor_input` to ensure that the input\n    is either a single tensor or an iterable of tensors. Returns this result as a\n    list of tensors for easier use here.\n\n    :param inputs: Single tensor or iterable of tensors to perform the\n    modulo(x, y) operation on.\n    :returns: The tensor resulting from the modulo(x, y) operation.\n    \"\"\"\n    if self.divisor is not None:\n        if len(inputs) &gt; 1:\n            raise ValueError(\"If divisor is set, cannot have multiple inputs\")\n        cast_input, cast_divisor = self._force_cast_to_compatible_numeric_type(\n            inputs[0], self.divisor\n        )\n        return tf.math.floormod(\n            cast_input,\n            cast_divisor,\n        )\n    else:\n        if len(inputs) != 2:\n            raise ValueError(\"If divisor is not set, must have exactly 2 inputs\")\n        return tf.math.floormod(inputs[0], inputs[1])\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/modulo/#src.kamae.tensorflow.layers.modulo.ModuloLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the Modulo layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>divisor</code> to the config dictionary.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/modulo.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the Modulo layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `divisor` to the config dictionary.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"divisor\": self.divisor})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/multiply/","title":"multiply","text":""},{"location":"reference/src/kamae/tensorflow/layers/multiply/#src.kamae.tensorflow.layers.multiply.MultiplyLayer","title":"MultiplyLayer","text":"<pre><code>MultiplyLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    multiplier=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs the multiply(x, y) operation on a given input tensor. If multiplier is not set, inputs are assumed to be a list of tensors and multiplied. If multiplier is set, inputs must be a tensor.</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param multiplier: The multiplier to multiply the input by, defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/multiply.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    multiplier: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MultiplyLayer layer\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param multiplier: The multiplier to multiply the input by, defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.multiplier = multiplier\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/multiply/#src.kamae.tensorflow.layers.multiply.MultiplyLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/multiply/#src.kamae.tensorflow.layers.multiply.MultiplyLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the multiply(x, y) operation on either an iterable of input tensors or a single input tensor and a constant.</p> <p>Decorated with <code>@allow_single_or_multiple_tensor_input</code> to ensure that the input is either a single tensor or an iterable of tensors. Returns this result as a list of tensors for easier use here.</p> <p>:param inputs: Single tensor or iterable of tensors to perform the multiply(x, y) operation on. :returns: The tensor resulting from the multiply(x, y) operation.</p> Source code in <code>src/kamae/tensorflow/layers/multiply.py</code> <pre><code>@allow_single_or_multiple_tensor_input\ndef _call(self, inputs: Union[Tensor, Iterable[Tensor]], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the multiply(x, y) operation on either an iterable of input tensors or\n    a single input tensor and a constant.\n\n    Decorated with `@allow_single_or_multiple_tensor_input` to ensure that the input\n    is either a single tensor or an iterable of tensors. Returns this result as a\n    list of tensors for easier use here.\n\n    :param inputs: Single tensor or iterable of tensors to perform the\n    multiply(x, y) operation on.\n    :returns: The tensor resulting from the multiply(x, y) operation.\n    \"\"\"\n    if self.multiplier is not None:\n        if len(inputs) &gt; 1:\n            raise ValueError(\"If multiplier is set, cannot have multiple inputs\")\n        cast_input, cast_multiplier = self._force_cast_to_compatible_numeric_type(\n            inputs[0], self.multiplier\n        )\n        return tf.math.multiply(\n            cast_input,\n            cast_multiplier,\n        )\n    else:\n        if not len(inputs) &gt; 1:\n            raise ValueError(\"If multiplier is not set, must have multiple inputs\")\n\n        return reduce(tf.math.multiply, inputs)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/multiply/#src.kamae.tensorflow.layers.multiply.MultiplyLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the Multiply layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>multiplier</code> to the config dictionary.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/multiply.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the Multiply layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `multiplier` to the config dictionary.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"multiplier\": self.multiplier})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/numerical_if_statement/","title":"numerical_if_statement","text":""},{"location":"reference/src/kamae/tensorflow/layers/numerical_if_statement/#src.kamae.tensorflow.layers.numerical_if_statement.NumericalIfStatementLayer","title":"NumericalIfStatementLayer","text":"<pre><code>NumericalIfStatementLayer(\n    condition_operator,\n    value_to_compare=None,\n    result_if_true=None,\n    result_if_false=None,\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs a numerical if statement on the input tensor, returning a tensor of the same shape as the input tensor.</p> <p>The condition operator can be one of the following: - \"eq\": Equal to - \"neq\": Not equal to - \"lt\": Less than - \"le\": Less than or equal to - \"gt\": Greater than - \"ge\": Greater than or equal to</p> <p>The value to compare must be a float. We will cast the input tensor to a float if it is not already a float.</p> <p>If the condition is true, the result is the result_if_true value. If the condition is false, the result is the result_if_false value.</p> <p>If any of [value_to_compare, result_if_true, result_if_false] are None, we assume they are passed in as inputs to the layer in the above order. If all of them are not None, then inputs is expected to be a tensor.</p> <p>:param condition_operator: Operator to use in the if statement. Can be one of:     - \"eq\": Equal to     - \"neq\": Not equal to     - \"lt\": Less than     - \"leq\": Less than or equal to     - \"gt\": Greater than     - \"geq\": Greater than or equal to :param value_to_compare: Float value to compare the input tensor to. If None, we assume it is passed in as an input to the layer. :param result_if_true: Float value to return if the condition is true. If None, we assume it is passed in as an input to the layer. :param result_if_false: Float value to return if the condition is false. If None, we assume it is passed in as an input to the layer. :param name: The name of the layer. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/numerical_if_statement.py</code> <pre><code>def __init__(\n    self,\n    condition_operator: str,\n    value_to_compare: Optional[float] = None,\n    result_if_true: Optional[float] = None,\n    result_if_false: Optional[float] = None,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises the NumericalIfStatementLayer layer.\n\n    :param condition_operator: Operator to use in the if statement. Can be one of:\n        - \"eq\": Equal to\n        - \"neq\": Not equal to\n        - \"lt\": Less than\n        - \"leq\": Less than or equal to\n        - \"gt\": Greater than\n        - \"geq\": Greater than or equal to\n    :param value_to_compare: Float value to compare the input tensor to. If None, we\n    assume it is passed in as an input to the layer.\n    :param result_if_true: Float value to return if the condition is true. If None,\n    we assume it is passed in as an input to the layer.\n    :param result_if_false: Float value to return if the condition is false. If\n    None, we assume it is passed in as an input to the layer.\n    :param name: The name of the layer. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.condition_operator = condition_operator\n    self.value_to_compare = value_to_compare\n    self.result_if_true = result_if_true\n    self.result_if_false = result_if_false\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/numerical_if_statement/#src.kamae.tensorflow.layers.numerical_if_statement.NumericalIfStatementLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/numerical_if_statement/#src.kamae.tensorflow.layers.numerical_if_statement.NumericalIfStatementLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the numerical if statement on the inputs. If the inputs are a tensor, we assume that the value_to_compare, result_if_true, and result_if_false are provided. If the inputs are not a tensor, we assume any not provided are provided as inputs to the layer.</p> <p>Decorated with <code>@allow_single_or_multiple_tensor_input</code> to ensure that the input is either a single tensor or an iterable of tensors. Returns this result as a list of tensors for easier use here.</p> <p>:param inputs: Tensor or list of tensors. :returns: Tensor after computing the numerical if statement.</p> Source code in <code>src/kamae/tensorflow/layers/numerical_if_statement.py</code> <pre><code>@allow_single_or_multiple_tensor_input\ndef _call(self, inputs: Union[Tensor, Iterable[Tensor]], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the numerical if statement on the inputs. If the inputs are a tensor,\n    we assume that the value_to_compare, result_if_true, and result_if_false are\n    provided. If the inputs are not a tensor, we assume any not provided are\n    provided as inputs to the layer.\n\n    Decorated with `@allow_single_or_multiple_tensor_input` to ensure that the input\n    is either a single tensor or an iterable of tensors. Returns this result as a\n    list of tensors for easier use here.\n\n    :param inputs: Tensor or list of tensors.\n    :returns: Tensor after computing the numerical if statement.\n    \"\"\"\n    condition_op = get_condition_operator(self.condition_operator)\n    if not len(inputs) &gt; 1:\n        # If the input is a tensor, we assume that the value_to_compare,\n        # result_if_true, and result_if_false are provided\n        if any(\n            [\n                v is None\n                for v in [\n                    self.value_to_compare,\n                    self.result_if_true,\n                    self.result_if_false,\n                ]\n            ]\n        ):\n            raise ValueError(\n                \"If inputs is a tensor, value_to_compare, result_if_true, and \"\n                \"result_if_false must be specified.\"\n            )\n        cond = tf.where(\n            condition_op(inputs[0], self.value_to_compare),\n            tf.constant(self.result_if_true, dtype=inputs[0].dtype),\n            tf.constant(self.result_if_false, dtype=inputs[0].dtype),\n        )\n        return cond\n    else:\n        # If the input is a list, we assume that the value_to_compare,\n        # result_if_true, and result_if_false are potentially provided in the inputs\n        input_tensors = self._construct_input_tensors(inputs)\n        cond = tf.where(\n            condition_op(input_tensors[0], input_tensors[1]),\n            input_tensors[2],\n            input_tensors[3],\n        )\n        return cond\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/numerical_if_statement/#src.kamae.tensorflow.layers.numerical_if_statement.NumericalIfStatementLayer._construct_input_tensors","title":"_construct_input_tensors","text":"<pre><code>_construct_input_tensors(inputs)\n</code></pre> <p>Constructs the input tensors for the layer in the case where all the optional parameters are not specified. We need to run through the provided inputs and either select an input or the specified parameter.</p> <p>Specifically for this layer, we assume the inputs are in the following order: [input_tensor, value_to_compare, result_if_true, result_if_false]</p> <p>Any but the input tensor can be None.</p> <p>:param inputs: List of input tensors. :returns: List of input tensors potentially containing constant tensors for the optional parameters.</p> Source code in <code>src/kamae/tensorflow/layers/numerical_if_statement.py</code> <pre><code>def _construct_input_tensors(\n    self, inputs: Iterable[tf.Tensor]\n) -&gt; Iterable[tf.Tensor]:\n    \"\"\"\n    Constructs the input tensors for the layer in the case where all the optional\n    parameters are not specified. We need to run through the provided inputs and\n    either select an input or the specified parameter.\n\n    Specifically for this layer, we assume the inputs are in the following order:\n    [input_tensor, value_to_compare, result_if_true, result_if_false]\n\n    Any but the input tensor can be None.\n\n    :param inputs: List of input tensors.\n    :returns: List of input tensors potentially containing constant tensors for the\n    optional parameters.\n    \"\"\"\n    optional_params = [\n        self.value_to_compare,\n        self.result_if_true,\n        self.result_if_false,\n    ]\n    # Setup the inputs. Keep a counter to know how many tensors from inputs have\n    # been used.\n    input_col_counter = 1\n    # First input is always the input tensor\n    multiple_inputs = [inputs[0]]\n    for param in optional_params:\n        if param is None:\n            # If the param is None, we assume it is an input tensor at the next\n            # index\n            multiple_inputs.append(inputs[input_col_counter])\n            input_col_counter += 1\n        else:\n            # Otherwise, we create a constant tensor for the parameter\n            # and do not increment the counter.\n            multiple_inputs.append(tf.constant(param, dtype=inputs[0].dtype))\n    return multiple_inputs\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/numerical_if_statement/#src.kamae.tensorflow.layers.numerical_if_statement.NumericalIfStatementLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the NumericalIfStatement layer.</p> <p>Specifically adds the following to the base configuration: - condition_operator - value_to_compare - result_if_true - result_if_false</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/numerical_if_statement.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the NumericalIfStatement layer.\n\n    Specifically adds the following to the base configuration:\n    - condition_operator\n    - value_to_compare\n    - result_if_true\n    - result_if_false\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"condition_operator\": self.condition_operator,\n            \"value_to_compare\": self.value_to_compare,\n            \"result_if_true\": self.result_if_true,\n            \"result_if_false\": self.result_if_false,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/one_hot_encode/","title":"one_hot_encode","text":""},{"location":"reference/src/kamae/tensorflow/layers/one_hot_encode/#src.kamae.tensorflow.layers.one_hot_encode.OneHotEncodeLayer","title":"OneHotEncodeLayer","text":"<pre><code>OneHotEncodeLayer(\n    vocabulary,\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    mask_token=None,\n    num_oov_indices=1,\n    drop_unseen=False,\n    encoding=\"utf-8\",\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs a one-hot encoding of a string input tensor.</p> <p>Encodes each individual element in the input into an array the same size as the vocabulary, containing a 1 at the element index. If the last dimension is size 1, will encode on that dimension. If the last dimension is not size 1, will append a new dimension for the encoded output.</p> <p>:param vocabulary: Either an array of strings or a string path to a text file. If passing an array, can pass a tuple, list, 1D numpy array, or 1D tensor containing the string vocbulary terms. If passing a file path, the file should contain one line per term in the vocabulary. :param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param mask_token: A token that represents masked inputs. The token is included in vocabulary and mapped to index 0. If set to None, no mask term will be added. Defaults to <code>None</code>. :param num_oov_indices: The number of out-of-vocabulary indices to use. The out-of-vocabulary indices are used to represent unseen labels and are placed at the beginning of the one-hot encoding. Defaults to 1. :param drop_unseen: Whether to drop unseen label indices. If set to True, the layer will not add an extra dimension for unseen labels in the one-hot encoding. Defaults to False. :param encoding: The text encoding to use to interpret the input strings. Defaults to <code>\"utf-8\"</code>.</p> Source code in <code>src/kamae/tensorflow/layers/one_hot_encode.py</code> <pre><code>def __init__(\n    self,\n    vocabulary: Union[str, List[str]],\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    mask_token: Optional[str] = None,\n    num_oov_indices: int = 1,\n    drop_unseen: bool = False,\n    encoding: str = \"utf-8\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Intialises the OneHotLayer layer.\n\n    :param vocabulary: Either an array of strings or a string path to a\n    text file. If passing an array, can pass a tuple, list, 1D numpy array,\n    or 1D tensor containing the string vocbulary terms. If passing a file\n    path, the file should contain one line per term in the vocabulary.\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param mask_token: A token that represents masked inputs. The token is included\n    in vocabulary and mapped to index 0. If set to None, no mask term will be added.\n    Defaults to `None`.\n    :param num_oov_indices: The number of out-of-vocabulary indices to use. The\n    out-of-vocabulary indices are used to represent unseen labels and are placed at\n    the beginning of the one-hot encoding. Defaults to 1.\n    :param drop_unseen: Whether to drop unseen label indices. If set to True, the\n    layer will not add an extra dimension for unseen labels in the one-hot\n    encoding. Defaults to False.\n    :param encoding: The text encoding to use to interpret the input strings.\n    Defaults to `\"utf-8\"`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.num_oov_indices = num_oov_indices\n    self.vocabulary = vocabulary\n    self.drop_unseen = drop_unseen\n    self.mask_token = mask_token\n    self.encoding = encoding\n    self.lookup_layer = tf.keras.layers.StringLookup(\n        vocabulary=self.vocabulary,\n        output_mode=\"int\",\n        num_oov_indices=self.num_oov_indices,\n        mask_token=self.mask_token,\n        encoding=self.encoding,\n    )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/one_hot_encode/#src.kamae.tensorflow.layers.one_hot_encode.OneHotEncodeLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/one_hot_encode/#src.kamae.tensorflow.layers.one_hot_encode.OneHotEncodeLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the one-hot encoding on the input tensor.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param inputs: Input tensor to one-hot encode. :returns: One-hot encoded input tensor.</p> Source code in <code>src/kamae/tensorflow/layers/one_hot_encode.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the one-hot encoding on the input tensor.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that the input\n    is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n\n    :param inputs: Input tensor to one-hot encode.\n    :returns: One-hot encoded input tensor.\n    \"\"\"\n    casted_inputs = (\n        tf.strings.as_string(inputs, scientific=False)\n        if inputs.dtype != tf.string\n        else inputs\n    )\n    indexed_inputs = self.lookup_layer(casted_inputs)\n    mask_offset = 1 if self.mask_token is not None else 0\n\n    # If last dimension to encode is 1,\n    # remove it after one-hot encoding.\n    # E.g. (None, None, 1) -&gt; (None, None, 1, N) -&gt; (None, None, N)\n    # But (None, None, M) -&gt; (None, None, M, N)\n    ohe_depth = len(self.vocabulary) + self.num_oov_indices + mask_offset\n    encoded_inputs = (\n        tf.squeeze(tf.one_hot(indexed_inputs, ohe_depth), axis=-2)\n        if indexed_inputs.get_shape()[-1] == 1\n        else tf.one_hot(indexed_inputs, ohe_depth)\n    )\n\n    # If drop unseen, slice off the first num_oov_indices + mask_offset columns\n    if self.drop_unseen:\n        encoded_inputs = encoded_inputs[..., (self.num_oov_indices + mask_offset) :]\n\n    return encoded_inputs\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/one_hot_encode/#src.kamae.tensorflow.layers.one_hot_encode.OneHotEncodeLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the OneHot layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>vocabulary</code>, <code>num_oov_indices</code>, <code>mask_token</code>, and <code>encoding</code> to the config.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/one_hot_encode.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the OneHot layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `vocabulary`, `num_oov_indices`, `mask_token`, and\n    `encoding` to the config.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"vocabulary\": self.vocabulary,\n            \"num_oov_indices\": self.num_oov_indices,\n            \"drop_unseen\": self.drop_unseen,\n            \"mask_token\": self.mask_token,\n            \"encoding\": self.encoding,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/ordinal_array_encode/","title":"ordinal_array_encode","text":""},{"location":"reference/src/kamae/tensorflow/layers/ordinal_array_encode/#src.kamae.tensorflow.layers.ordinal_array_encode.OrdinalArrayEncodeLayer","title":"OrdinalArrayEncodeLayer","text":"<pre><code>OrdinalArrayEncodeLayer(\n    pad_value=None,\n    input_dtype=None,\n    output_dtype=None,\n    axis=-1,\n    name=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Transformer that encodes an array of strings into an array of integers.</p> <p>The transformer will map each unique string in the array to an integer, according to the order in which they appear in the array. It will also ignore the pad value if specified.</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param pad_value: The value which pad the array and as a result should be ignored in the encoding process.</p> <p>:returns: None</p> Source code in <code>src/kamae/tensorflow/layers/ordinal_array_encode.py</code> <pre><code>def __init__(\n    self,\n    pad_value: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    axis: int = -1,\n    name: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the OrdinalArrayEncodeLayer layer\n\n    :param name: Name of the layer, defaults to `None`.\n    :param pad_value: The value which pad the array and as a result should be\n    ignored in the encoding process.\n\n    :returns: None\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.pad_value = pad_value\n    self.axis = axis\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/ordinal_array_encode/#src.kamae.tensorflow.layers.ordinal_array_encode.OrdinalArrayEncodeLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/ordinal_array_encode/#src.kamae.tensorflow.layers.ordinal_array_encode.OrdinalArrayEncodeLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the ordinal encoding on the input dataset. Example:  input_tensor = tf.Tensor([     ['a', 'a', 'a', 'b', 'c', '-1', '-1', '-1'],     ['x', 'x', 'x', 'x', 'y', 'z', '-1', '-1'],     ]  )</p> tf.Tensor([[ <p>[0, 0, 0, 1, 2, -1, -1, -1], [0, 0, 0, 0, 1, 2, -1, -1], ]</p> <p>)</p> <p>:param inputs: The input tensor. :returns: Transformed tensor.</p> Source code in <code>src/kamae/tensorflow/layers/ordinal_array_encode.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the ordinal encoding on the input dataset.\n    Example:\n     input_tensor = tf.Tensor([\n        ['a', 'a', 'a', 'b', 'c', '-1', '-1', '-1'],\n        ['x', 'x', 'x', 'x', 'y', 'z', '-1', '-1'],\n        ]\n     )\n\n    Output: tf.Tensor([[\n        [0, 0, 0, 1, 2, -1, -1, -1],\n        [0, 0, 0, 0, 1, 2, -1, -1],\n        ]\n    )\n\n    :param inputs: The input tensor.\n    :returns: Transformed tensor.\n    \"\"\"\n\n    @tf.function\n    def _transform_row(input_row: Tensor) -&gt; Tensor:\n        if self.pad_value is None:\n            converted_tensor = tf.unique(input_row).idx\n        else:\n            not_pad_mask = tf.where(\n                tf.not_equal(input_row, self.pad_value),\n                tf.constant(True),\n                tf.constant(False),\n            )\n            # If all values are the pad value return -1s\n            if not tf.reduce_any(not_pad_mask):\n                converted_tensor = tf.fill(tf.shape(input_row), -1)\n            else:\n                non_pad_values = tf.boolean_mask(input_row, not_pad_mask)\n                first_non_pad_value = non_pad_values[0]\n                replace_pad_with_first = tf.where(\n                    tf.equal(input_row, self.pad_value),\n                    first_non_pad_value,\n                    input_row,\n                )\n                converted_tensor = tf.where(\n                    not_pad_mask,\n                    tf.unique(replace_pad_with_first).idx,\n                    tf.constant(-1),\n                )\n        return self._cast(converted_tensor, cast_dtype=tf.int32.name)\n\n    output = map_fn_w_axis(\n        elems=inputs,\n        fn=_transform_row,\n        axis=self.axis,\n        fn_output_signature=tf.int32,\n    )\n\n    return output\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/ordinal_array_encode/#src.kamae.tensorflow.layers.ordinal_array_encode.OrdinalArrayEncodeLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the OrdinalArrayEncoder layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>pad_value</code> value to the configuration.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/ordinal_array_encode.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the OrdinalArrayEncoder layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `pad_value` value to the configuration.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"pad_value\": self.pad_value, \"axis\": self.axis})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/round/","title":"round","text":""},{"location":"reference/src/kamae/tensorflow/layers/round/#src.kamae.tensorflow.layers.round.RoundLayer","title":"RoundLayer","text":"<pre><code>RoundLayer(\n    round_type=\"round\",\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs a standard rounding operation on the input tensor. Supported rounding types are 'ceil', 'floor' and 'round'.</p> <ul> <li>'ceil' rounds up to the nearest integer.</li> <li>'floor' rounds down to the nearest integer.</li> <li>'round' rounds to the nearest integer.</li> </ul> <p>:param round_type: The type of rounding to perform. Supported types are 'ceil', 'floor' and 'round'. Defaults to 'round'. :param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/round.py</code> <pre><code>def __init__(\n    self,\n    round_type: str = \"round\",\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises the RoundLayer layer.\n\n    :param round_type: The type of rounding to perform.\n    Supported types are 'ceil', 'floor' and 'round'. Defaults to 'round'.\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    if round_type not in [\"ceil\", \"floor\", \"round\"]:\n        raise ValueError(\"\"\"roundType must be one of 'ceil', 'floor' or 'round'.\"\"\")\n    self.round_type = round_type\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/round/#src.kamae.tensorflow.layers.round.RoundLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/round/#src.kamae.tensorflow.layers.round.RoundLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the rounding operation on the input tensor.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param inputs: Input tensor to perform the rounding on. :returns: The input tensor with the rounding applied.</p> Source code in <code>src/kamae/tensorflow/layers/round.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the rounding operation on the input tensor.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that the input is a\n    single tensor. Raises an error if multiple tensors are passed in as an iterable.\n\n    :param inputs: Input tensor to perform the rounding on.\n    :returns: The input tensor with the rounding applied.\n    \"\"\"\n    if self.round_type == \"ceil\":\n        return tf.math.ceil(inputs)\n    elif self.round_type == \"floor\":\n        return tf.math.floor(inputs)\n    elif self.round_type == \"round\":\n        return tf.math.round(inputs)\n    else:\n        raise ValueError(\"\"\"roundType must be one of 'ceil', 'floor' or 'round'.\"\"\")\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/round/#src.kamae.tensorflow.layers.round.RoundLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the Round layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>round_type</code> value to the configuration.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/round.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the Round layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `round_type` value to the configuration.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"round_type\": self.round_type})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/round_to_decimal/","title":"round_to_decimal","text":""},{"location":"reference/src/kamae/tensorflow/layers/round_to_decimal/#src.kamae.tensorflow.layers.round_to_decimal.RoundToDecimalLayer","title":"RoundToDecimalLayer","text":"<pre><code>RoundToDecimalLayer(\n    decimals=1,\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs a rounding to the nearest decimal operation on the input tensor.</p> <p>If the specified number of decimals is too large for the input precision type, this operation can result in overflow. This is because the operation is performed by multiplying the input tensor by 10 to the power of the number of decimals, rounding the result to the nearest integer, and then dividing by 10 to the power of the number of decimals.</p> <p>:param decimals: The number of decimal places to round to. :param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/round_to_decimal.py</code> <pre><code>def __init__(\n    self,\n    decimals: int = 1,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises the RoundToDecimalLayer layer.\n\n    :param decimals: The number of decimal places to round to.\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    if decimals &lt; 0:\n        raise ValueError(\"\"\"decimals must be greater than or equal to 0.\"\"\")\n    self.decimals = decimals\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/round_to_decimal/#src.kamae.tensorflow.layers.round_to_decimal.RoundToDecimalLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/round_to_decimal/#src.kamae.tensorflow.layers.round_to_decimal.RoundToDecimalLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the rounding operation on the input tensor.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param inputs: Input tensor to perform the rounding on. :returns: The input tensor with the rounding applied.</p> Source code in <code>src/kamae/tensorflow/layers/round_to_decimal.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the rounding operation on the input tensor.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that the input is a\n    single tensor. Raises an error if multiple tensors are passed in as an iterable.\n\n    :param inputs: Input tensor to perform the rounding on.\n    :returns: The input tensor with the rounding applied.\n    \"\"\"\n    # WARNING: Depending on the type of the input and the number of decimals,\n    # this multiplier could overflow.\n    max_val = inputs.dtype.max\n    if 10**self.decimals &gt; max_val:\n        raise ValueError(\n            \"\"\"The number of decimals is too large for the input dtype.\n            Overflow expected.\"\"\"\n        )\n    multiplier = tf.constant(10**self.decimals, dtype=inputs.dtype)\n    return tf.round(inputs * multiplier) / multiplier\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/round_to_decimal/#src.kamae.tensorflow.layers.round_to_decimal.RoundToDecimalLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the RoundToDecimal layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>decimals</code> value to the configuration.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/round_to_decimal.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the RoundToDecimal layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `decimals` value to the configuration.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"decimals\": self.decimals})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/standard_scale/","title":"standard_scale","text":""},{"location":"reference/src/kamae/tensorflow/layers/standard_scale/#src.kamae.tensorflow.layers.standard_scale.StandardScaleLayer","title":"StandardScaleLayer","text":"<pre><code>StandardScaleLayer(\n    mean,\n    variance,\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    axis=-1,\n    mask_value=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>NormalizeLayer</code></p> <p>Performs the standard scaling of the input. This layer will shift and scale inputs into a distribution centered around 0 with standard deviation 1. It accomplishes this by precomputing the mean and variance of the data, and calling <code>(input - mean) / sqrt(var)</code> at runtime. mask_value is used to ignore certain values in the standard scaling process. They will remain the same value in the output value as they were in the input value.</p> <p>will be broadcast to the shape of the kept axes above; if the value(s) cannot be broadcast, an error will be raised when this layer's <code>build()</code> method is called. :param variance: The variance value(s) to use during normalization. The passed value(s) will be broadcast to the shape of the kept axes above; if the value(s) cannot be broadcast, an error will be raised when this layer's <code>build()</code> method is called. :param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param axis: Integer, tuple of integers, or None. The axis or axes that should have a separate mean and variance for each index in the shape. For example, if shape is <code>(None, 5)</code> and <code>axis=1</code>, the layer will track 5 separate mean and variance values for the last axis. If <code>axis</code> is set to <code>None</code>, the layer will normalize all elements in the input by a scalar mean and variance. Defaults to -1, where the last axis of the input is assumed to be a feature dimension and is normalized per index. Note that in the specific case of batched scalar inputs where the only axis is the batch axis, the default will normalize each index in the batch separately. In this case, consider passing <code>axis=None</code>. :param mask_value: Value which should be ignored in the standard scaling process and left unchanged.</p> Source code in <code>src/kamae/tensorflow/layers/standard_scale.py</code> <pre><code>def __init__(\n    self,\n    mean: Union[List[float], np.array],\n    variance: Union[List[float], np.array],\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    axis: Optional[Union[int, tuple[int]]] = -1,\n    mask_value: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Intialise the StandardScaleLayer layer.\n    :param mean: The mean value(s) to use during normalization. The passed value(s)\n    will be broadcast to the shape of the kept axes above; if the value(s)\n    cannot be broadcast, an error will be raised when this layer's\n    `build()` method is called.\n    :param variance: The variance value(s) to use during normalization. The passed\n    value(s) will be broadcast to the shape of the kept axes above; if the\n    value(s) cannot be broadcast, an error will be raised when this\n    layer's `build()` method is called.\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param axis: Integer, tuple of integers, or None. The axis or axes that should\n    have a separate mean and variance for each index in the shape. For\n    example, if shape is `(None, 5)` and `axis=1`, the layer will track 5\n    separate mean and variance values for the last axis. If `axis` is set\n    to `None`, the layer will normalize all elements in the input by a\n    scalar mean and variance. Defaults to -1, where the last axis of the\n    input is assumed to be a feature dimension and is normalized per\n    index. Note that in the specific case of batched scalar inputs where\n    the only axis is the batch axis, the default will normalize each index\n    in the batch separately. In this case, consider passing `axis=None`.\n    :param mask_value: Value which should be ignored in the standard scaling\n    process and left unchanged.\n    \"\"\"\n    super().__init__(\n        name=name,\n        mean=mean,\n        variance=variance,\n        axis=axis,\n        input_dtype=input_dtype,\n        output_dtype=output_dtype,\n        **kwargs,\n    )\n    self.mask_value = mask_value\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/standard_scale/#src.kamae.tensorflow.layers.standard_scale.StandardScaleLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs normalization on the input tensor(s) by calling the keras StandardScaleLayer layer. It ignores values which are equal to the mask_value. Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable. :param inputs: Input tensor to perform the normalization on. :returns: The input tensor with the normalization applied.</p> Source code in <code>src/kamae/tensorflow/layers/standard_scale.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs normalization on the input tensor(s) by calling the keras\n    StandardScaleLayer layer. It ignores values which are equal to the\n    mask_value.\n    Decorated with `@enforce_single_tensor_input` to ensure that\n    the input is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n    :param inputs: Input tensor to perform the normalization on.\n    :returns: The input tensor with the normalization applied.\n    \"\"\"\n    # Ensure mean and variance match input dtype.\n    mean = self._cast(self.mean, inputs.dtype.name)\n    variance = self._cast(self.variance, inputs.dtype.name)\n    normalized_outputs = tf.math.divide_no_nan(\n        tf.math.subtract(inputs, mean),\n        tf.math.maximum(tf.sqrt(variance), tf.constant(1e-8, dtype=inputs.dtype)),\n    )\n    if self.mask_value is not None:\n        mask = tf.equal(inputs, self.mask_value)\n        normalized_outputs = tf.where(\n            mask, inputs, self._cast(normalized_outputs, inputs.dtype.name)\n        )\n    return normalized_outputs\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/standard_scale/#src.kamae.tensorflow.layers.standard_scale.StandardScaleLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the StandardScaleLayer layer. Used for saving and loading from a model. Specifically adds additional parameters to the base configuration. :returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/standard_scale.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the StandardScaleLayer layer.\n    Used for saving and loading from a model.\n    Specifically adds additional parameters to the base configuration.\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    # Ensure mean and variance are lists for serialization.\n    config.update(\n        {\n            \"mask_value\": self.mask_value,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_affix/","title":"string_affix","text":""},{"location":"reference/src/kamae/tensorflow/layers/string_affix/#src.kamae.tensorflow.layers.string_affix.StringAffixLayer","title":"StringAffixLayer","text":"<pre><code>StringAffixLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    prefix=None,\n    suffix=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs a prefixing and suffing on the input tensor.</p> <p>:param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param prefix: The prefix to apply to tensor. :param suffix: The suffix to apply to tensor.</p> Source code in <code>src/kamae/tensorflow/layers/string_affix.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    prefix: Optional[str] = None,\n    suffix: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises the String Affix layer.\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param prefix: The prefix to apply to tensor.\n    :param suffix: The suffix to apply to tensor.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.prefix = prefix\n    self.suffix = suffix\n    self.validate_params()\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_affix/#src.kamae.tensorflow.layers.string_affix.StringAffixLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/string_affix/#src.kamae.tensorflow.layers.string_affix.StringAffixLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Prefixes and suffixes a given input tensor.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param inputs: Input tensor to affix. Must be string tensors. :returns: A tensor with affixed values - same shape as input.</p> Source code in <code>src/kamae/tensorflow/layers/string_affix.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Prefixes and suffixes a given input tensor.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that the input\n    is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n\n    :param inputs: Input tensor to affix. Must be string tensors.\n    :returns: A tensor with affixed values - same shape as input.\n    \"\"\"\n    x = inputs\n    if self.prefix:\n        x = tf.strings.join([self.prefix, x])\n    if self.suffix:\n        x = tf.strings.join([x, self.suffix])\n    return x\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_affix/#src.kamae.tensorflow.layers.string_affix.StringAffixLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the StringAffix layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>prefix</code> and <code>suffix</code> values to the configuration.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/string_affix.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the StringAffix layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `prefix` and `suffix` values to the configuration.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"prefix\": self.prefix, \"suffix\": self.suffix})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_affix/#src.kamae.tensorflow.layers.string_affix.StringAffixLayer.validate_params","title":"validate_params","text":"<pre><code>validate_params()\n</code></pre> <p>Validates the parameters of the layer. :raises ValueError: If both prefix and suffix are not set.</p> Source code in <code>src/kamae/tensorflow/layers/string_affix.py</code> <pre><code>def validate_params(self) -&gt; None:\n    \"\"\"\n    Validates the parameters of the layer.\n    :raises ValueError: If both prefix and suffix are not set.\n    \"\"\"\n    if (self.prefix is None or self.prefix == \"\") and (\n        self.suffix is None or self.suffix == \"\"\n    ):\n        raise ValueError(\n            \"Either prefix or suffix must be set. Otherwise nothing to affix.\"\n        )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_array_constant/","title":"string_array_constant","text":""},{"location":"reference/src/kamae/tensorflow/layers/string_array_constant/#src.kamae.tensorflow.layers.string_array_constant.StringArrayConstantLayer","title":"StringArrayConstantLayer","text":"<pre><code>StringArrayConstantLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    constant_string_array=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Tensorflow keras layer that outputs a constant string array.</p> <p>:param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param constant_string_array: The constant string array to output.</p> Source code in <code>src/kamae/tensorflow/layers/string_array_constant.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    constant_string_array: Optional[List[str]] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises the String Array Constant layer.\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param constant_string_array: The constant string array to output.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.constant_string_array = constant_string_array\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_array_constant/#src.kamae.tensorflow.layers.string_array_constant.StringArrayConstantLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/string_array_constant/#src.kamae.tensorflow.layers.string_array_constant.StringArrayConstantLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Returns the constant string array with the same shape as the input tensor.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param inputs: Tensor to replicate shape of for constant string array. :returns: A tensor with the constant string array</p> Source code in <code>src/kamae/tensorflow/layers/string_array_constant.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Returns the constant string array with the same shape as the input tensor.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that the input\n    is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n\n    :param inputs: Tensor to replicate shape of for constant string array.\n    :returns: A tensor with the constant string array\n    \"\"\"\n    input_shape = tf.shape(inputs)\n    string_tensor = tf.constant(self.constant_string_array)\n    broadcast_shape = tf.concat(\n        [input_shape[:-1], [tf.size(string_tensor)]], axis=0\n    )\n    broadcasted_strings = tf.broadcast_to(string_tensor, broadcast_shape)\n    return broadcasted_strings\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_array_constant/#src.kamae.tensorflow.layers.string_array_constant.StringArrayConstantLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the StringArrayConstant layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>constant_string_array</code> to the config.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/string_array_constant.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the StringArrayConstant layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `constant_string_array` to the config.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"constant_string_array\": self.constant_string_array})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_case/","title":"string_case","text":""},{"location":"reference/src/kamae/tensorflow/layers/string_case/#src.kamae.tensorflow.layers.string_case.StringCaseLayer","title":"StringCaseLayer","text":"<pre><code>StringCaseLayer(\n    string_case_type=\"lower\",\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs a string case transform on the input tensor. Supported string case types are 'upper' and 'lower'.</p> <p>:param string_case_type: The type of string case transform to perform. Supported types are 'upper' and 'lower'. Defaults to 'lower'. :param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/string_case.py</code> <pre><code>def __init__(\n    self,\n    string_case_type: str = \"lower\",\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises the StringCaseLayer layer.\n\n    :param string_case_type: The type of string case transform to perform.\n    Supported types are 'upper' and 'lower'. Defaults to 'lower'.\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.string_case_type = string_case_type\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_case/#src.kamae.tensorflow.layers.string_case.StringCaseLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/string_case/#src.kamae.tensorflow.layers.string_case.StringCaseLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the string case transform on the input tensor.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param inputs: Input tensor to perform the string case transform on. :returns: The input tensor with the string case transform applied.</p> Source code in <code>src/kamae/tensorflow/layers/string_case.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the string case transform on the input tensor.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that the input is a\n    single tensor. Raises an error if multiple tensors are passed in as an iterable.\n\n    :param inputs: Input tensor to perform the string case transform on.\n    :returns: The input tensor with the string case transform applied.\n    \"\"\"\n    if self.string_case_type == \"upper\":\n        return tf.strings.upper(inputs)\n    elif self.string_case_type == \"lower\":\n        return tf.strings.lower(inputs)\n    else:\n        raise ValueError(\n            f\"\"\"stringCaseType must be one of 'upper' or 'lower'.\n            Got {self.string_case_type}\"\"\"\n        )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_case/#src.kamae.tensorflow.layers.string_case.StringCaseLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the StringCase layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>string_case_type</code> value to the configuration.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/string_case.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the StringCase layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `string_case_type` value to the configuration.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"string_case_type\": self.string_case_type})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_concatenate/","title":"string_concatenate","text":""},{"location":"reference/src/kamae/tensorflow/layers/string_concatenate/#src.kamae.tensorflow.layers.string_concatenate.StringConcatenateLayer","title":"StringConcatenateLayer","text":"<pre><code>StringConcatenateLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    separator=\"_\",\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs a concatenation of the input tensors.</p> <p>:param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param separator: The separator to use when joining the input tensors.</p> Source code in <code>src/kamae/tensorflow/layers/string_concatenate.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    separator: str = \"_\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises the Concat layer.\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param separator: The separator to use when joining the input tensors.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.separator = separator\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_concatenate/#src.kamae.tensorflow.layers.string_concatenate.StringConcatenateLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/string_concatenate/#src.kamae.tensorflow.layers.string_concatenate.StringConcatenateLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Concatenates the input tensors.</p> <p>Decorated with <code>@enforce_multiple_tensor_input</code> to ensure that the input is an iterable of multiple tensors. Raises an error if a single tensor is passed in.</p> <p>:param inputs: Input tensors that will be concatenated on the last axis. Must be string tensors. :returns: A tensor with the concatenated values - same shape as each of the input tensors.</p> Source code in <code>src/kamae/tensorflow/layers/string_concatenate.py</code> <pre><code>@enforce_multiple_tensor_input\ndef _call(self, inputs: Iterable[Tensor], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Concatenates the input tensors.\n\n    Decorated with `@enforce_multiple_tensor_input` to ensure that the input is an\n    iterable of multiple tensors. Raises an error if a single tensor is passed in.\n\n    :param inputs: Input tensors that will be concatenated on the last axis.\n    Must be string tensors.\n    :returns: A tensor with the concatenated values - same shape as each of\n    the input tensors.\n    \"\"\"\n    return tf.strings.join(inputs, separator=self.separator)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_concatenate/#src.kamae.tensorflow.layers.string_concatenate.StringConcatenateLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the StringConcatenate layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>separator</code> to the config.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/string_concatenate.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the StringConcatenate layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `separator` to the config.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"separator\": self.separator})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_contains/","title":"string_contains","text":""},{"location":"reference/src/kamae/tensorflow/layers/string_contains/#src.kamae.tensorflow.layers.string_contains.StringContainsLayer","title":"StringContainsLayer","text":"<pre><code>StringContainsLayer(\n    string_constant=None,\n    negation=False,\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs a string contains operation on the input tensor, matching against a string constant or element-wise against a second input tensor. WARNING: While it works, the use of tensors in matching/replacement     is not recommended due to the complexity of the regex matching which requires     use of a map_fn. This will be comparatively VERY slow and may not be suitable     for inference use-cases.     If you know where in the string the match is, you will be much     better off slicing the string and checking for equality. This implementation will only match an empty string with another empty string and does not support matching of newline characters.</p> <p>:param negation: Whether to negate the output. Defaults to <code>False</code>. :param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/string_contains.py</code> <pre><code>def __init__(\n    self,\n    string_constant: Optional[str] = None,\n    negation: bool = False,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises the StringContainsLayer layer.\n    :param string_constant: The string to match against. Defaults to `None`.\n    :param negation: Whether to negate the output. Defaults to `False`.\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.negation = negation\n    self.string_constant = string_constant\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_contains/#src.kamae.tensorflow.layers.string_contains.StringContainsLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/string_contains/#src.kamae.tensorflow.layers.string_contains.StringContainsLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Checks for the existence of a substring/pattern within a tensor. WARNING: While it works, the use of tensors in matching is not recommended due to the complexity of the regex matching which requires use of a map_fn. This will be comparatively VERY slow and may not be suitable for inference use-cases. If you know where in the string the match is, you will be much better off slicing the string and checking for equality.</p> <p>Decorated with <code>@allow_single_or_multiple_tensor_input</code> to ensure that the input is either a single tensor or an iterable of tensors. Returns this result as a list of tensors for easier use here.</p> <p>:param: inputs: A string tensor or iterable of up to two string tensors.     In the case two tensors are passed, require that the first tensor is the     tensor to match a pattern/substring against. :returns: A boolean tensor whether the string/string elements are matched.</p> Source code in <code>src/kamae/tensorflow/layers/string_contains.py</code> <pre><code>@allow_single_or_multiple_tensor_input\ndef _call(self, inputs: Union[Tensor, Iterable[Tensor]], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Checks for the existence of a substring/pattern within a tensor.\n    WARNING: While it works, the use of tensors in matching\n    is not recommended due to the complexity of the regex matching which requires\n    use of a map_fn. This will be comparatively VERY slow and may not be suitable\n    for inference use-cases.\n    If you know where in the string the match is, you will be much\n    better off slicing the string and checking for equality.\n\n    Decorated with `@allow_single_or_multiple_tensor_input` to ensure that the input\n    is either a single tensor or an iterable of tensors. Returns this result as a\n    list of tensors for easier use here.\n\n    :param: inputs: A string tensor or iterable of up to two string tensors.\n        In the case two tensors are passed, require that the first tensor is the\n        tensor to match a pattern/substring against.\n    :returns: A boolean tensor whether the string/string elements are matched.\n    \"\"\"\n\n    match_all_pattern = \".*\"\n\n    # Checking input\n    if self.string_constant is not None:\n        if len(inputs) == 1:\n            # To preserve shape, need to pass tensor to regex_full_match\n            input_tensor = inputs[0]\n\n            match_substring = self.string_constant\n            match_substring = self._escape_special_characters(match_substring)\n            matched_tensor = tf.strings.regex_full_match(\n                input_tensor,\n                tf.constant(\n                    match_all_pattern + match_substring + match_all_pattern\n                    if match_substring != \"\"\n                    else \"^$\"\n                ),\n            )\n        else:\n            raise ValueError(\n                \"With string_constant defined, expected a single tensor as input.\"\n            )\n    else:\n        if len(inputs) != 2:\n            raise ValueError(\n                \"Expected iterable of tensors of length 2, \\\n                 or string_constant to be defined.\"\n            )\n\n        # Two tensors provided\n        @tf.function\n        def tensor_match(x: List[Tensor]) -&gt; Tensor:\n            match_substring = x[1]\n            match_substring = self._escape_special_characters(match_substring)\n            return tf.strings.regex_full_match(\n                x[0],\n                match_all_pattern + match_substring + match_all_pattern\n                if x[1] != \"\"\n                else \"^$\",\n            )\n\n        # Stack inputs to match element-wise with map_fn\n        # Requires ordering of inputs to be correct\n        stacked_inputs = tf.stack(inputs, axis=-1)\n        input_shape = tf.shape(inputs[0])\n\n        mappable_tensor = tf.reshape(stacked_inputs, [-1, 2])\n\n        # Apply element-wise matching\n        # TODO: tf.vectorized_map may be slightly faster with larger batches\n        #  but this requires some refactoring\n        matched_tensor = tf.map_fn(\n            fn=tensor_match, elems=mappable_tensor, dtype=tf.bool\n        )\n\n        matched_tensor = tf.reshape(matched_tensor, input_shape)\n\n    output_tensor = (\n        tf.math.logical_not(matched_tensor) if self.negation else matched_tensor\n    )\n\n    return output_tensor\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_contains/#src.kamae.tensorflow.layers.string_contains.StringContainsLayer._escape_special_characters","title":"_escape_special_characters","text":"<pre><code>_escape_special_characters(string)\n</code></pre> <p>Escapes special characters in a string so they are not parsed as regex. :param string: The string or string tensor to escape special characters in. :returns: The escaped string or string tensor.</p> Source code in <code>src/kamae/tensorflow/layers/string_contains.py</code> <pre><code>def _escape_special_characters(\n    self, string: Union[str, Tensor]\n) -&gt; Union[str, Tensor]:\n    \"\"\"\n    Escapes special characters in a string so they are not parsed as regex.\n    :param string: The string or string tensor to escape special characters in.\n    :returns: The escaped string or string tensor.\n    \"\"\"\n    escaped_string = string\n    for char in [\n        \"\\\\\",\n        \".\",\n        \"^\",\n        \"$\",\n        \"*\",\n        \"+\",\n        \"?\",\n        \"{\",\n        \"}\",\n        \"[\",\n        \"]\",\n        \"(\",\n        \")\",\n        \"|\",\n    ]:\n        if isinstance(escaped_string, str):\n            escaped_string = escaped_string.replace(char, \"\\\\\" + char)\n        else:\n            escaped_string = tf.strings.regex_replace(\n                escaped_string, \"\\\\\" + char, \"\\\\\" + char\n            )\n    return escaped_string\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_contains/#src.kamae.tensorflow.layers.string_contains.StringContainsLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the StringContains layer. Used for saving and loading from a model.</p> <p>Specifically adds the string_constant and negation parameters to the config dictionary.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/string_contains.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the StringContains layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the string_constant and negation parameters to the config\n    dictionary.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\"string_constant\": self.string_constant, \"negation\": self.negation}\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_contains_list/","title":"string_contains_list","text":""},{"location":"reference/src/kamae/tensorflow/layers/string_contains_list/#src.kamae.tensorflow.layers.string_contains_list.StringContainsListLayer","title":"StringContainsListLayer","text":"<pre><code>StringContainsListLayer(\n    string_constant_list,\n    negation=False,\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs a string contains operation on the input tensor over entries in the string constant list.</p> <p>This implementation does not support matching of newline characters or empty strings.</p> <p>:param negation: Whether to negate the output. Defaults to <code>False</code>. :param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/string_contains_list.py</code> <pre><code>def __init__(\n    self,\n    string_constant_list: List[str],\n    negation: bool = False,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises the StringContainsListLayer layer.\n    :param string_constant_list: The string to match against.\n    :param negation: Whether to negate the output. Defaults to `False`.\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.negation = negation\n    self.string_constant_list = string_constant_list\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_contains_list/#src.kamae.tensorflow.layers.string_contains_list.StringContainsListLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/string_contains_list/#src.kamae.tensorflow.layers.string_contains_list.StringContainsListLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Checks for the existence of any substring in the string_contains_list within a tensor.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param: inputs: Input string tensor. :returns: A boolean tensor indicating whether any of the string constants are matched.</p> Source code in <code>src/kamae/tensorflow/layers/string_contains_list.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Checks for the existence of any substring in the string_contains_list\n    within a tensor.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that the input\n    is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n\n    :param: inputs: Input string tensor.\n    :returns: A boolean tensor indicating whether any of the string constants are\n    matched.\n    \"\"\"\n    match_substring = \"|\".join(\n        [\n            \"(.*\" + self._escape_special_characters(x) + \".*)\"\n            for x in self.string_constant_list\n        ]\n    )\n    matched_tensor = tf.strings.regex_full_match(\n        inputs,\n        match_substring,\n    )\n\n    output_tensor = (\n        tf.math.logical_not(matched_tensor) if self.negation else matched_tensor\n    )\n\n    return output_tensor\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_contains_list/#src.kamae.tensorflow.layers.string_contains_list.StringContainsListLayer._escape_special_characters","title":"_escape_special_characters","text":"<pre><code>_escape_special_characters(string)\n</code></pre> <p>Escapes special characters in a string so they are not parsed as regex. :param string: The string or string tensor to escape special characters in. :returns: The escaped string or string tensor.</p> Source code in <code>src/kamae/tensorflow/layers/string_contains_list.py</code> <pre><code>def _escape_special_characters(self, string: str) -&gt; str:\n    \"\"\"\n    Escapes special characters in a string so they are not parsed as regex.\n    :param string: The string or string tensor to escape special characters in.\n    :returns: The escaped string or string tensor.\n    \"\"\"\n    escaped_string = string\n    for char in [\n        \"\\\\\",\n        \".\",\n        \"^\",\n        \"$\",\n        \"*\",\n        \"+\",\n        \"?\",\n        \"{\",\n        \"}\",\n        \"[\",\n        \"]\",\n        \"(\",\n        \")\",\n        \"|\",\n    ]:\n        if isinstance(escaped_string, str):\n            escaped_string = escaped_string.replace(char, \"\\\\\" + char)\n        else:\n            escaped_string = tf.strings.regex_replace(\n                escaped_string, \"\\\\\" + char, \"\\\\\" + char\n            )\n    return escaped_string\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_contains_list/#src.kamae.tensorflow.layers.string_contains_list.StringContainsListLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the StringContainsList layer. Used for saving and loading from a model.</p> <p>Specifically adds the string_constant_list and negation parameters to the config dictionary.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/string_contains_list.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the StringContainsList layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the string_constant_list and negation parameters to the\n    config dictionary.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"string_constant_list\": self.string_constant_list,\n            \"negation\": self.negation,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_equals_if_statement/","title":"string_equals_if_statement","text":""},{"location":"reference/src/kamae/tensorflow/layers/string_equals_if_statement/#src.kamae.tensorflow.layers.string_equals_if_statement.StringEqualsIfStatementLayer","title":"StringEqualsIfStatementLayer","text":"<pre><code>StringEqualsIfStatementLayer(\n    value_to_compare=None,\n    result_if_true=None,\n    result_if_false=None,\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs a string if equals statement on the input tensor, returning a tensor of the same shape as the input tensor.</p> <p>The value to compare must be a string. We will cast the input tensor to a string if it is not already a string. This could cause unexpected behaviour if the input tensor is not a string.</p> <p>If the condition is true, the result is the result_if_true value. If the condition is false, the result is the result_if_false value.</p> <p>If any of [value_to_compare, result_if_true, result_if_false] are None, we assume they are passed in as inputs to the layer in the above order. If all of them are not None, then inputs is expected to be a tensor.</p> <p>:param value_to_compare: String value to compare the input tensor to. If None, we assume it is passed in as an input to the layer. :param result_if_true: String value to return if the condition is true. If None, we assume it is passed in as an input to the layer. :param result_if_false: String value to return if the condition is false. If None, we assume it is passed in as an input to the layer. :param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/string_equals_if_statement.py</code> <pre><code>def __init__(\n    self,\n    value_to_compare: Optional[str] = None,\n    result_if_true: Optional[str] = None,\n    result_if_false: Optional[str] = None,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises the StringIfEqualStatement layer.\n\n    :param value_to_compare: String value to compare the input tensor to.\n    If None, we assume it is passed in as an input to the layer.\n    :param result_if_true: String value to return if the condition is true.\n    If None, we assume it is passed in as an input to the layer.\n    :param result_if_false: String value to return if the condition is false.\n    If None, we assume it is passed in as an input to the layer.\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.value_to_compare = value_to_compare\n    self.result_if_true = result_if_true\n    self.result_if_false = result_if_false\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_equals_if_statement/#src.kamae.tensorflow.layers.string_equals_if_statement.StringEqualsIfStatementLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/string_equals_if_statement/#src.kamae.tensorflow.layers.string_equals_if_statement.StringEqualsIfStatementLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the string if equals statement on the inputs. If the inputs are a tensor, we assume that the value_to_compare, result_if_true, and result_if_false are provided. If the inputs are not a tensor, we assume any not provided are provided as inputs to the layer.</p> <p>Decorated with <code>@allow_single_or_multiple_tensor_input</code> to ensure that the input is either a single tensor or an iterable of tensors. Returns this result as a list of tensors for easier use here.</p> <p>:param inputs: Tensor or iterable of tensors. :returns: Tensor after computing the string if equal statement.</p> Source code in <code>src/kamae/tensorflow/layers/string_equals_if_statement.py</code> <pre><code>@allow_single_or_multiple_tensor_input\ndef _call(self, inputs: Union[Tensor, Iterable[Tensor]], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the string if equals statement on the inputs. If the inputs are a\n    tensor, we assume that the value_to_compare, result_if_true, and\n    result_if_false are provided. If the inputs are not a tensor, we assume any\n    not provided are provided as inputs to the layer.\n\n    Decorated with `@allow_single_or_multiple_tensor_input` to ensure that the input\n    is either a single tensor or an iterable of tensors. Returns this result as a\n    list of tensors for easier use here.\n\n    :param inputs: Tensor or iterable of tensors.\n    :returns: Tensor after computing the string if equal statement.\n    \"\"\"\n    if len(inputs) == 1:\n        # If the input is a tensor, we assume that the value_to_compare,\n        # result_if_true, and result_if_false are provided\n        if any(\n            [\n                v is None\n                for v in [\n                    self.value_to_compare,\n                    self.result_if_true,\n                    self.result_if_false,\n                ]\n            ]\n        ):\n            raise ValueError(\n                \"If inputs is a tensor, value_to_compare, result_if_true, and \"\n                \"result_if_false must be specified.\"\n            )\n        string_inputs = (\n            tf.strings.as_string(inputs[0])\n            if inputs[0].dtype != tf.string\n            else inputs[0]\n        )\n        cond = tf.where(\n            string_inputs == self.value_to_compare,\n            tf.constant(self.result_if_true, dtype=tf.string),\n            tf.constant(self.result_if_false, dtype=tf.string),\n        )\n        return cond\n    else:\n        # If the input is a list, we assume that the value_to_compare,\n        # result_if_true, and result_if_false are potentially provided in the inputs\n        string_inputs = [\n            tf.strings.as_string(i) if i.dtype != tf.string else i for i in inputs\n        ]\n        input_tensors = self._construct_input_tensors(string_inputs)\n        cond = tf.where(\n            input_tensors[0] == input_tensors[1],\n            input_tensors[2],\n            input_tensors[3],\n        )\n        return cond\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_equals_if_statement/#src.kamae.tensorflow.layers.string_equals_if_statement.StringEqualsIfStatementLayer._construct_input_tensors","title":"_construct_input_tensors","text":"<pre><code>_construct_input_tensors(inputs)\n</code></pre> <p>Constructs the input tensors for the layer in the case where all the optional parameters are not specified. We need to run through the provided inputs and either select an input or the specified parameter.</p> <p>Specifically for this layer, we assume the inputs are in the following order: [input_tensor, value_to_compare, result_if_true, result_if_false]</p> <p>Any but the input tensor can be None.</p> <p>:param inputs: List of input tensors. :returns: List of input tensors potentially containing constant tensors for the optional parameters.</p> Source code in <code>src/kamae/tensorflow/layers/string_equals_if_statement.py</code> <pre><code>def _construct_input_tensors(self, inputs: List[Tensor]) -&gt; List[Tensor]:\n    \"\"\"\n    Constructs the input tensors for the layer in the case where all the optional\n    parameters are not specified. We need to run through the provided inputs and\n    either select an input or the specified parameter.\n\n    Specifically for this layer, we assume the inputs are in the following order:\n    [input_tensor, value_to_compare, result_if_true, result_if_false]\n\n    Any but the input tensor can be None.\n\n    :param inputs: List of input tensors.\n    :returns: List of input tensors potentially containing constant tensors for the\n    optional parameters.\n    \"\"\"\n    optional_params = [\n        self.value_to_compare,\n        self.result_if_true,\n        self.result_if_false,\n    ]\n    # Setup the inputs. Keep a counter to know how many tensors from inputs have\n    # been used.\n    input_col_counter = 1\n    # First input is always the input tensor\n    multiple_inputs = [inputs[0]]\n    for param in optional_params:\n        if param is None:\n            # If the param is None, we assume it is an input tensor at the next\n            # index\n            multiple_inputs.append(inputs[input_col_counter])\n            input_col_counter += 1\n        else:\n            # Otherwise, we create a constant tensor for the parameter\n            # and do not increment the counter.\n            multiple_inputs.append(tf.constant(param, dtype=tf.string))\n    return multiple_inputs\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_equals_if_statement/#src.kamae.tensorflow.layers.string_equals_if_statement.StringEqualsIfStatementLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the StringEqualsIfStatement layer. Used for saving and loading from a model.</p> <p>Specifically adds the following to the config dictionary: - value_to_compare - result_if_true - result_if_false</p> <p>:returns: Dictionary configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/string_equals_if_statement.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the StringEqualsIfStatement layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the following to the config dictionary:\n    - value_to_compare\n    - result_if_true\n    - result_if_false\n\n    :returns: Dictionary configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"value_to_compare\": self.value_to_compare,\n            \"result_if_true\": self.result_if_true,\n            \"result_if_false\": self.result_if_false,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_index/","title":"string_index","text":""},{"location":"reference/src/kamae/tensorflow/layers/string_index/#src.kamae.tensorflow.layers.string_index.StringIndexLayer","title":"StringIndexLayer","text":"<pre><code>StringIndexLayer(\n    vocabulary,\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    num_oov_indices=1,\n    mask_token=None,\n    encoding=\"utf-8\",\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Wrapper around the Keras StringLookup layer.</p> <p>This layer translates a set of arbitrary strings into integer output via a table-based vocabulary lookup. This layer will perform no splitting or transformation of input strings.</p> <p>:param vocabulary: Either an array of strings or a string path to a text file. If passing an array, can pass a tuple, list, 1D numpy array, or 1D tensor containing the string vocbulary terms. If passing a file path, the file should contain one line per term in the vocabulary. :param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param num_oov_indices: The number of out-of-vocabulary tokens to use. If this value is more than 1, OOV inputs are hashed to determine their OOV value. If this value is 0, OOV inputs will cause an error when calling the layer.  Defaults to 1. :param mask_token: A token that represents masked inputs. The token is included in vocabulary and mapped to index 0. If set to None, no mask term will be added. Defaults to <code>None</code>. :param encoding: Optional. The text encoding to use to interpret the input strings. Defaults to <code>\"utf-8\"</code>.</p> Source code in <code>src/kamae/tensorflow/layers/string_index.py</code> <pre><code>def __init__(\n    self,\n    vocabulary: Union[str, List[str]],\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    num_oov_indices: int = 1,\n    mask_token: Optional[str] = None,\n    encoding: str = \"utf-8\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Intialise the StringIndexLayer layer.\n\n    :param vocabulary: Either an array of strings or a string path to a\n    text file. If passing an array, can pass a tuple, list, 1D numpy array,\n    or 1D tensor containing the string vocbulary terms. If passing a file\n    path, the file should contain one line per term in the vocabulary.\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param num_oov_indices: The number of out-of-vocabulary tokens to use. If this\n    value is more than 1, OOV inputs are hashed to determine their OOV\n    value. If this value is 0, OOV inputs will cause an error when calling\n    the layer.  Defaults to 1.\n    :param mask_token: A token that represents masked inputs. The token is included\n    in vocabulary and mapped to index 0. If set to None, no mask term will be added.\n    Defaults to `None`.\n    :param encoding: Optional. The text encoding to use to interpret the input\n    strings. Defaults to `\"utf-8\"`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.vocabulary = vocabulary\n    self.num_oov_indices = num_oov_indices\n    self.mask_token = mask_token\n    self.encoding = encoding\n    self.indexer = StringLookup(\n        vocabulary=vocabulary,\n        num_oov_indices=num_oov_indices,\n        mask_token=mask_token,\n        encoding=encoding,\n    )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_index/#src.kamae.tensorflow.layers.string_index.StringIndexLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/string_index/#src.kamae.tensorflow.layers.string_index.StringIndexLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs string indexing by calling the StringLookup layer.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param inputs: Input string tensor to index. :returns: Indexed tensor.</p> Source code in <code>src/kamae/tensorflow/layers/string_index.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs string indexing by calling the StringLookup layer.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that the input\n    is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n\n    :param inputs: Input string tensor to index.\n    :returns: Indexed tensor.\n    \"\"\"\n    return self.indexer(inputs)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_index/#src.kamae.tensorflow.layers.string_index.StringIndexLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the StringIndexer layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>vocabulary</code>, <code>num_oov_indices</code>, <code>mask_token</code>, and <code>encoding</code> to the config.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/string_index.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the StringIndexer layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `vocabulary`, `num_oov_indices`, `mask_token`, and\n    `encoding` to the config.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"vocabulary\": self.vocabulary,\n            \"num_oov_indices\": self.num_oov_indices,\n            \"mask_token\": self.mask_token,\n            \"encoding\": self.encoding,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_isin_list/","title":"string_isin_list","text":""},{"location":"reference/src/kamae/tensorflow/layers/string_isin_list/#src.kamae.tensorflow.layers.string_isin_list.StringIsInListLayer","title":"StringIsInListLayer","text":"<pre><code>StringIsInListLayer(\n    string_constant_list,\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    negation=False,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs a string isin operation on the input tensor over entries in the string constant list.</p> <p>:param negation: Whether to negate the output. Defaults to <code>False</code>. :param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/string_isin_list.py</code> <pre><code>def __init__(\n    self,\n    string_constant_list: List[str],\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    negation: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises the StringIsInListLayer layer.\n    :param string_constant_list: The string to match against.\n    :param negation: Whether to negate the output. Defaults to `False`.\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.negation = negation\n    self.string_constant_list = string_constant_list\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_isin_list/#src.kamae.tensorflow.layers.string_isin_list.StringIsInListLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/string_isin_list/#src.kamae.tensorflow.layers.string_isin_list.StringIsInListLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Checks if the input tensor is matching any string in the string_constant_list.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param: inputs: Input string tensor. :returns: A boolean tensor indicating whether any of the string is matched.</p> Source code in <code>src/kamae/tensorflow/layers/string_isin_list.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Checks if the input tensor is matching any string in the string_constant_list.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that the input\n    is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n\n    :param: inputs: Input string tensor.\n    :returns: A boolean tensor indicating whether any of the string is matched.\n    \"\"\"\n    strings = tf.constant(self.string_constant_list)\n    tile_multiples = tf.concat(\n        [tf.ones(tf.rank(inputs), dtype=tf.int32), tf.shape(strings)],\n        axis=0,\n    )\n    x_tile = tf.tile(tf.expand_dims(inputs, -1), tile_multiples)\n    matched_tensor = tf.reduce_any(tf.equal(x_tile, strings), -1)\n    output_tensor = (\n        tf.math.logical_not(matched_tensor) if self.negation else matched_tensor\n    )\n    return output_tensor\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_isin_list/#src.kamae.tensorflow.layers.string_isin_list.StringIsInListLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the StringIsInListLayer layer. Used for saving and loading from a model.</p> <p>Specifically adds the string_constant_list and negation parameters to the config dictionary.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/string_isin_list.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the StringIsInListLayer layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the string_constant_list and negation parameters to the\n    config dictionary.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"string_constant_list\": self.string_constant_list,\n            \"negation\": self.negation,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_list_to_string/","title":"string_list_to_string","text":""},{"location":"reference/src/kamae/tensorflow/layers/string_list_to_string/#src.kamae.tensorflow.layers.string_list_to_string.StringListToStringLayer","title":"StringListToStringLayer","text":"<pre><code>StringListToStringLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    axis=-1,\n    separator=\"\",\n    keepdims=False,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>A layer that converts a list of strings to a single string along the specified axis. If <code>keepdims</code> is <code>True</code>, the shape is retained.</p> <p>:param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param axis: The axis along which to join the strings. Defaults to <code>-1</code>. :param separator: The separator to use when joining the strings. Defaults to <code>\"\"</code>. :param keepdims: Whether to keep the shape of the input tensor. Defaults to <code>False</code>.</p> Source code in <code>src/kamae/tensorflow/layers/string_list_to_string.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    axis: int = -1,\n    separator: str = \"\",\n    keepdims: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises the StringListToStringLayer layer.\n\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param axis: The axis along which to join the strings. Defaults to `-1`.\n    :param separator: The separator to use when joining the strings.\n    Defaults to `\"\"`.\n    :param keepdims: Whether to keep the shape of the input tensor. Defaults to\n    `False`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.axis = axis\n    self.separator = separator\n    self.keepdims = keepdims\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_list_to_string/#src.kamae.tensorflow.layers.string_list_to_string.StringListToStringLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/string_list_to_string/#src.kamae.tensorflow.layers.string_list_to_string.StringListToStringLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Joins the strings along the specified axis with the specified separator. If <code>keepdims</code> is <code>True</code>, the shape is retained. Otherwise the shape is reduced along the specified axis.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if an iterable of tensors is passed in.</p> <p>:param inputs: Input tensor. :returns: Tensor with strings joined along the specified axis.</p> Source code in <code>src/kamae/tensorflow/layers/string_list_to_string.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Joins the strings along the specified axis with the specified separator.\n    If `keepdims` is `True`, the shape is retained. Otherwise the shape is\n    reduced along the specified axis.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that the input\n    is a single tensor. Raises an error if an iterable of tensors is passed\n    in.\n\n    :param inputs: Input tensor.\n    :returns: Tensor with strings joined along the specified axis.\n    \"\"\"\n    return tf.strings.reduce_join(\n        inputs, axis=self.axis, separator=self.separator, keepdims=self.keepdims\n    )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_list_to_string/#src.kamae.tensorflow.layers.string_list_to_string.StringListToStringLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the StringListToString layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>axis</code>, <code>separator</code> and <code>keepdims</code> to the config dictionary.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/string_list_to_string.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the StringListToString layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `axis`, `separator` and `keepdims` to the config\n    dictionary.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"axis\": self.axis,\n            \"separator\": self.separator,\n            \"keepdims\": self.keepdims,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_map/","title":"string_map","text":""},{"location":"reference/src/kamae/tensorflow/layers/string_map/#src.kamae.tensorflow.layers.string_map.StringMapLayer","title":"StringMapLayer","text":"<pre><code>StringMapLayer(\n    string_match_values,\n    string_replace_values,\n    default_replace_value=None,\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>StringMapLayer layer for TensorFlow.</p> <p>:param string_match_values: The list of strings to match against. :param string_replace_values: The list of strings to replace the matched strings with. :param default_replace_value: The default value to replace the unmatched strings with. If None, the original string is kept unchanged. :param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/string_map.py</code> <pre><code>def __init__(\n    self,\n    string_match_values: List[str],\n    string_replace_values: List[str],\n    default_replace_value: Optional[str] = None,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises the StringMapLayer layer.\n\n    :param string_match_values: The list of strings to match against.\n    :param string_replace_values: The list of strings to replace the matched\n    strings with.\n    :param default_replace_value: The default value to replace the unmatched\n    strings with. If None, the original string is kept unchanged.\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.string_match_values = string_match_values\n    self.string_replace_values = string_replace_values\n    self.default_replace_value = default_replace_value\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_map/#src.kamae.tensorflow.layers.string_map.StringMapLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/string_map/#src.kamae.tensorflow.layers.string_map.StringMapLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Checks if the input tensor is matching any of the string_match_values and replaces it with the corresponding string_replace_values.</p> <p>If default_replace_value is set, it will replace the unmatched strings with the default_replace_value. If default_replace_value is None, the original string is kept unchanged.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param: inputs: Input string tensor. :returns: A string tensor with the matched strings replaced.</p> Source code in <code>src/kamae/tensorflow/layers/string_map.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Checks if the input tensor is matching any of the string_match_values\n    and replaces it with the corresponding string_replace_values.\n\n    If default_replace_value is set, it will replace the unmatched strings\n    with the default_replace_value. If default_replace_value is None, the\n    original string is kept unchanged.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that the input\n    is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n\n    :param: inputs: Input string tensor.\n    :returns: A string tensor with the matched strings replaced.\n    \"\"\"\n\n    # Iterate through each match/replace pair\n    output_tensor = inputs\n    for match_value, replace_value in zip(\n        self.string_match_values, self.string_replace_values\n    ):\n        output_tensor = tf.where(\n            tf.equal(output_tensor, match_value), replace_value, output_tensor\n        )\n\n    # Handle the default replacement for unmatched strings\n    # Chain tf.logical_and for each match to check if there is no match\n    if self.default_replace_value is not None:\n        matches = self.string_match_values\n        unmatched_condition = tf.not_equal(inputs, matches[0])\n        if len(matches) &gt; 1:\n            for match in matches[1:]:\n                unmatched_condition = tf.logical_and(\n                    unmatched_condition,\n                    tf.not_equal(inputs, match),\n                )\n        expected_dtype = output_tensor.dtype\n        default_val = tf.constant(self.default_replace_value, dtype=expected_dtype)\n        output_tensor = tf.where(unmatched_condition, default_val, output_tensor)\n\n    return output_tensor\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_map/#src.kamae.tensorflow.layers.string_map.StringMapLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the StringMapLayer layer. Used for saving and loading the layer from disk.</p> <p>Specifically, <code>string_match_values</code> and <code>string_replace_values</code> are added to the config.</p> <p>:returns: Dictionary configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/string_map.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the StringMapLayer layer.\n    Used for saving and loading the layer from disk.\n\n    Specifically, `string_match_values` and `string_replace_values`\n    are added to the config.\n\n    :returns: Dictionary configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"string_match_values\": self.string_match_values,\n            \"string_replace_values\": self.string_replace_values,\n            \"default_replace_value\": self.default_replace_value,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_replace/","title":"string_replace","text":""},{"location":"reference/src/kamae/tensorflow/layers/string_replace/#src.kamae.tensorflow.layers.string_replace.StringReplaceLayer","title":"StringReplaceLayer","text":"<pre><code>StringReplaceLayer(\n    string_match_constant=None,\n    string_replace_constant=None,\n    regex=False,\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>StringReplaceLayer layer for TensorFlow.</p> <p>WARNING: While it works, the use of tensors in matching/replacement is not recommended due to the complexity of the regex matching which requires use of a map_fn. This will be comparatively VERY slow and may not be suitable for inference use-cases. If you know where in the string the match is, you will be much better off slicing the string and checking for equality.</p> <p>:param string_match_constant: The string to match against and replace.     Defaults to <code>None</code>. :param string_replace_constant: The string to replace the matched string with.     Defaults to <code>None</code>. :param regex: Whether to treat the string match as a regular expression.     Defaults to <code>False</code>. In the case regex is enabled, the string_match_constant     or second input tensor elements are treated as a regex pattern. Please be     aware that while testing has tried to catch corner cases, this is not     guaranteed to be bug-free due to slight differences in the regex     implementations between Spark and TensorFlow. :param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/string_replace.py</code> <pre><code>def __init__(\n    self,\n    string_match_constant: Optional[str] = None,\n    string_replace_constant: Optional[str] = None,\n    regex: bool = False,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises the StringReplaceLayer layer.\n\n    WARNING: While it works, the use of tensors in matching/replacement\n    is not recommended due to the complexity of the regex matching which requires\n    use of a map_fn. This will be comparatively VERY slow and may not be suitable\n    for inference use-cases.\n    If you know where in the string the match is, you will be much\n    better off slicing the string and checking for equality.\n\n    :param string_match_constant: The string to match against and replace.\n        Defaults to `None`.\n    :param string_replace_constant: The string to replace the matched string with.\n        Defaults to `None`.\n    :param regex: Whether to treat the string match as a regular expression.\n        Defaults to `False`. In the case regex is enabled, the string_match_constant\n        or second input tensor elements are treated as a regex pattern. Please be\n        aware that while testing has tried to catch corner cases, this is not\n        guaranteed to be bug-free due to slight differences in the regex\n        implementations between Spark and TensorFlow.\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.string_match_constant = string_match_constant\n    self.string_replace_constant = string_replace_constant\n    self.regex = regex\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_replace/#src.kamae.tensorflow.layers.string_replace.StringReplaceLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/string_replace/#src.kamae.tensorflow.layers.string_replace.StringReplaceLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Checks for the existence of a substring/pattern within a tensor and replaces if there is a match.</p> <p>KNOWN ISSUE: when replacing with a string that contains a backslash, the backslash must be double escaped (\\) in order to be added properly. This is consistent in both spark and tensorflow components.</p> <p>WARNING: While it works, the use of tensors in matching/replacement is not recommended due to the complexity of the regex matching which requires use of a map_fn. This will be comparatively VERY slow and may not be suitable for inference use-cases. If you know where in the string the match is, you will be much better off slicing the string and checking for equality.</p> <p>Decorated with <code>@allow_single_or_multiple_tensor_input</code> to ensure that the input is either a single tensor or an iterable of tensors. Returns this result as a list of tensors for easier use here.</p> <p>:param: inputs: A string tensor or iterable of up to three string     tensors.     In the case multiple tensors are passed, require that the order of inputs is      [string input, {string match tensor}, {string replace tensor}]. :returns: A string tensor of regex replaced strings.</p> Source code in <code>src/kamae/tensorflow/layers/string_replace.py</code> <pre><code>@allow_single_or_multiple_tensor_input\ndef _call(self, inputs: Union[Tensor, Iterable[Tensor]], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Checks for the existence of a substring/pattern within a tensor and replaces\n    if there is a match.\n\n    KNOWN ISSUE: when replacing with a string that contains a backslash,\n    the backslash must be double escaped (\\\\\\\\) in order to be added properly.\n    This is consistent in both spark and tensorflow components.\n\n    WARNING: While it works, the use of tensors in matching/replacement\n    is not recommended due to the complexity of the regex matching which requires\n    use of a map_fn. This will be comparatively VERY slow and may not be suitable\n    for inference use-cases.\n    If you know where in the string the match is, you will be much\n    better off slicing the string and checking for equality.\n\n    Decorated with `@allow_single_or_multiple_tensor_input` to ensure that the input\n    is either a single tensor or an iterable of tensors. Returns this result as a\n    list of tensors for easier use here.\n\n    :param: inputs: A string tensor or iterable of up to three string\n        tensors.\n        In the case multiple tensors are passed, require that the order of inputs is\n         [string input, {string match tensor}, {string replace tensor}].\n    :returns: A string tensor of regex replaced strings.\n    \"\"\"\n\n    match_all_pattern = r\"([\\w]\\\\+\\_+\\!+\\?+)*\"\n\n    # Case both match and replacement are constant\n    if (\n        self.string_replace_constant is not None\n        and self.string_match_constant is not None\n    ):\n        if len(inputs) == 1:\n            # Need the tensor for shapes to be consistent\n            input_tensor = inputs[0]\n\n            match_substring = self.string_match_constant\n\n            if not self.regex:\n                match_substring = self._escape_special_characters(match_substring)\n\n            # Calls regex replace function on the input tensor, matching\n            # with match constant and replacing with replace constant\n            replaced_tensor = tf.strings.regex_replace(\n                input_tensor,\n                tf.constant(\n                    match_all_pattern + match_substring + match_all_pattern\n                    if match_substring != \"\"\n                    else \"^$\"\n                ),\n                tf.constant(self.string_replace_constant),\n            )\n\n        else:\n            raise ValueError(\n                \"\"\"When string_match_constant and string_replace_constant are\n                defined, expected a single tensor as input.\"\"\"\n            )\n    else:\n        # Preserve input shape\n        input_shape = tf.shape(inputs[0])\n        # Generate a tensor that can be used by map_fn\n        # First we define 3 tensors, the input string, the match string and the\n        # replace string\n        string_tensor = inputs[0]\n        match_substring = (\n            tf.constant(self.string_match_constant, shape=string_tensor.shape)\n            if self.string_match_constant is not None\n            else inputs[1]\n        )\n        replace_substring = (\n            tf.constant(self.string_replace_constant, shape=string_tensor.shape)\n            if self.string_replace_constant is not None\n            else inputs[1 + (len(inputs) == 3)]\n        )\n\n        # Stack the input, match and replace elements into a single tensor\n        # then flatten for use in map_fn\n        mappable_tensor = tf.stack(\n            [string_tensor, match_substring, replace_substring], axis=-1\n        )\n        mappable_tensor = tf.reshape(mappable_tensor, [-1, 3])\n\n        def _tensor_replace(x: List[Tensor]) -&gt; Tensor:\n            match_substring = x[1]\n            if not self.regex:\n                match_substring = self._escape_special_characters(x[1])\n            return tf.strings.regex_replace(\n                input=x[0],\n                pattern=match_all_pattern + match_substring + match_all_pattern\n                if match_substring != \"\"\n                else \"^$\",\n                rewrite=x[2],\n            )\n\n        # TODO: tf.vectorized_map may be slightly faster with larger batches\n        #  but this requires some refactoring\n        replaced_tensor = tf.map_fn(\n            _tensor_replace,\n            elems=mappable_tensor,\n            dtype=tf.string,\n        )\n\n        # Reshape to the preserved input shape\n        replaced_tensor = tf.reshape(replaced_tensor, input_shape)\n\n    return replaced_tensor\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_replace/#src.kamae.tensorflow.layers.string_replace.StringReplaceLayer._escape_special_characters","title":"_escape_special_characters","text":"<pre><code>_escape_special_characters(string_to_escape)\n</code></pre> <p>Escapes special characters in a string so they are not parsed as regex. :param string: The string or string tensor to escape special characters in. :returns: The escaped string or string tensor.</p> Source code in <code>src/kamae/tensorflow/layers/string_replace.py</code> <pre><code>def _escape_special_characters(\n    self, string_to_escape: Union[str, Tensor]\n) -&gt; Union[str, Tensor]:\n    \"\"\"\n    Escapes special characters in a string so they are not parsed as regex.\n    :param string: The string or string tensor to escape special characters in.\n    :returns: The escaped string or string tensor.\n    \"\"\"\n\n    for char in [\n        \".\",\n        \"^\",\n        \"$\",\n        \"*\",\n        \"+\",\n        \"?\",\n        \"{\",\n        \"}\",\n        \"[\",\n        \"]\",\n        \"(\",\n        \")\",\n        \"|\",\n    ]:\n        if isinstance(string_to_escape, str):\n            string_to_escape = string_to_escape.replace(char, \"\\\\\\\\\" + char)\n        else:\n            string_to_escape = tf.strings.regex_replace(\n                string_to_escape, \"\\\\\" + char, \"\\\\\\\\\" + char\n            )\n    return string_to_escape\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_replace/#src.kamae.tensorflow.layers.string_replace.StringReplaceLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the StringReplace layer. Used for saving and loading the layer from disk.</p> <p>Specifically, <code>regex</code>, <code>string_match_constant</code> and <code>string_replace_constant</code> are added to the config.</p> <p>:returns: Dictionary configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/string_replace.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the StringReplace layer.\n    Used for saving and loading the layer from disk.\n\n    Specifically, `regex`, `string_match_constant` and `string_replace_constant`\n    are added to the config.\n\n    :returns: Dictionary configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"regex\": self.regex,\n            \"string_match_constant\": self.string_match_constant,\n            \"string_replace_constant\": self.string_replace_constant,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_to_string_list/","title":"string_to_string_list","text":""},{"location":"reference/src/kamae/tensorflow/layers/string_to_string_list/#src.kamae.tensorflow.layers.string_to_string_list.StringToStringListLayer","title":"StringToStringListLayer","text":"<pre><code>StringToStringListLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    separator=\",\",\n    default_value=\"\",\n    list_length=1,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>A layer that converts a string to a list of strings by splitting on a separator. It takes a default value and a list_length parameter to ensure that the output tensor has the correct shape.</p> <p>If the separator is empty, the string is split on bytes/characters.</p> <p>:param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param separator: The separator to use when joining the strings. Defaults to <code>\",\"</code>. :param default_value: The value to use when the input is empty. Defaults to <code>\"\"</code>. :param list_length: The length of the string list in the output tensor. Defaults to <code>1</code>.</p> Source code in <code>src/kamae/tensorflow/layers/string_to_string_list.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    separator: str = \",\",\n    default_value: str = \"\",\n    list_length: int = 1,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises the StringToStringListLayer layer.\n\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param separator: The separator to use when joining the strings.\n    Defaults to `\",\"`.\n    :param default_value: The value to use when the input is empty.\n    Defaults to `\"\"`.\n    :param list_length: The length of the string list in the output tensor.\n    Defaults to `1`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.separator = separator\n    self.list_length = list_length\n    self.default_value = default_value\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_to_string_list/#src.kamae.tensorflow.layers.string_to_string_list.StringToStringListLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/string_to_string_list/#src.kamae.tensorflow.layers.string_to_string_list.StringToStringListLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Splits the input string tensor by the separator and returns the list of strings. A list_length parameter is used to ensure that the output tensor has a fixed shape. If the separator is empty, the string is split on bytes/characters.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if an iterable of tensors is passed in.</p> <p>:param inputs: Input tensor. :returns: Tensor with the list of strings.</p> Source code in <code>src/kamae/tensorflow/layers/string_to_string_list.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Splits the input string tensor by the separator and returns the list of\n    strings. A list_length parameter is used to ensure that the output tensor has a\n    fixed shape. If the separator is empty, the string is split on bytes/characters.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that the input\n    is a single tensor. Raises an error if an iterable of tensors is passed\n    in.\n\n    :param inputs: Input tensor.\n    :returns: Tensor with the list of strings.\n    \"\"\"\n    input_shape = inputs.get_shape().as_list()\n    input_shape.append(self.list_length)\n    # If the separator is empty, we split on bytes/characters.\n    # Otherwise, we use the standard string split.\n    ragged_strings_split = (\n        tf.strings.split(inputs, sep=self.separator)\n        if self.separator != \"\"\n        else tf.strings.bytes_split(inputs)\n    )\n    split_strings_tensor = ragged_strings_split.to_tensor(\n        default_value=self.default_value, shape=input_shape\n    )\n\n    # Replace empty strings with the default value\n    split_strings_tensor = tf.where(\n        tf.equal(split_strings_tensor, \"\"), self.default_value, split_strings_tensor\n    )\n\n    # If the dimension of the feature was 1, we squeeze it out\n    # E.g. (None, None, 1) -&gt; (None, None, 1, N) -&gt; (None, None, N)\n    # But (None, None, M) -&gt; (None, None, M, N)\n    return (\n        tf.squeeze(split_strings_tensor, axis=-2)\n        if input_shape[-2] == 1\n        else split_strings_tensor\n    )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/string_to_string_list/#src.kamae.tensorflow.layers.string_to_string_list.StringToStringListLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the StringToStringList layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>axis</code>, <code>separator</code> and <code>keepdims</code> to the config dictionary.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/string_to_string_list.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the StringToStringList layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `axis`, `separator` and `keepdims` to the config\n    dictionary.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"separator\": self.separator,\n            \"default_value\": self.default_value,\n            \"list_length\": self.list_length,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/sub_string_delim_at_index/","title":"sub_string_delim_at_index","text":""},{"location":"reference/src/kamae/tensorflow/layers/sub_string_delim_at_index/#src.kamae.tensorflow.layers.sub_string_delim_at_index.SubStringDelimAtIndexLayer","title":"SubStringDelimAtIndexLayer","text":"<pre><code>SubStringDelimAtIndexLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    delimiter=\"_\",\n    index=0,\n    default_value=\"\",\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Layer which splits a string tensor by a delimiter and returns the substring at the specified index. If the delimiter is the empty string, the string is split into bytes/characters. If the index is negative, start counting from the end of the string. If the index is out of bounds, the default value is returned.</p> <p>:param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param delimiter: String to split on. Defaults to <code>\"_\"</code>. :param index: Index of the substring to return. Defaults to <code>0</code>. If the index is negative, start counting from the end of the string. :param default_value: Value to return if index is out of bounds. Defaults to <code>\"\"</code>. Defaults to <code>\"\"</code>.</p> Source code in <code>src/kamae/tensorflow/layers/sub_string_delim_at_index.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    delimiter: str = \"_\",\n    index: int = 0,\n    default_value: str = \"\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialise the SubStringDelimAtIndexLayer layer.\n\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param delimiter: String to split on. Defaults to `\"_\"`.\n    :param index: Index of the substring to return. Defaults to `0`.\n    If the index is negative, start counting from the end of the string.\n    :param default_value: Value to return if index is out of bounds.\n    Defaults to `\"\"`.\n    Defaults to `\"\"`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.delimiter = delimiter\n    self.index = index\n    self.default_value = default_value\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/sub_string_delim_at_index/#src.kamae.tensorflow.layers.sub_string_delim_at_index.SubStringDelimAtIndexLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/sub_string_delim_at_index/#src.kamae.tensorflow.layers.sub_string_delim_at_index.SubStringDelimAtIndexLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Splits the input string tensor by the delimiter and returns the substring at the specified index. If the index is out of bounds, the default value is returned.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if an iterable of tensors is passed in.</p> <p>:param inputs: Input tensor. :returns: Tensor with the substring at the specified index.</p> Source code in <code>src/kamae/tensorflow/layers/sub_string_delim_at_index.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Splits the input string tensor by the delimiter and returns the substring\n    at the specified index. If the index is out of bounds, the default value\n    is returned.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that the input\n    is a single tensor. Raises an error if an iterable of tensors is passed\n    in.\n\n    :param inputs: Input tensor.\n    :returns: Tensor with the substring at the specified index.\n    \"\"\"\n    input_shape = tf.shape(inputs)\n    # If the delimiter is empty, we split on bytes/characters.\n    # Otherwise, we use the standard string split.\n    ragged_strings_split = (\n        tf.strings.split(inputs, sep=self.delimiter)\n        if self.delimiter != \"\"\n        else tf.strings.bytes_split(inputs)\n    )\n\n    if self.index &gt;= 0:\n        # The index is fully qualified, therefore, add the index + 1 to the shape\n        # and then pad the ragged tensor to that shape. If the index is\n        # out of bounds, it returns the default value\n        index_shape = tf.constant([self.index + 1])\n        input_shape = tf.concat([input_shape, index_shape], axis=0)\n        return ragged_strings_split.to_tensor(\n            default_value=self.default_value, shape=input_shape\n        )[..., self.index]\n    else:\n        # The index is negative, so we need to resolve the positive index from it.\n        resolved_index_tensor = self.resolve_negative_indices(\n            ragged_tensor=ragged_strings_split, index=self.index\n        )\n        if isinstance(resolved_index_tensor, tf.RaggedTensor):\n            # The resolved indices can be ragged or a regular tensor, however\n            # are always rectangular since we only have a single ragged dimension,\n            # and we have found the required index within this.\n            resolved_index_tensor = resolved_index_tensor.to_tensor(\n                shape=tf.shape(inputs)\n            )\n\n        # Pad the ragged tensor to the maximum row_length of the ragged tensor\n        # This could be different for each batch, however we return a single index\n        # from it, and thus we will have consistent output shapes per batch.\n        max_ragged_dim = tf.cast(\n            tf.reduce_max(ragged_strings_split.row_lengths(axis=-1)), dtype=tf.int32\n        )\n        input_shape = tf.concat(\n            [input_shape, tf.expand_dims(max_ragged_dim, axis=0)], axis=0\n        )\n        padded_tensor = ragged_strings_split.to_tensor(\n            default_value=self.default_value, shape=input_shape\n        )\n        # Expand the indices to match the shape of the input\n        expanded_indices = tf.expand_dims(resolved_index_tensor, axis=-1)\n        # Replace negative indices with zeros temporarily, we will send these to the\n        # default value as they are out of bounds\n        non_negative_expanded_indices = tf.where(\n            expanded_indices &lt; 0,\n            tf.constant(0, dtype=expanded_indices.dtype),\n            expanded_indices,\n        )\n        # Gather the resolved indices from the padded tensor, send any negative\n        # indices to the default value\n        gathered_tensor = tf.where(\n            expanded_indices &gt;= 0,\n            tf.gather(padded_tensor, non_negative_expanded_indices, batch_dims=-1),\n            tf.constant(self.default_value),\n        )\n        # Squeeze out the extra dimension\n        return tf.squeeze(gathered_tensor, axis=-1)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/sub_string_delim_at_index/#src.kamae.tensorflow.layers.sub_string_delim_at_index.SubStringDelimAtIndexLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Returns the config of the SubStringDelimAtIndex layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>delimiter</code>, <code>index</code> and <code>default_value</code> to the config.</p> <p>:returns: Dictionary of the config of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/sub_string_delim_at_index.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Returns the config of the SubStringDelimAtIndex layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `delimiter`, `index` and `default_value` to the config.\n\n    :returns: Dictionary of the config of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"delimiter\": self.delimiter,\n            \"index\": self.index,\n            \"default_value\": self.default_value,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/sub_string_delim_at_index/#src.kamae.tensorflow.layers.sub_string_delim_at_index.SubStringDelimAtIndexLayer.resolve_negative_indices","title":"resolve_negative_indices  <code>staticmethod</code>","text":"<pre><code>resolve_negative_indices(ragged_tensor, index)\n</code></pre> <p>Resolves negative indices to positive indices.</p> <p>:param ragged_tensor: Ragged tensor :param index: The index to resolve. :returns: The resolved index.</p> Source code in <code>src/kamae/tensorflow/layers/sub_string_delim_at_index.py</code> <pre><code>@staticmethod\ndef resolve_negative_indices(\n    ragged_tensor: tf.RaggedTensor, index: int\n) -&gt; tf.Tensor:\n    \"\"\"\n    Resolves negative indices to positive indices.\n\n    :param ragged_tensor: Ragged tensor\n    :param index: The index to resolve.\n    :returns: The resolved index.\n    \"\"\"\n    if index &gt;= 0:\n        raise ValueError(\"Index should be negative to resolve. Got positive index.\")\n    ragged_row_lengths = ragged_tensor.row_lengths(axis=-1)\n    # Positive index is the length of the row + index. So that index = -1\n    # resolves to the last dimension\n    return tf.math.add(ragged_row_lengths, index)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/subtract/","title":"subtract","text":""},{"location":"reference/src/kamae/tensorflow/layers/subtract/#src.kamae.tensorflow.layers.subtract.SubtractLayer","title":"SubtractLayer","text":"<pre><code>SubtractLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    subtrahend=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to, defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to, defaults to <code>None</code>. :param subtrahend: The subtrahend to subtract from the input, defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/subtract.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    subtrahend: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the SubtractLayer layer\n\n    :param name: Name of the layer, defaults to `None`.\n    :param input_dtype: The dtype to cast the input to, defaults to `None`.\n    :param output_dtype: The dtype to cast the output to, defaults to `None`.\n    :param subtrahend: The subtrahend to subtract from the input,\n    defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.subtrahend = subtrahend\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/subtract/#src.kamae.tensorflow.layers.subtract.SubtractLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/subtract/#src.kamae.tensorflow.layers.subtract.SubtractLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the subtract(x, y) operation on either an iterable of input tensors or a single input tensor and a constant.</p> <p>Decorated with <code>@allow_single_or_multiple_tensor_input</code> to ensure that the input is either a single tensor or an iterable of tensors. Returns this result as a list of tensors for easier use here.</p> <p>:param inputs: Single tensor or iterable of tensors to perform the subtract(x, y) operation on. :returns: The tensor resulting from the subtract(x, y) operation.</p> Source code in <code>src/kamae/tensorflow/layers/subtract.py</code> <pre><code>@allow_single_or_multiple_tensor_input\ndef _call(self, inputs: Union[Tensor, Iterable[Tensor]], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the subtract(x, y) operation on either an iterable of input tensors or\n    a single input tensor and a constant.\n\n    Decorated with `@allow_single_or_multiple_tensor_input` to ensure that the input\n    is either a single tensor or an iterable of tensors. Returns this result as a\n    list of tensors for easier use here.\n\n    :param inputs: Single tensor or iterable of tensors to perform the\n    subtract(x, y) operation on.\n    :returns: The tensor resulting from the subtract(x, y) operation.\n    \"\"\"\n    if self.subtrahend is not None:\n        if len(inputs) &gt; 1:\n            raise ValueError(\"If subtrahend is set, cannot have multiple inputs\")\n        cast_input, cast_subtrahend = self._force_cast_to_compatible_numeric_type(\n            inputs[0], self.subtrahend\n        )\n        return tf.math.subtract(\n            cast_input,\n            cast_subtrahend,\n        )\n    else:\n        if not len(inputs) &gt; 1:\n            raise ValueError(\"If subtrahend is not set, must have multiple inputs\")\n        return reduce(tf.math.subtract, inputs)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/subtract/#src.kamae.tensorflow.layers.subtract.SubtractLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the Subtract layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>subtrahend</code> to the config dictionary.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/subtract.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the Subtract layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `subtrahend` to the config dictionary.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"subtrahend\": self.subtrahend})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/sum/","title":"sum","text":""},{"location":"reference/src/kamae/tensorflow/layers/sum/#src.kamae.tensorflow.layers.sum.SumLayer","title":"SumLayer","text":"<pre><code>SumLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    addend=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Performs the sum(x, y) operation on a given input tensor. If added is not set, inputs are assumed to be a list of tensors and summed. If added is set, inputs must be a tensor.</p> <p>:param name: Name of the layer, defaults to <code>None</code>. :param addend: The addend to add to the input, defaults to <code>None</code>.</p> Source code in <code>src/kamae/tensorflow/layers/sum.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    addend: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the SumLayer layer\n\n    :param name: Name of the layer, defaults to `None`.\n    :param addend: The addend to add to the input, defaults to `None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    self.addend = addend\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/sum/#src.kamae.tensorflow.layers.sum.SumLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/sum/#src.kamae.tensorflow.layers.sum.SumLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Performs the sum(x, y) operation on either an iterable of input tensors or a single input tensor and a constant.</p> <p>Decorated with <code>@allow_single_or_multiple_tensor_input</code> to ensure that the input is either a single tensor or an iterable of tensors. Returns this result as a list of tensors for easier use here.</p> <p>:param inputs: Single tensor or iterable of tensors to perform the sum(x, y) operation on. :returns: The tensor resulting from the sum(x, y) operation.</p> Source code in <code>src/kamae/tensorflow/layers/sum.py</code> <pre><code>@allow_single_or_multiple_tensor_input\ndef _call(self, inputs: Union[Tensor, Iterable[Tensor]], **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Performs the sum(x, y) operation on either an iterable of input tensors or\n    a single input tensor and a constant.\n\n    Decorated with `@allow_single_or_multiple_tensor_input` to ensure that the input\n    is either a single tensor or an iterable of tensors. Returns this result as a\n    list of tensors for easier use here.\n\n    :param inputs: Single tensor or iterable of tensors to perform the\n    sum(x, y) operation on.\n    :returns: The tensor resulting from the sum(x, y) operation.\n    \"\"\"\n    if self.addend is not None:\n        if len(inputs) &gt; 1:\n            raise ValueError(\"If addend is set, cannot have multiple inputs\")\n        cast_input, cast_addend = self._force_cast_to_compatible_numeric_type(\n            inputs[0], self.addend\n        )\n        return tf.math.add(\n            cast_input,\n            cast_addend,\n        )\n    else:\n        if not len(inputs) &gt; 1:\n            raise ValueError(\"If addend is not set, must have multiple inputs\")\n        return reduce(tf.math.add, inputs)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/sum/#src.kamae.tensorflow.layers.sum.SumLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the Sum layer. Used for saving and loading from a model.</p> <p>Specifically adds the <code>addend</code> to the config dictionary.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/sum.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the Sum layer.\n    Used for saving and loading from a model.\n\n    Specifically adds the `addend` to the config dictionary.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update({\"addend\": self.addend})\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/unix_timestamp_to_date_time/","title":"unix_timestamp_to_date_time","text":""},{"location":"reference/src/kamae/tensorflow/layers/unix_timestamp_to_date_time/#src.kamae.tensorflow.layers.unix_timestamp_to_date_time.UnixTimestampToDateTimeLayer","title":"UnixTimestampToDateTimeLayer","text":"<pre><code>UnixTimestampToDateTimeLayer(\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    unit=\"s\",\n    include_time=True,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Returns the date in yyyy-MM-dd HHss.SSS format from a Unix timestamp. If <code>include_time</code> is set to <code>False</code>, the output will be in yyyy-MM-dd format.</p> <p>:param name: Name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param unit: Unit of the timestamp. Can be <code>milliseconds</code> (or <code>ms</code>) or <code>seconds</code> (or <code>s</code>). Defaults to <code>s</code>. :param include_time: Whether to include the time in the output. Defaults to <code>True</code>.</p> Source code in <code>src/kamae/tensorflow/layers/unix_timestamp_to_date_time.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    unit: str = \"s\",\n    include_time: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialises an instance of the UnixTimestampToDateTime layer.\n\n    :param name: Name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param unit: Unit of the timestamp. Can be `milliseconds` (or `ms`)\n    or `seconds` (or `s`). Defaults to `s`.\n    :param include_time: Whether to include the time in the output.\n    Defaults to `True`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    if unit not in [\"milliseconds\", \"seconds\", \"ms\", \"s\"]:\n        raise ValueError(\n            \"\"\"Unit must be one of [\"milliseconds\", \"seconds\", \"ms\", \"s\"]\"\"\"\n        )\n    if unit == \"milliseconds\":\n        unit = \"ms\"\n    if unit == \"seconds\":\n        unit = \"s\"\n    self.unit = unit\n    self.include_time = include_time\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/unix_timestamp_to_date_time/#src.kamae.tensorflow.layers.unix_timestamp_to_date_time.UnixTimestampToDateTimeLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer. Returns <code>None</code> as the layer only returns the current date as a string. It does not transform any input.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/layers/unix_timestamp_to_date_time/#src.kamae.tensorflow.layers.unix_timestamp_to_date_time.UnixTimestampToDateTimeLayer._call","title":"_call","text":"<pre><code>_call(inputs, **kwargs)\n</code></pre> <p>Returns the datetime in yyyy-MM-dd HHss.SSS format if <code>include_time</code> is set to <code>True</code>. Otherwise, returns the date in yyyy-MM-dd format.</p> <p>Decorated with <code>@enforce_single_tensor_input</code> to ensure that the input is a single tensor. Raises an error if multiple tensors are passed in as an iterable.</p> <p>:param inputs: Input tensor to determine the shape of the output tensor. :returns: Datetime in either yyyy-MM-dd HHss.SSS or yyyy-MM-dd format.</p> Source code in <code>src/kamae/tensorflow/layers/unix_timestamp_to_date_time.py</code> <pre><code>@enforce_single_tensor_input\ndef _call(self, inputs: Tensor, **kwargs: Any) -&gt; Tensor:\n    \"\"\"\n    Returns the datetime in yyyy-MM-dd HH:mm:ss.SSS format if `include_time` is\n    set to `True`. Otherwise, returns the date in yyyy-MM-dd format.\n\n    Decorated with `@enforce_single_tensor_input` to ensure that\n    the input is a single tensor. Raises an error if multiple tensors are passed\n    in as an iterable.\n\n    :param inputs: Input tensor to determine the shape of the output tensor.\n    :returns: Datetime in either yyyy-MM-dd HH:mm:ss.SSS or yyyy-MM-dd format.\n    \"\"\"\n    # Timestamp needs to be in float64 for unix_timestamp_to_datetime\n    timestamp_in_seconds = (\n        self._cast(inputs, cast_dtype=\"float64\")\n        if self.unit == \"s\"\n        else tf.math.divide_no_nan(self._cast(inputs, cast_dtype=\"float64\"), 1000.0)\n    )\n    outputs = unix_timestamp_to_datetime(\n        timestamp_in_seconds, include_time=self.include_time\n    )\n    return outputs\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/layers/unix_timestamp_to_date_time/#src.kamae.tensorflow.layers.unix_timestamp_to_date_time.UnixTimestampToDateTimeLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the UnixTimestampToDateTime layer. Used for saving and loading from a model.</p> <p>Specifically sets the <code>unit</code> and <code>include_time</code> parameters in the config.</p> <p>:returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/layers/unix_timestamp_to_date_time.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the UnixTimestampToDateTime layer.\n    Used for saving and loading from a model.\n\n    Specifically sets the `unit` and `include_time` parameters in the config.\n\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    config.update(\n        {\n            \"unit\": self.unit,\n            \"include_time\": self.include_time,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/typing/","title":"typing","text":""},{"location":"reference/src/kamae/tensorflow/typing/types/","title":"types","text":"<p>Creates typing objects for common tensorflow types.</p>"},{"location":"reference/src/kamae/tensorflow/utils/","title":"utils","text":""},{"location":"reference/src/kamae/tensorflow/utils/date_utils/","title":"date_utils","text":""},{"location":"reference/src/kamae/tensorflow/utils/date_utils/#src.kamae.tensorflow.utils.date_utils.add_missing_time_components_to_datetime_tensor","title":"add_missing_time_components_to_datetime_tensor","text":"<pre><code>add_missing_time_components_to_datetime_tensor(\n    datetime_tensor, max_len=None\n)\n</code></pre> <p>Adds missing time components to a date string tensor. If the time components are missing, they will be added as zeros.</p> <p>:param datetime_tensor: date string tensor. Must be in yyyy-MM-dd (HHss.SSS) format. Can be truncated, and missing time components will be added as zeros. :param max_len: Maximum length to append time to if the time is missing. Used to avoid unnecessary computation. E.g. if we only need hour, then don't add milliseconds. Default is None. :returns: Date string tensor with missing time components added as zeros.</p> Source code in <code>src/kamae/tensorflow/utils/date_utils.py</code> <pre><code>def add_missing_time_components_to_datetime_tensor(\n    datetime_tensor: tf.Tensor, max_len: Optional[int] = None\n) -&gt; tf.Tensor:\n    \"\"\"\n    Adds missing time components to a date string tensor.\n    If the time components are missing, they will be added as zeros.\n\n    :param datetime_tensor: date string tensor.\n    Must be in yyyy-MM-dd (HH:mm:ss.SSS) format. Can be truncated, and missing time\n    components will be added as zeros.\n    :param max_len: Maximum length to append time to if the time is missing. Used to\n    avoid unnecessary computation. E.g. if we only need hour, then don't add\n    milliseconds. Default is None.\n    :returns: Date string tensor with missing time components added as zeros.\n    \"\"\"\n    if max_len is not None and max_len &lt; 10:\n        raise ValueError(\n            \"\"\"max_len must be at least 10, as this is the minimum length\n            of a date string.\"\"\"\n        )\n    # Add missing time components, these are at 10, 13, 16 and 19 characters\n    # For hours, minutes, seconds and milliseconds respectively\n    str_lens = [10, 13, 16, 19]\n    str_suffixes = [\" 00:00:00.000\", \":00:00.000\", \":00.000\", \".000\"]\n    # Filter out the suffixes that are longer than the max_len. This allows us to not\n    # add time components if we don't need them.\n    str_loop = (\n        filter(lambda x: x[0] &lt;= max_len, zip(str_lens, str_suffixes))\n        if max_len is not None\n        else zip(str_lens, str_suffixes)\n    )\n    for str_len, str_suffix in str_loop:\n        dynamic_str_len = tf.strings.length(datetime_tensor)\n        datetime_tensor = tf.where(\n            dynamic_str_len == str_len,\n            tf.strings.join([datetime_tensor, str_suffix], \"\"),\n            datetime_tensor,\n        )\n    return datetime_tensor\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/date_utils/#src.kamae.tensorflow.utils.date_utils.datetime_add_days","title":"datetime_add_days","text":"<pre><code>datetime_add_days(\n    datetime_tensor, num_days, include_time=True\n)\n</code></pre> <p>Adds a number of days to a date(time) string tensor.</p> <p>:param datetime_tensor: date(time) string tensor. Must be in yyyy-MM-dd (HHss.SSS) format. :param num_days: Number of days to add. :param include_time: Whether to include the time in the output. If True, the output will be in yyyy-MM-dd HHss.SSS format. If False, the output will be in yyyy-MM-dd format. Default is True. :returns: Date(time) string tensor with num_days added.</p> Source code in <code>src/kamae/tensorflow/utils/date_utils.py</code> <pre><code>def datetime_add_days(\n    datetime_tensor: tf.Tensor, num_days: tf.Tensor, include_time: bool = True\n) -&gt; tf.Tensor:\n    \"\"\"\n    Adds a number of days to a date(time) string tensor.\n\n    :param datetime_tensor: date(time) string tensor.\n    Must be in yyyy-MM-dd (HH:mm:ss.SSS) format.\n    :param num_days: Number of days to add.\n    :param include_time: Whether to include the time in the output. If True, the output\n    will be in yyyy-MM-dd HH:mm:ss.SSS format. If False, the output will be in\n    yyyy-MM-dd format. Default is True.\n    :returns: Date(time) string tensor with num_days added.\n    \"\"\"\n    total_seconds = datetime_total_seconds(datetime_tensor)\n    num_days_seconds = num_days * tf.constant(24 * 60 * 60, dtype=num_days.dtype)\n    total_seconds += num_days_seconds\n    return unix_timestamp_to_datetime(\n        tf.cast(total_seconds, dtype=tf.float64), include_time=include_time\n    )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/date_utils/#src.kamae.tensorflow.utils.date_utils.datetime_day","title":"datetime_day","text":"<pre><code>datetime_day(datetime_tensor)\n</code></pre> <p>Utility function to parse a date(time) tensor into a day tensor. Uses native tf functions only to avoid serialization issues.</p> <p>:param: datetime_tensor: date(time) string tensor. Must be in yyyy-MM-dd (HHss.SSS) format.</p> <p>WARNING: Dates are not checked for validity, so if you pass in a date such as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.</p> <p>:returns: Day tensor, stored as tf.int64.</p> Source code in <code>src/kamae/tensorflow/utils/date_utils.py</code> <pre><code>def datetime_day(datetime_tensor: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"\n    Utility function to parse a date(time) tensor into a day tensor.\n    Uses native tf functions only to avoid serialization issues.\n\n    :param: datetime_tensor: date(time) string tensor.\n    Must be in yyyy-MM-dd (HH:mm:ss.SSS) format.\n\n    WARNING: Dates are not checked for validity, so if you pass in a date such\n    as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.\n\n    :returns: Day tensor, stored as tf.int64.\n    \"\"\"\n    day = tf.strings.to_number(\n        tf.strings.substr(datetime_tensor, 8, 2), out_type=tf.int64\n    )\n    return day\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/date_utils/#src.kamae.tensorflow.utils.date_utils.datetime_day_of_year","title":"datetime_day_of_year","text":"<pre><code>datetime_day_of_year(datetime_tensor)\n</code></pre> <p>Utility function to parse a date(time) tensor into a day of year tensor. Uses native tf functions only to avoid serialization issues.</p> <p>:param: datetime_tensor: date(time) string tensor. Must be in yyyy-MM-dd (HHss.SSS) format.</p> <p>WARNING: Dates are not checked for validity, so if you pass in a date such as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.</p> <p>:returns: Day of year tensor, stored as tf.int64.</p> Source code in <code>src/kamae/tensorflow/utils/date_utils.py</code> <pre><code>def datetime_day_of_year(datetime_tensor: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"\n    Utility function to parse a date(time) tensor into a day of year tensor.\n    Uses native tf functions only to avoid serialization issues.\n\n    :param: datetime_tensor: date(time) string tensor.\n    Must be in yyyy-MM-dd (HH:mm:ss.SSS) format.\n\n    WARNING: Dates are not checked for validity, so if you pass in a date such\n    as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.\n\n    :returns: Day of year tensor, stored as tf.int64.\n    \"\"\"\n    day = datetime_day(datetime_tensor)\n    days_to_month = datetime_days_to_month(datetime_tensor)\n    # Add all the days together\n    day_of_year = days_to_month + day\n\n    return day_of_year\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/date_utils/#src.kamae.tensorflow.utils.date_utils.datetime_days_to_month","title":"datetime_days_to_month","text":"<pre><code>datetime_days_to_month(datetime_tensor)\n</code></pre> <p>Helper function for some datetime functions. Gets the number of days to the month of the given datetime tensor.</p> <p>:param: datetime_tensor: date(time) string tensor. Must be in yyyy-MM-dd (HHss.SSS) format.</p> <p>WARNING: Dates are not checked for validity, so if you pass in a date such as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.</p> <p>:returns: Number of days to month, stored as tf.int64.</p> Source code in <code>src/kamae/tensorflow/utils/date_utils.py</code> <pre><code>def datetime_days_to_month(datetime_tensor: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"\n    Helper function for some datetime functions.\n    Gets the number of days to the month of the given datetime tensor.\n\n    :param: datetime_tensor: date(time) string tensor.\n    Must be in yyyy-MM-dd (HH:mm:ss.SSS) format.\n\n    WARNING: Dates are not checked for validity, so if you pass in a date such\n    as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.\n\n    :returns: Number of days to month, stored as tf.int64.\n    \"\"\"\n    # 30 days have September...\n    days_in_month = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n    # Extract date parts\n    year = datetime_year(datetime_tensor)\n    month = datetime_month(datetime_tensor)\n    days_to_month = tf.reduce_sum(\n        tf.stack(\n            [\n                tf.where(month &gt; idx + 1, 1, 0) * n_days\n                for idx, n_days in enumerate(days_in_month)\n            ],\n            axis=-1,\n        ),\n        -1,\n    ) + (\n        tf.where(month &gt; 2, 1, 0)\n        * tf.where((year % 4 == 0) &amp; ((year % 100 != 0) | (year % 400 == 0)), 1, 0)\n    )\n\n    days_to_month = tf.cast(days_to_month, tf.int64)\n\n    return days_to_month\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/date_utils/#src.kamae.tensorflow.utils.date_utils.datetime_hour","title":"datetime_hour","text":"<pre><code>datetime_hour(datetime_tensor)\n</code></pre> <p>Utility function to parse a date(time) tensor into an hour tensor. Uses native tf functions only to avoid serialization issues.</p> <p>:param: datetime_tensor: date(time) string tensor. Must be in yyyy-MM-dd (HHss.SSS) format.</p> <p>WARNING: Dates are not checked for validity, so if you pass in a date such as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.</p> <p>:returns: Hour tensor, stored as tf.int64.</p> Source code in <code>src/kamae/tensorflow/utils/date_utils.py</code> <pre><code>def datetime_hour(datetime_tensor: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"\n    Utility function to parse a date(time) tensor into an hour tensor.\n    Uses native tf functions only to avoid serialization issues.\n\n    :param: datetime_tensor: date(time) string tensor.\n    Must be in yyyy-MM-dd (HH:mm:ss.SSS) format.\n\n    WARNING: Dates are not checked for validity, so if you pass in a date such\n    as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.\n\n    :returns: Hour tensor, stored as tf.int64.\n    \"\"\"\n    datetime_tensor = add_missing_time_components_to_datetime_tensor(\n        datetime_tensor, max_len=13\n    )\n    hour = tf.strings.to_number(\n        tf.strings.substr(datetime_tensor, 11, 2), out_type=tf.int64\n    )\n    return hour\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/date_utils/#src.kamae.tensorflow.utils.date_utils.datetime_is_weekend","title":"datetime_is_weekend","text":"<pre><code>datetime_is_weekend(datetime_tensor)\n</code></pre> <p>Utility function to parse a date(time) tensor into a weekend tensor. Uses native tf functions only to avoid serialization issues.</p> <p>:param: datetime_tensor: date(time) string tensor. Must be in yyyy-MM-dd (HHss.SSS) format.</p> <p>WARNING: Dates are not checked for validity, so if you pass in a date such as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.</p> <p>:returns: Weekend tensor, stored as tf.int64.</p> Source code in <code>src/kamae/tensorflow/utils/date_utils.py</code> <pre><code>def datetime_is_weekend(datetime_tensor: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"\n    Utility function to parse a date(time) tensor into a weekend tensor.\n    Uses native tf functions only to avoid serialization issues.\n\n    :param: datetime_tensor: date(time) string tensor.\n    Must be in yyyy-MM-dd (HH:mm:ss.SSS) format.\n\n    WARNING: Dates are not checked for validity, so if you pass in a date such\n    as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.\n\n    :returns: Weekend tensor, stored as tf.int64.\n    \"\"\"\n    week_day = datetime_weekday(datetime_tensor)\n    # Compute the weekend\n    is_weekend = tf.cast(tf.where(week_day &gt; 5, 1, 0), tf.int64)\n    return is_weekend\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/date_utils/#src.kamae.tensorflow.utils.date_utils.datetime_millisecond","title":"datetime_millisecond","text":"<pre><code>datetime_millisecond(datetime_tensor)\n</code></pre> <p>Utility function to parse a date(time) tensor into a millisecond tensor. Uses native tf functions only to avoid serialization issues.</p> <p>:param: datetime_tensor: date(time) string tensor. Must be in yyyy-MM-dd (HHss.SSS) format.</p> <p>WARNING: Dates are not checked for validity, so if you pass in a date such as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.</p> <p>:returns: Millisecond tensor, stored as tf.int64.</p> Source code in <code>src/kamae/tensorflow/utils/date_utils.py</code> <pre><code>def datetime_millisecond(datetime_tensor: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"\n    Utility function to parse a date(time) tensor into a millisecond tensor.\n    Uses native tf functions only to avoid serialization issues.\n\n    :param: datetime_tensor: date(time) string tensor.\n    Must be in yyyy-MM-dd (HH:mm:ss.SSS) format.\n\n    WARNING: Dates are not checked for validity, so if you pass in a date such\n    as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.\n\n    :returns: Millisecond tensor, stored as tf.int64.\n    \"\"\"\n    datetime_tensor = add_missing_time_components_to_datetime_tensor(datetime_tensor)\n    millisecond = tf.strings.to_number(\n        tf.strings.substr(datetime_tensor, 20, 3), out_type=tf.int64\n    )\n    return millisecond\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/date_utils/#src.kamae.tensorflow.utils.date_utils.datetime_minute","title":"datetime_minute","text":"<pre><code>datetime_minute(datetime_tensor)\n</code></pre> <p>Utility function to parse a date(time) tensor into a minute tensor. Uses native tf functions only to avoid serialization issues.</p> <p>:param: datetime_tensor: date(time) string tensor. Must be in yyyy-MM-dd (HHss.SSS) format.</p> <p>WARNING: Dates are not checked for validity, so if you pass in a date such as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.</p> <p>:returns: Minute tensor, stored as tf.int64.</p> Source code in <code>src/kamae/tensorflow/utils/date_utils.py</code> <pre><code>def datetime_minute(datetime_tensor: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"\n    Utility function to parse a date(time) tensor into a minute tensor.\n    Uses native tf functions only to avoid serialization issues.\n\n    :param: datetime_tensor: date(time) string tensor.\n    Must be in yyyy-MM-dd (HH:mm:ss.SSS) format.\n\n    WARNING: Dates are not checked for validity, so if you pass in a date such\n    as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.\n\n    :returns: Minute tensor, stored as tf.int64.\n    \"\"\"\n    datetime_tensor = add_missing_time_components_to_datetime_tensor(\n        datetime_tensor, max_len=16\n    )\n    minute = tf.strings.to_number(\n        tf.strings.substr(datetime_tensor, 14, 2), out_type=tf.int64\n    )\n    return minute\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/date_utils/#src.kamae.tensorflow.utils.date_utils.datetime_month","title":"datetime_month","text":"<pre><code>datetime_month(datetime_tensor)\n</code></pre> <p>Utility function to parse a date(time) tensor into a month tensor. Uses native tf functions only to avoid serialization issues.</p> <p>:param: datetime_tensor: date(time) string tensor. Must be in yyyy-MM-dd (HHss.SSS) format.</p> <p>WARNING: Dates are not checked for validity, so if you pass in a date such as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.</p> <p>:returns: Month tensor, stored as tf.int64.</p> Source code in <code>src/kamae/tensorflow/utils/date_utils.py</code> <pre><code>def datetime_month(datetime_tensor: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"\n    Utility function to parse a date(time) tensor into a month tensor.\n    Uses native tf functions only to avoid serialization issues.\n\n    :param: datetime_tensor: date(time) string tensor.\n    Must be in yyyy-MM-dd (HH:mm:ss.SSS) format.\n\n    WARNING: Dates are not checked for validity, so if you pass in a date such\n    as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.\n\n    :returns: Month tensor, stored as tf.int64.\n    \"\"\"\n    month = tf.strings.to_number(\n        tf.strings.substr(datetime_tensor, 5, 2), out_type=tf.int64\n    )\n    return month\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/date_utils/#src.kamae.tensorflow.utils.date_utils.datetime_second","title":"datetime_second","text":"<pre><code>datetime_second(datetime_tensor)\n</code></pre> <p>Utility function to parse a date(time) tensor into a second tensor. Uses native tf functions only to avoid serialization issues.</p> <p>:param: datetime_tensor: date(time) string tensor. Must be in yyyy-MM-dd (HHss.SSS) format.</p> <p>WARNING: Dates are not checked for validity, so if you pass in a date such as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.</p> <p>:returns: Second tensor, stored as tf.int64.</p> Source code in <code>src/kamae/tensorflow/utils/date_utils.py</code> <pre><code>def datetime_second(datetime_tensor: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"\n    Utility function to parse a date(time) tensor into a second tensor.\n    Uses native tf functions only to avoid serialization issues.\n\n    :param: datetime_tensor: date(time) string tensor.\n    Must be in yyyy-MM-dd (HH:mm:ss.SSS) format.\n\n    WARNING: Dates are not checked for validity, so if you pass in a date such\n    as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.\n\n    :returns: Second tensor, stored as tf.int64.\n    \"\"\"\n    datetime_tensor = add_missing_time_components_to_datetime_tensor(\n        datetime_tensor, max_len=19\n    )\n    second = tf.strings.to_number(\n        tf.strings.substr(datetime_tensor, 17, 2), out_type=tf.int64\n    )\n    return second\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/date_utils/#src.kamae.tensorflow.utils.date_utils.datetime_to_unix_timestamp","title":"datetime_to_unix_timestamp","text":"<pre><code>datetime_to_unix_timestamp(datetime_tensor)\n</code></pre> <p>Converts a date string tensor into a timestamp tensor (seconds since Unix Epoch).</p> <p>:param datetime_tensor: the date tensor to convert.     Must be in yyyy-MM-dd (HHss.SSS) format. :returns: Timestamp tensor in seconds since Unix Epoch</p> Source code in <code>src/kamae/tensorflow/utils/date_utils.py</code> <pre><code>def datetime_to_unix_timestamp(datetime_tensor: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"\n    Converts a date string tensor into a timestamp tensor (seconds since Unix Epoch).\n\n    :param datetime_tensor: the date tensor to convert.\n        Must be in yyyy-MM-dd (HH:mm:ss.SSS) format.\n    :returns: Timestamp tensor in seconds since Unix Epoch\n    \"\"\"\n    return datetime_total_seconds(datetime_tensor)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/date_utils/#src.kamae.tensorflow.utils.date_utils.datetime_total_days","title":"datetime_total_days","text":"<pre><code>datetime_total_days(datetime_tensor)\n</code></pre> <p>Utility function to parse a date(time) tensor into a total days tensor. Uses native tf functions only to avoid serialization issues.</p> <p>:param: datetime_tensor: date(time) string tensor. Must be in yyyy-MM-dd (HHss.SSS) format.</p> <p>WARNING: Dates are not checked for validity, so if you pass in a date such as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.</p> <p>:returns: Total days tensor, stored as tf.int64.</p> Source code in <code>src/kamae/tensorflow/utils/date_utils.py</code> <pre><code>def datetime_total_days(datetime_tensor: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"\n    Utility function to parse a date(time) tensor into a total days tensor.\n    Uses native tf functions only to avoid serialization issues.\n\n    :param: datetime_tensor: date(time) string tensor.\n    Must be in yyyy-MM-dd (HH:mm:ss.SSS) format.\n\n    WARNING: Dates are not checked for validity, so if you pass in a date such\n    as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.\n\n    :returns: Total days tensor, stored as tf.int64.\n    \"\"\"\n    year = datetime_year(datetime_tensor)\n    day = datetime_day(datetime_tensor)\n    first_century_year_post_1970 = tf.constant([2000], dtype=tf.int64)\n    num_standard_days = (year - 1970) * 365\n    # Compute the number of leap years to know if we need to add extra days.\n    # We only consider year - 1, since if we are currently in a leap year, this will\n    # be catered for in days_to_month.\n    num_standard_leap_years = ((year - 1) - 1972) // 4\n    num_century_years = tf.where(\n        year &gt; first_century_year_post_1970,\n        ((year - 1) - first_century_year_post_1970) // 100,\n        0,\n    )\n    num_century_leap_years = tf.where(\n        year &gt; first_century_year_post_1970,\n        ((year - 1) - first_century_year_post_1970) // 400,\n        0,\n    )\n    # Subtract all century years and add all century leap years.\n    num_leap_years = (\n        num_standard_leap_years - num_century_years + num_century_leap_years\n    )\n    # Days to year is the number of standard days across all the years plus the number\n    # of leap years (as each leap year adds exactly 1 day)\n    days_to_year = num_standard_days + num_leap_years\n    days_to_month = datetime_days_to_month(datetime_tensor)\n    # Add all the days together\n    total_days = days_to_year + days_to_month + day\n\n    return total_days\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/date_utils/#src.kamae.tensorflow.utils.date_utils.datetime_total_milliseconds","title":"datetime_total_milliseconds","text":"<pre><code>datetime_total_milliseconds(datetime_tensor)\n</code></pre> <p>Utility function to parse a date(time) tensor into a total milliseconds tensor. Uses native tf functions only to avoid serialization issues.</p> <p>:param: datetime_tensor: date(time) string tensor. Must be in yyyy-MM-dd (HHss.SSS) format.</p> <p>WARNING: Dates are not checked for validity, so if you pass in a date such as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.</p> <p>:returns: Total milliseconds tensor, stored as tf.int64.</p> Source code in <code>src/kamae/tensorflow/utils/date_utils.py</code> <pre><code>def datetime_total_milliseconds(datetime_tensor: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"\n    Utility function to parse a date(time) tensor into a total milliseconds tensor.\n    Uses native tf functions only to avoid serialization issues.\n\n    :param: datetime_tensor: date(time) string tensor.\n    Must be in yyyy-MM-dd (HH:mm:ss.SSS) format.\n\n    WARNING: Dates are not checked for validity, so if you pass in a date such\n    as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.\n\n    :returns: Total milliseconds tensor, stored as tf.int64.\n    \"\"\"\n    # Extract date parts\n    total_days = datetime_total_days(datetime_tensor)\n    hour = datetime_hour(datetime_tensor)\n    minute = datetime_minute(datetime_tensor)\n    second = datetime_second(datetime_tensor)\n    millisecond = datetime_millisecond(datetime_tensor)\n    # Add all the milliseconds together\n    total_milliseconds = (\n        (total_days * 24 * 60 * 60 * 1000)\n        + (hour * 60 * 60 * 1000)\n        + (minute * 60 * 1000)\n        + (second * 1000)\n        + millisecond\n    )\n    return total_milliseconds\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/date_utils/#src.kamae.tensorflow.utils.date_utils.datetime_total_seconds","title":"datetime_total_seconds","text":"<pre><code>datetime_total_seconds(datetime_tensor)\n</code></pre> <p>Utility function to parse a date(time) tensor into a total seconds tensor. Uses native tf functions only to avoid serialization issues.</p> <p>:param: datetime_tensor: date(time) string tensor. Must be in yyyy-MM-dd (HHss.SSS) format.</p> <p>WARNING: Dates are not checked for validity, so if you pass in a date such as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.</p> <p>:returns: Total seconds tensor, stored as tf.int64.</p> Source code in <code>src/kamae/tensorflow/utils/date_utils.py</code> <pre><code>def datetime_total_seconds(datetime_tensor: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"\n    Utility function to parse a date(time) tensor into a total seconds tensor.\n    Uses native tf functions only to avoid serialization issues.\n\n    :param: datetime_tensor: date(time) string tensor.\n    Must be in yyyy-MM-dd (HH:mm:ss.SSS) format.\n\n    WARNING: Dates are not checked for validity, so if you pass in a date such\n    as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.\n\n    :returns: Total seconds tensor, stored as tf.int64.\n    \"\"\"\n    # Extract date parts\n    total_days = tf.cast(datetime_total_days(datetime_tensor), dtype=tf.float64)\n    hour = tf.cast(datetime_hour(datetime_tensor), dtype=tf.float64)\n    minute = tf.cast(datetime_minute(datetime_tensor), dtype=tf.float64)\n    second = tf.cast(datetime_second(datetime_tensor), dtype=tf.float64)\n    milliseconds = tf.cast(datetime_millisecond(datetime_tensor), dtype=tf.float64)\n    # Add all the seconds together\n    total_seconds = (\n        (total_days * 24 * 60 * 60)\n        + (hour * 60 * 60)\n        + (minute * 60)\n        + second\n        + (milliseconds / tf.constant(1000.0, dtype=tf.float64))\n    )\n    return total_seconds\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/date_utils/#src.kamae.tensorflow.utils.date_utils.datetime_weekday","title":"datetime_weekday","text":"<pre><code>datetime_weekday(datetime_tensor)\n</code></pre> <p>Utility function to parse a date(time) tensor into a weekday tensor. Uses native tf functions only to avoid serialization issues.</p> <p>:param: datetime_tensor: date(time) string tensor. Must be in yyyy-MM-dd (HHss.SSS) format.</p> <p>WARNING: Dates are not checked for validity, so if you pass in a date such as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.</p> <p>:returns: Weekday tensor, stored as tf.int64.</p> Source code in <code>src/kamae/tensorflow/utils/date_utils.py</code> <pre><code>def datetime_weekday(datetime_tensor: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"\n    Utility function to parse a date(time) tensor into a weekday tensor.\n    Uses native tf functions only to avoid serialization issues.\n\n    :param: datetime_tensor: date(time) string tensor.\n    Must be in yyyy-MM-dd (HH:mm:ss.SSS) format.\n\n    WARNING: Dates are not checked for validity, so if you pass in a date such\n    as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.\n\n    :returns: Weekday tensor, stored as tf.int64.\n    \"\"\"\n    total_days = datetime_total_days(datetime_tensor)\n    # Compute the weekday\n    week_day = (total_days - 4) % 7 + 1\n    return week_day\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/date_utils/#src.kamae.tensorflow.utils.date_utils.datetime_year","title":"datetime_year","text":"<pre><code>datetime_year(datetime_tensor)\n</code></pre> <p>Utility function to parse a date(time) tensor into a year tensor. Uses native tf functions only to avoid serialization issues.</p> <p>:param: datetime_tensor: date(time) string tensor. Must be in yyyy-MM-dd (HHss.SSS) format.</p> <p>WARNING: Dates are not checked for validity, so if you pass in a date such as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.</p> <p>:returns: Year tensor, stored as tf.int64.</p> Source code in <code>src/kamae/tensorflow/utils/date_utils.py</code> <pre><code>def datetime_year(datetime_tensor: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"\n    Utility function to parse a date(time) tensor into a year tensor.\n    Uses native tf functions only to avoid serialization issues.\n\n    :param: datetime_tensor: date(time) string tensor.\n    Must be in yyyy-MM-dd (HH:mm:ss.SSS) format.\n\n    WARNING: Dates are not checked for validity, so if you pass in a date such\n    as \"2020-02-30\" no errors will be thrown, and you will get a nonsense output.\n\n    :returns: Year tensor, stored as tf.int64.\n    \"\"\"\n    year = tf.strings.to_number(\n        tf.strings.substr(datetime_tensor, 0, 4), out_type=tf.int64\n    )\n    return year\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/date_utils/#src.kamae.tensorflow.utils.date_utils.unix_timestamp_to_datetime","title":"unix_timestamp_to_datetime","text":"<pre><code>unix_timestamp_to_datetime(\n    timestamp_tensor, include_time=True\n)\n</code></pre> <p>Converts a timestamp tensor (seconds since Unix Epoch) into a datetime string tensor. If include_time is False, the output will be in yyyy-MM-dd, if include_time is True, the output will be in yyyy-MM-dd HHss.SSS format.</p> <p>:param timestamp_tensor: the timestamp tensor to convert. Timestamps must be in seconds since unix epoch. :param include_time: Whether to include the time in the output. If True, the output will be in yyyy-MM-dd HHss.SSS format. If False, the output will be in yyyy-MM-dd format. Default is True. :returns: Datetime string tensor in either yyyy-MM-dd or yyyy-MM-dd HHss.SSS format.</p> Source code in <code>src/kamae/tensorflow/utils/date_utils.py</code> <pre><code>def unix_timestamp_to_datetime(\n    timestamp_tensor: tf.Tensor, include_time: bool = True\n) -&gt; tf.Tensor:\n    \"\"\"\n    Converts a timestamp tensor (seconds since Unix Epoch) into a datetime string\n    tensor. If include_time is False, the output will be in yyyy-MM-dd, if include_time\n    is True, the output will be in yyyy-MM-dd HH:mm:ss.SSS format.\n\n    :param timestamp_tensor: the timestamp tensor to convert.\n    Timestamps must be in seconds since unix epoch.\n    :param include_time: Whether to include the time in the output. If True, the output\n    will be in yyyy-MM-dd HH:mm:ss.SSS format. If False, the output will be in\n    yyyy-MM-dd format. Default is True.\n    :returns: Datetime string tensor in either yyyy-MM-dd or yyyy-MM-dd HH:mm:ss.SSS\n    format.\n    \"\"\"\n\n    # Days, hours, minutes and seconds since Unix Epoch\n    seconds_in_one_minute = tf.constant(60.0, dtype=tf.float64)\n    seconds_in_one_hour = tf.math.multiply(seconds_in_one_minute, 60.0)\n    seconds_in_one_day = tf.math.multiply(seconds_in_one_hour, 24.0)\n    total_days = tf.math.floordiv(timestamp_tensor, seconds_in_one_day)\n\n    # Initialise the remainder days variable\n    remainder_days = total_days\n    days_in_4_years = tf.constant(1461.0, dtype=tf.float64)\n    year = tf.add(\n        tf.constant(1970.0, dtype=tf.float64),\n        tf.multiply(\n            tf.math.floordiv(remainder_days, days_in_4_years),\n            tf.constant(4.0, dtype=tf.float64),\n        ),\n    )\n    remainder_days = tf.math.mod(remainder_days, days_in_4_years)\n\n    # Let k = the number of 4 year chunks since 1970\n    # We count from 1970 + 4k, so every 3rd year is a leap year\n    # (e.g. 1970 + 4k, 1971 + 4k, ^^1972 + 4^^)\n    # We don't need to count the last year as the remainder will get\n    # carried on to the next loop where the month is computed\n    # TODO: Is there a better abstraction instead of for loops?\n    #  These are O(1) operations, but feel clunky and also not very clear\n    year_days = [\n        tf.constant(365.0, dtype=tf.float64),\n        tf.constant(365.0, dtype=tf.float64),\n        tf.constant(366.0, dtype=tf.float64),\n    ]\n    for d in year_days:\n        year_passed = tf.where(\n            remainder_days &gt;= d,\n            tf.constant(1.0, dtype=tf.float64),\n            tf.constant(0.0, dtype=tf.float64),\n        )\n        year += year_passed\n        remainder_days -= year_passed * d\n\n    # The full days in year that have been realised\n    full_days_in_year = remainder_days\n\n    # Initialise month loop variables\n    # Days in the month (we treat leap years in the loop)\n    month_days = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n    months_to_month = tf.zeros_like(total_days)\n    remainder_days = full_days_in_year\n\n    # First loop starts from December and works backwards\n    for idx, _ in enumerate(month_days):\n        n_months = 12 - idx\n\n        cumulative_days_to_month = (\n            # Leap year treatment (if we are in a leap year)\n            # A leap year is one that is divisible by 4, unless it is divisible by 100\n            # but not divisible by 400\n            (\n                tf.where(\n                    (year % 4 == 0) &amp; ((year % 100 != 0) | (year % 400 == 0)),\n                    tf.constant(1.0, dtype=tf.float64),\n                    tf.constant(0.0, dtype=tf.float64),\n                )\n                * tf.where(\n                    n_months &gt;= 2,\n                    tf.constant(1.0, dtype=tf.float64),\n                    tf.constant(0.0, dtype=tf.float64),\n                )\n            )\n            # Cumulative days in a normal year\n            + sum(month_days[:n_months])\n        )\n\n        # Elements will be zero unless ALL cumulative_days_to_month have been realised,\n        # in which case the element will be 1\n        month_has_been_realised = remainder_days // cumulative_days_to_month\n        remainder_days -= month_has_been_realised * cumulative_days_to_month\n        months_to_month += n_months * month_has_been_realised\n\n    # The month we are in hasn't been realised fully, but we are in it (so +1)\n    month = months_to_month + 1\n    # The day we are in has not been realised fully, but we are in it (so +1)\n    day = remainder_days + 1\n\n    year_str = tf.strings.as_string(tf.cast(year, dtype=tf.int64))\n    month_str = tf.strings.as_string(tf.cast(month, dtype=tf.int64), width=2, fill=\"0\")\n    day_str = tf.strings.as_string(tf.cast(day, dtype=tf.int64), width=2, fill=\"0\")\n    date = tf.strings.join([year_str, month_str, day_str], \"-\")\n\n    if include_time:\n        leftover_seconds = timestamp_tensor - tf.math.multiply(\n            total_days, seconds_in_one_day\n        )\n        total_hours = tf.math.floordiv(leftover_seconds, seconds_in_one_hour)\n        leftover_seconds -= tf.math.multiply(total_hours, seconds_in_one_hour)\n\n        total_mins = tf.math.floordiv(leftover_seconds, seconds_in_one_minute)\n        leftover_seconds -= tf.math.multiply(total_mins, seconds_in_one_minute)\n        total_seconds = tf.math.floor(leftover_seconds)\n        total_milliseconds = leftover_seconds - total_seconds\n\n        hours_str = tf.strings.as_string(\n            tf.cast(total_hours, dtype=tf.int64), width=2, fill=\"0\"\n        )\n        minutes_str = tf.strings.as_string(\n            tf.cast(total_mins, dtype=tf.int64), width=2, fill=\"0\"\n        )\n        seconds_str = tf.strings.as_string(\n            tf.cast(total_seconds, dtype=tf.int64), width=2, fill=\"0\"\n        )\n        milliseconds_str = tf.strings.as_string(\n            # We need to round the milliseconds to fix them to 3 decimal places\n            tf.cast(tf.math.round(total_milliseconds * 1000.0), tf.int64),\n            width=3,\n            fill=\"0\",\n        )\n\n        time = tf.strings.join(\n            [\n                tf.strings.join([hours_str, minutes_str, seconds_str], \":\"),\n                milliseconds_str,\n            ],\n            \".\",\n        )\n        datetime = tf.strings.join([date, time], \" \")\n        return datetime\n\n    return date\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/input_utils/","title":"input_utils","text":"<p>Provides utilities for tensorflow layer inputs</p>"},{"location":"reference/src/kamae/tensorflow/utils/input_utils/#src.kamae.tensorflow.utils.input_utils.allow_single_or_multiple_tensor_input","title":"allow_single_or_multiple_tensor_input","text":"<pre><code>allow_single_or_multiple_tensor_input(layer_call_method)\n</code></pre> <p>Enforces that the inputs to a layer are either a single tensor or a list of tensors. If the inputs are an iterable, then we check that all elements are tensors. If the inputs are a tensor, then we return a list containing the tensor.</p> <p>:param layer_call_method: The layer's call method to decorate. :returns: The function called with a list of tensors.</p> Source code in <code>src/kamae/tensorflow/utils/input_utils.py</code> <pre><code>def allow_single_or_multiple_tensor_input(layer_call_method: Callable) -&gt; Callable:\n    \"\"\"\n    Enforces that the inputs to a layer are either a single tensor or a list of tensors.\n    If the inputs are an iterable, then we check that all elements are tensors. If the\n    inputs are a tensor, then we return a list containing the tensor.\n\n    :param layer_call_method: The layer's call method to decorate.\n    :returns: The function called with a list of tensors.\n    \"\"\"\n\n    def _allow_single_or_multiple_tensor_input(\n        self: Any,\n        inputs: Union[Tensor, Iterable[Tensor]],\n        **kwargs: Any,\n    ) -&gt; List[Tensor]:\n        if tf.is_tensor(inputs):\n            processed_inputs = [inputs]\n        else:\n            input_list = list(iter_values(inputs))\n            if all([tf.is_tensor(input_tensor) for input_tensor in input_list]):\n                processed_inputs = input_list\n            else:\n                raise ValueError(\n                    \"\"\"All elements of the inputs must be tensors, but got an iterable\n                    containing non-tensors.\"\"\"\n                )\n        return layer_call_method(self, processed_inputs, **kwargs)\n\n    return _allow_single_or_multiple_tensor_input\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/input_utils/#src.kamae.tensorflow.utils.input_utils.enforce_multiple_tensor_input","title":"enforce_multiple_tensor_input","text":"<pre><code>enforce_multiple_tensor_input(layer_call_method)\n</code></pre> <p>Enforces that the inputs to a layer are an iterable of tensors. We check that all elements are tensors. If the inputs are a single tensor, rather than an iterable we raise an error.</p> <p>:param layer_call_method: The layer's call method to decorate. :raises TypeError: If the inputs are a single tensor, an iterable of length 1 or an iterable of non-tensors. :returns: The function called with a list of tensors.</p> Source code in <code>src/kamae/tensorflow/utils/input_utils.py</code> <pre><code>def enforce_multiple_tensor_input(layer_call_method: Callable) -&gt; Callable:\n    \"\"\"\n    Enforces that the inputs to a layer are an iterable of tensors.\n    We check that all elements are tensors. If the inputs are a single tensor, rather\n    than an iterable we raise an error.\n\n    :param layer_call_method: The layer's call method to decorate.\n    :raises TypeError: If the inputs are a single tensor, an iterable of length 1\n    or an iterable of non-tensors.\n    :returns: The function called with a list of tensors.\n    \"\"\"\n\n    def _enforce_multiple_tensor_input(\n        self: Any,\n        inputs: Union[Tensor, Iterable[Tensor]],\n        **kwargs: Any,\n    ) -&gt; List[Tensor]:\n        if tf.is_tensor(inputs):\n            raise ValueError(\n                \"\"\"Expected inputs to be a iterable of tensors,\n                but got a single tensor.\"\"\"\n            )\n        else:\n            input_list = list(iter_values(inputs))\n            if len(input_list) &gt; 1 and all(\n                [tf.is_tensor(input_tensor) for input_tensor in input_list]\n            ):\n                processed_inputs = input_list\n            else:\n                raise ValueError(\n                    \"\"\"Invalid inputs. Expected inputs to be an iterable of tensors,\n                    but either got an iterable of non-tensors or a single tensor.\"\"\"\n                )\n        return layer_call_method(self, processed_inputs, **kwargs)\n\n    return _enforce_multiple_tensor_input\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/input_utils/#src.kamae.tensorflow.utils.input_utils.enforce_single_tensor_input","title":"enforce_single_tensor_input","text":"<pre><code>enforce_single_tensor_input(layer_call_method)\n</code></pre> <p>Enforces that the inputs to a layer are a single tensor. If the inputs are an iterable, then we check it has a single element and that the element is a tensor. If the inputs are a tensor, then we return the tensor.</p> <p>:param layer_call_method: The layer's call method to decorate. :raises TypeError: If the inputs are an iterable with more than one element. :returns: The function called with a single tensor.</p> Source code in <code>src/kamae/tensorflow/utils/input_utils.py</code> <pre><code>def enforce_single_tensor_input(layer_call_method: Callable) -&gt; Callable:\n    \"\"\"\n    Enforces that the inputs to a layer are a single tensor. If the inputs are an\n    iterable, then we check it has a single element and that the element is a tensor.\n    If the inputs are a tensor, then we return the tensor.\n\n    :param layer_call_method: The layer's call method to decorate.\n    :raises TypeError: If the inputs are an iterable with more than one element.\n    :returns: The function called with a single tensor.\n    \"\"\"\n\n    def _enforce_single_tensor_input(\n        self: Any,\n        inputs: Union[Tensor, Iterable[Tensor]],\n        **kwargs: Any,\n    ) -&gt; Tensor:\n        if tf.is_tensor(inputs):\n            # If the inputs are a tensor, then we return the tensor.\n            processed_inputs = inputs\n        else:\n            input_list = list(iter_values(inputs))\n            if len(input_list) == 1 and tf.is_tensor(input_list[0]):\n                # If the inputs are an iterable with a single tensor,\n                # then we return the tensor.\n                processed_inputs = input_list[0]\n            else:\n                # Otherwise, we raise an error.\n                raise ValueError(\n                    f\"\"\"Expected inputs to be a single tensor, but got a list of\n                    {len(input_list)} tensors.\"\"\"\n                )\n        return layer_call_method(self, processed_inputs, **kwargs)\n\n    return _enforce_single_tensor_input\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/input_utils/#src.kamae.tensorflow.utils.input_utils.iter_values","title":"iter_values","text":"<pre><code>iter_values(x)\n</code></pre> <p>Returns an iterator over the values of a generic iterator. Will be used to construct lists from iterables such as lists, tuples, dicts, etc.</p> <p>:param x: An iterable :returns: An iterator over the values of the iterable.</p> Source code in <code>src/kamae/tensorflow/utils/input_utils.py</code> <pre><code>def iter_values(x: Iterable) -&gt; Iterable:\n    \"\"\"\n    Returns an iterator over the values of a generic iterator.\n    Will be used to construct lists from iterables such as lists, tuples, dicts, etc.\n\n    :param x: An iterable\n    :returns: An iterator over the values of the iterable.\n    \"\"\"\n    if hasattr(x, \"itervalues\"):\n        return x.itervalues()\n    if hasattr(x, \"values\"):\n        return iter(x.values())\n    return iter(x)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/layer_utils/","title":"layer_utils","text":""},{"location":"reference/src/kamae/tensorflow/utils/layer_utils/#src.kamae.tensorflow.utils.layer_utils.NormalizeLayer","title":"NormalizeLayer","text":"<pre><code>NormalizeLayer(\n    mean,\n    variance,\n    name=None,\n    input_dtype=None,\n    output_dtype=None,\n    axis=-1,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseLayer</code></p> <p>Intermediate layer for normalization layers.</p> <p>Reduces code duplication by providing a common interface for normalization layers.</p> <p>:param mean: The mean value(s) to use during normalization. The passed value(s) will be broadcast to the shape of the kept axes above; if the value(s) cannot be broadcast, an error will be raised when this layer's <code>build()</code> method is called. :param variance: The variance value(s) to use during normalization. The passed value(s) will be broadcast to the shape of the kept axes above; if the value(s) cannot be broadcast, an error will be raised when this layer's <code>build()</code> method is called. :param name: The name of the layer. Defaults to <code>None</code>. :param input_dtype: The dtype to cast the input to. Defaults to <code>None</code>. :param output_dtype: The dtype to cast the output to. Defaults to <code>None</code>. :param axis: Integer, tuple of integers, or None. The axis or axes that should have a separate mean and variance for each index in the shape. For example, if shape is <code>(None, 5)</code> and <code>axis=1</code>, the layer will track 5 separate mean and variance values for the last axis. If <code>axis</code> is set to <code>None</code>, the layer will normalize all elements in the input by a scalar mean and variance. Defaults to -1, where the last axis of the input is assumed to be a feature dimension and is normalized per index. Note that in the specific case of batched scalar inputs where the only axis is the batch axis, the default will normalize each index in the batch separately. In this case, consider passing <code>axis=None</code>.</p> Source code in <code>src/kamae/tensorflow/utils/layer_utils.py</code> <pre><code>def __init__(\n    self,\n    mean: Union[List[float], np.array],\n    variance: Union[List[float], np.array],\n    name: Optional[str] = None,\n    input_dtype: Optional[str] = None,\n    output_dtype: Optional[str] = None,\n    axis: Optional[Union[int, tuple[int]]] = -1,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the NormalizeLayer\n\n    :param mean: The mean value(s) to use during normalization. The passed value(s)\n    will be broadcast to the shape of the kept axes above; if the value(s)\n    cannot be broadcast, an error will be raised when this layer's\n    `build()` method is called.\n    :param variance: The variance value(s) to use during normalization. The passed\n    value(s) will be broadcast to the shape of the kept axes above; if the\n    value(s) cannot be broadcast, an error will be raised when this\n    layer's `build()` method is called.\n    :param name: The name of the layer. Defaults to `None`.\n    :param input_dtype: The dtype to cast the input to. Defaults to `None`.\n    :param output_dtype: The dtype to cast the output to. Defaults to `None`.\n    :param axis: Integer, tuple of integers, or None. The axis or axes that should\n    have a separate mean and variance for each index in the shape. For\n    example, if shape is `(None, 5)` and `axis=1`, the layer will track 5\n    separate mean and variance values for the last axis. If `axis` is set\n    to `None`, the layer will normalize all elements in the input by a\n    scalar mean and variance. Defaults to -1, where the last axis of the\n    input is assumed to be a feature dimension and is normalized per\n    index. Note that in the specific case of batched scalar inputs where\n    the only axis is the batch axis, the default will normalize each index\n    in the batch separately. In this case, consider passing `axis=None`.\n    \"\"\"\n    super().__init__(\n        name=name, input_dtype=input_dtype, output_dtype=output_dtype, **kwargs\n    )\n    # Standardize `axis` to a tuple.\n    if axis is None:\n        axis = ()\n    elif isinstance(axis, int):\n        axis = (axis,)\n    else:\n        axis = tuple(axis)\n\n    self.axis = axis\n    self.input_mean = mean\n    self.input_variance = variance\n    self.epsilon = 1e-8\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/layer_utils/#src.kamae.tensorflow.utils.layer_utils.NormalizeLayer.compatible_dtypes","title":"compatible_dtypes  <code>property</code>","text":"<pre><code>compatible_dtypes\n</code></pre> <p>Returns the compatible dtypes of the layer.</p> <p>:returns: The compatible dtypes of the layer.</p>"},{"location":"reference/src/kamae/tensorflow/utils/layer_utils/#src.kamae.tensorflow.utils.layer_utils.NormalizeLayer.build","title":"build","text":"<pre><code>build(input_shape)\n</code></pre> <p>Builds shapes for the mean and variance tensors.</p> <p>Specifically, understands which axis to compute the normalization across and broadcasts the mean and variance tensors to match the input shape.</p> <p>:param input_shape: The shape of the input tensor. :returns: None - layer is built.</p> Source code in <code>src/kamae/tensorflow/utils/layer_utils.py</code> <pre><code>def build(self, input_shape: Tuple[int]) -&gt; None:\n    \"\"\"\n    Builds shapes for the mean and variance tensors.\n\n    Specifically, understands which axis to compute the normalization across\n    and broadcasts the mean and variance tensors to match the input shape.\n\n    :param input_shape: The shape of the input tensor.\n    :returns: None - layer is built.\n    \"\"\"\n    super().build(input_shape)\n\n    if isinstance(input_shape, (list, tuple)) and all(\n        isinstance(shape, (tf.TensorShape, list, tuple)) for shape in input_shape\n    ):\n        # This seems to be needed to handle sending in multiple inputs as a list.\n        # Although this layer should only have one input, so this is a bit of a\n        # hack. We catch this nicely in call method with a decorator. Maybe we\n        # should do the same here?\n        input_shape = input_shape[0]\n\n    input_shape = tf.TensorShape(input_shape).as_list()\n    ndim = len(input_shape)\n    self._build_input_shape = input_shape\n\n    if any(a &lt; -ndim or a &gt;= ndim for a in self.axis):\n        raise ValueError(\n            f\"\"\"All `axis` values must be in the range [-ndim, ndim). \"\n            Found ndim: `{ndim}`, axis: {self.axis}\"\"\"\n        )\n\n    # Axes to be kept, replacing negative values with positive equivalents.\n    # Sorted to avoid transposing axes.\n    keep_axis = sorted([d if d &gt;= 0 else d + ndim for d in self.axis])\n    # All axes to be kept should have known shape.\n    for d in keep_axis:\n        if input_shape[d] is None:\n            raise ValueError(\n                f\"\"\"All `axis` values to be kept must have known shape. \"\n                Got axis: {self.axis},\n                input shape: {input_shape}, with unknown axis at index: {d}\"\"\"\n            )\n    # Broadcast any reduced axes.\n    broadcast_shape = [input_shape[d] if d in keep_axis else 1 for d in range(ndim)]\n    mean_and_var_shape = tuple(input_shape[d] for d in keep_axis)\n    mean = self.input_mean * np.ones(mean_and_var_shape)\n    variance = self.input_variance * np.ones(mean_and_var_shape)\n    self.mean = tf.reshape(mean, broadcast_shape)\n    self.variance = tf.reshape(variance, broadcast_shape)\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/layer_utils/#src.kamae.tensorflow.utils.layer_utils.NormalizeLayer.get_config","title":"get_config","text":"<pre><code>get_config()\n</code></pre> <p>Gets the configuration of the StandardScaleLayer layer. Used for saving and loading from a model. Specifically adds additional parameters to the base configuration. :returns: Dictionary of the configuration of the layer.</p> Source code in <code>src/kamae/tensorflow/utils/layer_utils.py</code> <pre><code>def get_config(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Gets the configuration of the StandardScaleLayer layer.\n    Used for saving and loading from a model.\n    Specifically adds additional parameters to the base configuration.\n    :returns: Dictionary of the configuration of the layer.\n    \"\"\"\n    config = super().get_config()\n    # Ensure mean and variance are lists for serialization.\n    config.update(\n        {\n            \"mean\": listify_tensors(self.input_mean),\n            \"variance\": listify_tensors(self.input_variance),\n            \"axis\": self.axis,\n        }\n    )\n    return config\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/list_utils/","title":"list_utils","text":""},{"location":"reference/src/kamae/tensorflow/utils/list_utils/#src.kamae.tensorflow.utils.list_utils.get_top_n","title":"get_top_n","text":"<pre><code>get_top_n(\n    val_tensor, axis, sort_tensor, top_n, sort_order=\"asc\"\n)\n</code></pre> <p>Get the top N items from the value tensor based on their position in the sort tensor, ordered by the sort order ('asc' or 'desc').</p> <p>:param val_tensor: Value tensor. :param axis: Axis to get the top N items. :param sort_tensor: Sort tensor. :param top_n: Number of top values to consider. :param sort_order: Order to sort the values by. Default is \"asc\". :returns: Tensor of the top N items</p> Source code in <code>src/kamae/tensorflow/utils/list_utils.py</code> <pre><code>def get_top_n(\n    val_tensor: Tensor,\n    axis: int,\n    sort_tensor: Tensor,\n    top_n: int,\n    sort_order: str = \"asc\",\n) -&gt; Tensor:\n    \"\"\"\n    Get the top N items from the value tensor based on their position in\n    the sort tensor, ordered by the sort order ('asc' or 'desc').\n\n    :param val_tensor: Value tensor.\n    :param axis: Axis to get the top N items.\n    :param sort_tensor: Sort tensor.\n    :param top_n: Number of top values to consider.\n    :param sort_order: Order to sort the values by. Default is \"asc\".\n    :returns: Tensor of the top N items\n    \"\"\"\n\n    # If K is less than the number of items at real time,\n    # replace K with the number of items in the list\n    top_n = tf.minimum(top_n, tf.shape(sort_tensor)[axis])\n\n    # Define sort direction\n    sort_tensor_with_order = None\n    if sort_order == \"desc\":\n        sort_tensor_with_order = sort_tensor\n    elif sort_order == \"asc\":\n        sort_tensor_with_order = -sort_tensor\n    else:\n        ValueError(f\"Invalid sort_order: {sort_order}\")\n\n    # If value of shape at position (axis + 1) is equal to 1, squeeze this dimension,\n    # otherwise the top_k would complain about the shape mismatch\n    # If we apply squeeze without axis, the inference when batch_size=1 would fail\n    if len(sort_tensor_with_order.shape) &gt; axis + 1:\n        if sort_tensor_with_order.shape[axis + 1] == 1:\n            sort_tensor_with_order = tf.squeeze(sort_tensor_with_order, axis=axis + 1)\n\n    # Get the indices of the top N items, using the sort tensor\n    _, sorted_indices = tf.math.top_k(sort_tensor_with_order, k=top_n, sorted=True)\n\n    # Gather elements from the value tensor using the top-k indices\n    return tf.gather(\n        val_tensor,\n        sorted_indices,\n        batch_dims=axis,\n        axis=axis,\n    )\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/list_utils/#src.kamae.tensorflow.utils.list_utils.listify_tensors","title":"listify_tensors","text":"<pre><code>listify_tensors(x)\n</code></pre> <p>Converts any tensors or numpy arrays to lists for config serialization.</p> <p>:param x: The input tensor or numpy array. :returns: The input as a list.</p> Source code in <code>src/kamae/tensorflow/utils/list_utils.py</code> <pre><code>def listify_tensors(x: Union[tf.Tensor, np.ndarray, List[Any]]) -&gt; List[Any]:\n    \"\"\"\n    Converts any tensors or numpy arrays to lists for config serialization.\n\n    :param x: The input tensor or numpy array.\n    :returns: The input as a list.\n    \"\"\"\n    if tf.is_tensor(x):\n        x = x.numpy()\n    if isinstance(x, np.ndarray):\n        x = x.tolist()\n    return x\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/shape_utils/","title":"shape_utils","text":""},{"location":"reference/src/kamae/tensorflow/utils/shape_utils/#src.kamae.tensorflow.utils.shape_utils.reshape_to_equal_rank","title":"reshape_to_equal_rank","text":"<pre><code>reshape_to_equal_rank(inputs)\n</code></pre> <p>Reshapes the input tensors to match the rank of the largest tensor.</p> <p>:param inputs: The input tensors to reshape. :return: The reshaped input tensors.</p> Source code in <code>src/kamae/tensorflow/utils/shape_utils.py</code> <pre><code>def reshape_to_equal_rank(inputs: Iterable[Tensor]) -&gt; List[Tensor]:\n    \"\"\"\n    Reshapes the input tensors to match the rank of the largest tensor.\n\n    :param inputs: The input tensors to reshape.\n    :return: The reshaped input tensors.\n    \"\"\"\n    max_rank = max([len(tensor.shape) for tensor in inputs])\n    reshaped_inputs = []\n    for x in inputs:\n        rank_diff = max_rank - len(x.shape)\n        if rank_diff &gt; 0:\n            reshape_dim = tf.concat(\n                [\n                    tf.shape(x)[:-1],\n                    tf.ones(rank_diff, dtype=tf.int32),\n                    tf.shape(x)[-1:],\n                ],\n                axis=0,\n            )\n            x = tf.reshape(x, reshape_dim)\n        reshaped_inputs.append(x)\n    return reshaped_inputs\n</code></pre>"},{"location":"reference/src/kamae/tensorflow/utils/transform_utils/","title":"transform_utils","text":""},{"location":"reference/src/kamae/tensorflow/utils/transform_utils/#src.kamae.tensorflow.utils.transform_utils.map_fn_w_axis","title":"map_fn_w_axis","text":"<pre><code>map_fn_w_axis(\n    elems,\n    fn,\n    fn_output_signature,\n    axis=-1,\n    parallel_iterations=None,\n    swap_memory=False,\n    infer_shape=True,\n    name=None,\n)\n</code></pre> <p>Applies a function to a specific axis of a tensor using map_fn. Specifically uses <code>tf.transpose</code> and <code>tf.reshape</code> to rearrange the tensor so that the specified axis is preserved, the tensor is 2D and thus can be used with map_fn.</p> <p>After applying map_fn, the tensor is reshaped and transposed back to the original shape.</p> <p>:param elems: The input tensor. :param fn: The function to apply to the tensor. Must take a single tensor as input and return a tensor. :param fn_output_signature: The output signature of the function. :param axis: The axis to apply the function to. Defaults to -1. :param parallel_iterations: The number of iterations to run in parallel. Defaults to None. :param swap_memory: Whether to use memory swapping. Defaults to False. :param infer_shape: Whether to infer the shape of the output. Defaults to True. :param name: The name of the operation. Defaults to None.</p> Source code in <code>src/kamae/tensorflow/utils/transform_utils.py</code> <pre><code>def map_fn_w_axis(\n    elems: Tensor,\n    fn: Callable[[Tensor], Tensor],\n    fn_output_signature: tf.dtypes.DType,\n    axis: int = -1,\n    parallel_iterations: Optional[int] = None,\n    swap_memory: bool = False,\n    infer_shape: bool = True,\n    name: Optional[str] = None,\n) -&gt; Tensor:\n    \"\"\"\n    Applies a function to a specific axis of a tensor using map_fn.\n    Specifically uses `tf.transpose` and `tf.reshape` to rearrange the tensor so that\n    the specified axis is preserved, the tensor is 2D and thus can be used with map_fn.\n\n    After applying map_fn, the tensor is reshaped and transposed back to the original\n    shape.\n\n    :param elems: The input tensor.\n    :param fn: The function to apply to the tensor. Must take a single tensor as input\n    and return a tensor.\n    :param fn_output_signature: The output signature of the function.\n    :param axis: The axis to apply the function to. Defaults to -1.\n    :param parallel_iterations: The number of iterations to run in parallel. Defaults to\n    None.\n    :param swap_memory: Whether to use memory swapping. Defaults to False.\n    :param infer_shape: Whether to infer the shape of the output. Defaults to True.\n    :param name: The name of the operation. Defaults to None.\n    \"\"\"\n    # Permutation tensor that does nothing/identity\n    identity_perm = tf.range(start=0, limit=tf.rank(elems))\n    # Mod the axis param by the rank of the tensor and add 1. To resolve the positive\n    # axis value when axis is negative.\n    # Create the shift axis. We will roll the identity permutation by this amount to\n    # transpose the input\n    shift_axis = tf.math.mod(axis, tf.rank(elems)) + 1\n    # Roll by negative shift axis. For example if\n    # axis=0, shift_axis=1, identity_perm=[0, 1, 2]\n    # Then transpose_perm = [1, 2, 0]\n\n    # Transpose and reshape\n    transpose_perm = tf.roll(identity_perm, shift=-shift_axis, axis=0)\n    transposed_input = tf.transpose(elems, perm=transpose_perm)\n    reshaped_input = tf.reshape(transposed_input, tf.stack([-1, tf.shape(elems)[axis]]))\n\n    # Apply map_fn\n    output = tf.map_fn(\n        fn=fn,\n        elems=reshaped_input,\n        parallel_iterations=parallel_iterations,\n        swap_memory=swap_memory,\n        infer_shape=infer_shape,\n        name=name,\n        fn_output_signature=fn_output_signature,\n    )\n\n    # Undo reshape and transpose\n    undo_reshaped_output = tf.reshape(output, tf.shape(transposed_input))\n    undo_transpose_perm = tf.roll(identity_perm, shift=shift_axis, axis=0)\n    return tf.transpose(undo_reshaped_output, perm=undo_transpose_perm)\n</code></pre>"},{"location":"reference/src/kamae/utils/","title":"utils","text":""},{"location":"reference/src/kamae/utils/dtype_enum/","title":"dtype_enum","text":""},{"location":"reference/src/kamae/utils/dtype_enum/#src.kamae.utils.dtype_enum.DType","title":"DType","text":"<pre><code>DType(\n    dtype_name,\n    spark_dtype,\n    tf_dtype,\n    bytes,\n    is_floating=False,\n    is_integer=False,\n)\n</code></pre> <p>               Bases: <code>Enum</code></p> <p>Enum class for supported data types in Kamae. Contains a string name, the corresponding Spark data type, the corresponding TensorFlow data type, and the number of bytes the data type takes up. String is a special case, as it can be of any length, so the number of bytes is set to 0.</p> Source code in <code>src/kamae/utils/dtype_enum.py</code> <pre><code>def __init__(\n    self,\n    dtype_name: str,\n    spark_dtype: DataType,\n    tf_dtype: tf.dtypes.DType,\n    bytes: int,\n    is_floating: bool = False,\n    is_integer: bool = False,\n) -&gt; None:\n    self.dtype_name = dtype_name\n    self.spark_dtype = spark_dtype\n    self.tf_dtype = tf_dtype\n    self.bytes = bytes\n    self.is_floating = is_floating\n    self.is_integer = is_integer\n</code></pre>"},{"location":"reference/src/kamae/utils/utils/","title":"utils","text":""},{"location":"reference/src/kamae/utils/utils/#src.kamae.utils.utils.get_condition_operator","title":"get_condition_operator","text":"<pre><code>get_condition_operator(cond_op_string)\n</code></pre> <p>Translates a string condition operator to a function operator.</p> <p>:returns: Function operator.</p> Source code in <code>src/kamae/utils/utils.py</code> <pre><code>def get_condition_operator(cond_op_string: str) -&gt; Callable[[Any, Any], Any]:\n    \"\"\"\n    Translates a string condition operator to a function operator.\n\n    :returns: Function operator.\n    \"\"\"\n    allowed_cond_ops = {\n        \"eq\": eq,\n        \"neq\": ne,\n        \"lt\": lt,\n        \"leq\": le,\n        \"gt\": gt,\n        \"geq\": ge,\n    }\n    try:\n        return allowed_cond_ops[cond_op_string]\n    except KeyError:\n        raise ValueError(\n            f\"\"\"Unknown condition operator: {cond_op_string}.\n            Allowed condition operators are: {allowed_cond_ops.keys()}\"\"\"\n        )\n</code></pre>"}]}